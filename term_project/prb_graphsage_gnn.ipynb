{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c319e0-eeb6-448d-9167-59928fb81e0d",
   "metadata": {},
   "source": [
    "# PRB-GraphSAGE: Resource Management Reconstruction GNN  \n",
    "## CPSC 8810 — ML4G Term Project\n",
    "### Authored by Ryan Barker\n",
    "\n",
    "This notebook implements a minimal viable **PRB-GraphSAGE** model: an offline, UE-level graph neural network that reconstructs resource-management behavior from noisy per-TTI proportional-fair (PF) scheduler logs. Using the existing TTI-Trust Anamoly Detector + Classifier preprocessing pipeline, each sliding window of TTIs is converted into a **UE-contention graph** whose nodes represent UEs active in the window and whose edges capture co-scheduling (PRB contention) relationships. Node features summarize PRB usage statistics, fairness/PF surrogates, and starvation patterns, while a compact **GraphSAGE → global pooling → MLP** architecture performs **graph-level classification** to distinguish benign scheduling behavior from PF-based PRB starvation indicative of attack conditions. This MVP focuses on methodology rather than headline accuracy, acknowledging the dataset’s timing noise and simulator artifacts, and serves as a prototype for future training on GPU-native, slot-accurate Aerial/cuMAC traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f0191-075a-48a1-ab6b-82eeabe344e1",
   "metadata": {},
   "source": [
    "> <span style=\"color:red; font-weight:bold\">Disclaimer — Prototype PRB-GraphSAGE Results</span>  \n",
    ">  \n",
    "> The PRB-GraphSAGE results in this notebook were obtained **only on a small, noisy prototype dataset** derived from the original ~30 GB Open Air Interface simulator logs. Due to known timing noise within the low-latency experiment which generated the data, label corruption, and severe class imbalance in this dataset, **the model was deliberately *not* tuned or optimized to the full data volume**.  \n",
    ">  \n",
    "> These experiments are intended to validate the **model architecture and end-to-end graph construction pipeline only**. The reported metrics (including the best configuration  \n",
    "> `hidden_dim=32, num_layers=1, dropout=0.0, conv_type=\"sage\", aggr=\"mean\", lr=2e-4` with `F1 = 0.0`) **should not be interpreted as meaningful performance claims for PRB starvation detection in real RAN systems**.  \n",
    ">  \n",
    "> A proper hyperparameter search and performance evaluation will be conducted **only after more accurate PF ledgers are collected from the new NVIDIA AI RAN stack**, at which point the same PRB-GraphSAGE code will be re-used on cleaner, slot-accurate traces. The limitations of the current dataset and their impact on model behavior will be discussed in detail in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f84dda-2fe7-4959-80fa-d8a0d1fb75c6",
   "metadata": {},
   "source": [
    "### preproc_tti_trust.py\n",
    "\n",
    "This script is a preprocessing scaffold for the TTI-Trust project that converts lightweight attack/benign CSV “shims” into parquet datasets, derives scheduler-native, identity-agnostic features, and emits windowed shards plus cross-validation splits for PyTorch. It first materializes CSVs into partitioned parquet (by `run_id`), adding a relative time axis `sec_rel` if needed, then for each run it auto-detects UE resource-block columns, computes `total_rb`, normalizes RB usage into per-UE PRB shares, and derives a Jain’s fairness index `J`. On top of these per-UE shares it builds “roles” by tracking an exponential-moving-average of the shares, selecting the top-K dominant UEs per TTI, and aggregating everyone else into a `rest_share` and corresponding “small RB” indicators, plus 1-second presence flags to capture coarse duty cycle behavior. Using these role-based features and the `phase` labels, it slides long and short windows (e.g., 240- and 64-TTI) with multiple strides, labels each window as benign, attack, or ambiguous via a dominance test on the top role (with thresholds like `ATTACK_DOMINANCE`, `OTHERS_MAX`, and `DUTY_KEEP`), and batches them into compressed NumPy `.npz` shards alongside parquet meta tables that record window boundaries, labels, and summary statistics. The script writes enriched per-run parquet files, then populates a `win_shards` directory with both long and short window shards for attack and benign sources. Finally, it builds grouped K-fold splits where entire `run_id`s are kept together in either train or validation, returning index pairs for each fold and printing a brief summary so downstream training can stream shards or assemble cross-validation sets directly from the generated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb1b65f-ab4d-4e4b-a8d5-44c615516c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[attack] Found 1 run_id partitions.\n",
      "[attack] run_id=040933 → attack_enriched_040933.parquet  (rows=8,001)\n",
      "[benign] Found 1 run_id partitions.\n",
      "[benign] run_id=004912 → benign_enriched_004912.parquet  (rows=4,001)\n",
      "\n",
      "[long] meta=(2403, 11), groups=2, folds=5\n",
      "[short] meta=(4950, 11), groups=2, folds=5\n",
      "\n",
      "Preprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\n"
     ]
    }
   ],
   "source": [
    "# TTI-Trust preprocessing scaffold for PyTorch: identity-agnostic feature composer,\n",
    "# phase-aware windowing, and grouped K-fold splits.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, warnings, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.csv as pv, pyarrow.parquet as pq\n",
    "\n",
    "# -------------------- DATASETS --------------------\n",
    "ATTACK_FN = \"data/shims/attack_shim_16000_24000.csv\" # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "BENIGN_FN = \"data/shims/benign_shim_8000_12000.csv\"  # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "\n",
    "# -------------------- Constants aligned to paper --------------------\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "\n",
    "W_LONG  = 240\n",
    "W_SHORT = 64\n",
    "\n",
    "STRIDES_LONG  = [8, 12]\n",
    "STRIDES_SHORT = [4, 6]\n",
    "\n",
    "TOP_K = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "ATTACK_DOMINANCE = 0.90\n",
    "OTHERS_MAX       = 0.10\n",
    "DUTY_KEEP        = 0.90\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def csv_to_parquet(src_csv: str, dest_root: str):\n",
    "    os.makedirs(dest_root, exist_ok=True)\n",
    "    for _root, _dirs, files in os.walk(dest_root):\n",
    "        if any(f.endswith(\".parquet\") for f in files):\n",
    "            return\n",
    "\n",
    "    convert_cols = {\"run_id\": pa.large_string(), \"phase\": pa.large_string()}\n",
    "    read_opts  = pv.ReadOptions(block_size=1<<26)\n",
    "    conv_opts  = pv.ConvertOptions(column_types=convert_cols, strings_can_be_null=True)\n",
    "    t = pv.read_csv(src_csv, read_options=read_opts, convert_options=conv_opts)\n",
    "\n",
    "    if \"run_id\" not in t.schema.names:\n",
    "        stem = os.path.splitext(os.path.basename(src_csv))[0]\n",
    "        run_id = pa.array(np.full(t.num_rows, stem), type=pa.large_string())\n",
    "        t = t.append_column(\"run_id\", run_id)\n",
    "\n",
    "    if \"sec_rel\" not in t.schema.names:\n",
    "        n = t.num_rows\n",
    "        sec_rel = pa.array((np.arange(n, dtype=np.int32) * np.float32(TTI_SEC)), type=pa.float32())\n",
    "        t = t.append_column(\"sec_rel\", sec_rel)\n",
    "\n",
    "    pq.write_to_dataset(t, root_path=dest_root, partition_cols=[\"run_id\"],\n",
    "                        compression=\"zstd\", use_dictionary=True)\n",
    "\n",
    "def _detect_ue_rb_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cand = [c for c in df.columns if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in df.columns if c.lower().endswith('_rb')]\n",
    "    def _key(c):\n",
    "        m = re.search(r'(\\d+)', c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "def _ensure_total_rb(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    if 'total_rb' not in df.columns:\n",
    "        df['total_rb'] = df[ue_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def _compute_shares_and_fairness(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    shares = (df[ue_cols].clip(lower=0).astype('float32') / np.float32(C_PRB))\n",
    "    shares.columns = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    for c in shares.columns:\n",
    "        df[c] = shares[c]\n",
    "    x = shares.to_numpy(dtype='float32', copy=False)\n",
    "    sum_x  = x.sum(axis=1, dtype='float32')\n",
    "    sum_x2 = (x*x).sum(axis=1, dtype='float32')\n",
    "    n = np.float32(max(1, len(ue_cols)))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        J = (sum_x*sum_x) / (n * sum_x2)\n",
    "    df['J'] = np.nan_to_num(J, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    return df\n",
    "\n",
    "def _compose_roles(df: pd.DataFrame, ue_cols: List[str], top_k: int = TOP_K) -> pd.DataFrame:\n",
    "    share_cols = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    alpha = 2.0 / (20 + 1.0)\n",
    "    for c in share_cols:\n",
    "        df[c+\"_ema\"] = df[c].astype('float32').ewm(alpha=alpha, adjust=False).mean().astype('float32')\n",
    "\n",
    "    S   = df[share_cols].to_numpy(dtype='float32', copy=False)\n",
    "    E   = df[[c+\"_ema\" for c in share_cols]].to_numpy(dtype='float32', copy=False)\n",
    "    PRB = df[ue_cols].to_numpy(dtype='int32',    copy=False)\n",
    "\n",
    "    T, U = S.shape\n",
    "    k = min(top_k, U)\n",
    "    order  = np.argsort(-E, axis=1)\n",
    "    top_ix = order[:, :k]\n",
    "\n",
    "    roles       = np.take_along_axis(S,   top_ix, axis=1).astype('float32')\n",
    "    roles_small = np.take_along_axis((PRB < SMALL_RB_EPS).astype('float32'), top_ix, axis=1)\n",
    "\n",
    "    sum_all    = S.sum(axis=1, dtype='float32')\n",
    "    sum_topk   = roles.sum(axis=1, dtype='float32')\n",
    "    rest_share = (sum_all - sum_topk).clip(min=0).astype('float32')\n",
    "\n",
    "    if U > k:\n",
    "        row_ix = np.arange(T)[:, None]\n",
    "        mask = np.ones_like(S, dtype=bool); mask[row_ix, top_ix] = False\n",
    "        cnt  = mask.sum(axis=1)\n",
    "        rest_small = ((PRB < SMALL_RB_EPS).astype('float32') * mask).sum(axis=1) / np.maximum(cnt, 1)\n",
    "    else:\n",
    "        rest_small = np.zeros(T, dtype='float32')\n",
    "\n",
    "    for i in range(k):\n",
    "        df[f'role{i+1}_share']   = roles[:, i]\n",
    "        df[f'role{i+1}_smallrb'] = roles_small[:, i]\n",
    "    for i in range(k, top_k):\n",
    "        df[f'role{i+1}_share']   = np.float32(0.0)\n",
    "        df[f'role{i+1}_smallrb'] = np.float32(0.0)\n",
    "    df['rest_share']   = rest_share\n",
    "    df['rest_smallrb'] = rest_small\n",
    "\n",
    "    sec_bin = np.floor(df['sec_rel'].to_numpy(dtype='float32')).astype('int32')\n",
    "    max_sec = int(sec_bin.max(initial=0))\n",
    "    def sec_presence(vec_bool: np.ndarray) -> np.ndarray:\n",
    "        agg = np.zeros(max_sec + 1, dtype='uint8')\n",
    "        np.maximum.at(agg, sec_bin, vec_bool.astype('uint8'))\n",
    "        return agg[sec_bin]\n",
    "    for i in range(top_k):\n",
    "        df[f'role{i+1}_present_1s'] = sec_presence((df[f'role{i+1}_share'].to_numpy() > 0))\n",
    "    df['rest_present_1s'] = sec_presence((df['rest_share'].to_numpy() > 0))\n",
    "    return df\n",
    "\n",
    "def _label_window(shares_mat: np.ndarray) -> bool:\n",
    "    if shares_mat.size == 0:\n",
    "        return False\n",
    "    top    = shares_mat[:, 0]\n",
    "    others = shares_mat[:, 1:]\n",
    "    ok = (top >= ATTACK_DOMINANCE) & (np.max(others, axis=1) <= OTHERS_MAX)\n",
    "    return (ok.mean() >= DUTY_KEEP)\n",
    "\n",
    "@dataclass\n",
    "class WindowSpec:\n",
    "    length_tti: int\n",
    "    stride_tti: int\n",
    "\n",
    "def iter_windows(df: pd.DataFrame, run_id: str, spec: WindowSpec, role_cols: List[str],\n",
    "                 phases_attack={\"attack\"}, phases_benign={\"baseline\",\"recovery\",\"benign\"},\n",
    "                 batch_windows: int = 8192):\n",
    "    W, S = spec.length_tti, spec.stride_tti\n",
    "    R = df[role_cols].to_numpy(dtype='float32', copy=False)\n",
    "    phase = df['phase'].astype(str).to_numpy()\n",
    "    T = len(df)\n",
    "    starts = np.arange(0, max(0, T - W + 1), S, dtype=np.int64)\n",
    "\n",
    "    bufX, rows = [], []\n",
    "    for s in starts:\n",
    "        e = s + W\n",
    "        blk = R[s:e, :]\n",
    "        if blk.shape[0] != W:\n",
    "            continue\n",
    "        win_phase = phase[s:e]\n",
    "        frac_attack = np.isin(win_phase, list(phases_attack)).mean()\n",
    "        frac_benign = np.isin(win_phase, list(phases_benign)).mean()\n",
    "        looks_attacky = _label_window(blk)\n",
    "        if (frac_attack >= 0.90) and looks_attacky: y = 2\n",
    "        elif (frac_benign >= 0.90):                y = 0\n",
    "        else:                                      y = 1\n",
    "\n",
    "        bufX.append(blk)\n",
    "        rows.append((run_id, int(s), int(e), int(W), int(y),\n",
    "                     float(frac_attack), float(frac_benign), int(looks_attacky)))\n",
    "\n",
    "        if len(bufX) == batch_windows:\n",
    "            X = np.stack(bufX, axis=0)\n",
    "            meta = pd.DataFrame.from_records(\n",
    "                rows,\n",
    "                columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                         \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "            )\n",
    "            yield X, meta\n",
    "            bufX.clear(); rows.clear()\n",
    "\n",
    "    if bufX:\n",
    "        X = np.stack(bufX, axis=0)\n",
    "        meta = pd.DataFrame.from_records(\n",
    "            rows,\n",
    "            columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                     \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "        )\n",
    "        yield X, meta\n",
    "\n",
    "# -------------------- Main entry: build datasets (partitioned, low RAM) --------------------\n",
    "\n",
    "csv_to_parquet(ATTACK_FN, \"parquet/attack\")\n",
    "csv_to_parquet(BENIGN_FN, \"parquet/benign\")\n",
    "\n",
    "def iter_run_partitions(root: str):\n",
    "    for run_dir in sorted(glob(os.path.join(root, \"run_id=*\"))):\n",
    "        rid = os.path.basename(run_dir).split(\"=\", 1)[1]\n",
    "        df = pd.read_parquet(run_dir, engine=\"pyarrow\")\n",
    "        if 'run_id' not in df.columns:\n",
    "            df['run_id'] = rid\n",
    "        if 'phase' not in df.columns:\n",
    "            df['phase'] = 'unknown'\n",
    "        if 'sec_rel' not in df.columns:\n",
    "            df['sec_rel'] = (np.arange(len(df), dtype=np.int32) * np.float32(TTI_SEC)).astype('float32')\n",
    "        yield rid, df\n",
    "\n",
    "def process_source(root: str, src_name: str):\n",
    "    if not os.path.isdir(root):\n",
    "        print(f\"[{src_name}] No parquet root found at {root} (skipping).\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"win_shards\", exist_ok=True)\n",
    "    shard_counter = itertools.count(0)\n",
    "\n",
    "    runs = list(iter_run_partitions(root))\n",
    "    if not runs:\n",
    "        print(f\"[{src_name}] No run_id partitions found in {root}.\")\n",
    "        return\n",
    "    print(f\"[{src_name}] Found {len(runs)} run_id partitions.\")\n",
    "\n",
    "    for rid, df in runs:\n",
    "        ue_cols = _detect_ue_rb_columns(df)\n",
    "        if not ue_cols:\n",
    "            warnings.warn(f\"[{src_name}] run_id={rid}: no UE *_rb columns; skipping\")\n",
    "            continue\n",
    "\n",
    "        df = _ensure_total_rb(df, ue_cols)\n",
    "        df = _compute_shares_and_fairness(df, ue_cols)\n",
    "        df = _compose_roles(df, ue_cols, top_k=TOP_K)\n",
    "\n",
    "        keep = [f'role{i}_share' for i in range(1, TOP_K+1)] + ['rest_share','J','sec_rel','run_id','phase']\n",
    "        enriched_out = f\"{src_name}_enriched_{rid}.parquet\"\n",
    "        df[keep].to_parquet(enriched_out, index=False)\n",
    "        print(f\"[{src_name}] run_id={rid} → {enriched_out}  (rows={len(df):,})\")\n",
    "\n",
    "        role_cols = [f'role{k}_share' for k in range(1, TOP_K+1)] + ['rest_share']\n",
    "        g = df.reset_index(drop=True)\n",
    "\n",
    "        for stride in STRIDES_LONG:\n",
    "            spec = WindowSpec(length_tti=W_LONG, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"long\"\n",
    "                meta.to_parquet(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "        for stride in STRIDES_SHORT:\n",
    "            spec = WindowSpec(length_tti=W_SHORT, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"short\"\n",
    "                meta.to_parquet(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "process_source(\"parquet/attack\", \"attack\")\n",
    "process_source(\"parquet/benign\", \"benign\")\n",
    "\n",
    "def load_all_meta(kind: str) -> pd.DataFrame:\n",
    "    metas = [pd.read_parquet(p) for p in glob(f\"win_shards/{kind}_*_meta.parquet\")]\n",
    "    return pd.concat(metas, ignore_index=True) if metas else pd.DataFrame()\n",
    "\n",
    "M_long  = load_all_meta(\"long\")\n",
    "M_short = load_all_meta(\"short\")\n",
    "\n",
    "def grouped_kfold(meta: pd.DataFrame, K: int = 5, seed: int = 42) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    if meta.empty:\n",
    "        return []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    run_ids = meta['run_id'].astype(str).unique().tolist()\n",
    "    rng.shuffle(run_ids)\n",
    "    folds = [set() for _ in range(K)]\n",
    "    for i, rid in enumerate(run_ids):\n",
    "        folds[i % K].add(rid)\n",
    "    splits = []\n",
    "    for i in range(K):\n",
    "        val_rids = folds[i]\n",
    "        train_idx = meta.index[~meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        val_idx   = meta.index[ meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        splits.append((train_idx, val_idx))\n",
    "    return splits\n",
    "\n",
    "splits_long  = grouped_kfold(M_long,  K=5, seed=2025)\n",
    "splits_short = grouped_kfold(M_short, K=5, seed=2025)\n",
    "\n",
    "if len(M_long):\n",
    "    print(f\"\\n[long] meta={M_long.shape}, groups={M_long['run_id'].nunique()}, folds={len(splits_long)}\")\n",
    "if len(M_short):\n",
    "    print(f\"[short] meta={M_short.shape}, groups={M_short['run_id'].nunique()}, folds={len(splits_short)}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb5f5-a951-47f5-99df-6b9d5a8601e9",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "This code builds a full PyTorch Geometric pipeline that turns PRB time-series windows into UE-level contention graphs and then into train/validation loaders for a PRB-GraphSAGE model. It starts by detecting UE RB columns (e.g., `UE1_rb`) in per-run parquet “PF ledger” data, then constructs node features for each UE in a window using PRB-based statistics such as mean/min/max/std share of PRBs, activity and zero fractions, and the longest zero (starvation) run. Edges are created between UEs that are co-scheduled at least once in the window, yielding an undirected contention graph. For each window in the preprocessed metadata (`win_shards/{kind}_*_meta.parquet`), `build_graph_from_window` slices the run dataframe, computes node features and edges, and assigns a graph label (binary or multiclass) from the window label, packaging everything into a `torch_geometric.data.Data` object plus a compact metadata dict. `build_prb_graphs` loads and filters all meta files, caches run-level parquet data and UE column detection by `(source, run_id)`, iterates over windows to construct graphs, and returns the list of graphs along with a metadata DataFrame. On top of this, `make_prb_graph_loaders` performs a grouped train/validation split by `run_id` (so windows from the same run don’t leak across splits), builds `GeoDataLoader` instances for training and validation, and returns them alongside split indices and run ID sets. Finally, the script instantiates these loaders for short windows, infers the node feature dimensionality with `detect_node_dim`, and prints it, providing everything needed to feed PRB-GraphSAGE with graph-structured inputs derived from scheduler PRB logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c496f167-721e-4197-9c1e-961bf4260af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dim: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "\n",
    "# -------------------- Shared constants --------------------\n",
    "\n",
    "C_PRB = 106  # total PRBs; must match preprocessing\n",
    "\n",
    "\n",
    "# -------------------- UE column detection --------------------\n",
    "\n",
    "def _detect_ue_rb_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Detect UE RB columns, e.g. UE1_rb, UE2_rb, ...\n",
    "    Falls back to '*_rb' if needed.\n",
    "    \"\"\"\n",
    "    cand = [c for c in df.columns if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in df.columns if c.lower().endswith(\"_rb\")]\n",
    "\n",
    "    def _key(c):\n",
    "        m = re.search(r\"(\\d+)\", c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "\n",
    "# -------------------- Node feature construction --------------------\n",
    "\n",
    "def _longest_zero_run(arr: np.ndarray) -> float:\n",
    "    \"\"\"Longest consecutive run of zeros in a 1D array.\"\"\"\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for v in arr:\n",
    "        if v == 0:\n",
    "            current += 1\n",
    "            if current > longest:\n",
    "                longest = current\n",
    "        else:\n",
    "            current = 0\n",
    "    return float(longest)\n",
    "\n",
    "\n",
    "def build_node_features(prb_window: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build UE-level node features for a single window.\n",
    "\n",
    "    prb_window: [W, U] PRBs per TTI per UE\n",
    "    Returns: [U, d_node] node feature matrix.\n",
    "    \"\"\"\n",
    "    # shape sanity\n",
    "    assert prb_window.ndim == 2\n",
    "    W, U = prb_window.shape\n",
    "    if U == 0:\n",
    "        return np.zeros((0, 1), dtype=np.float32)\n",
    "\n",
    "    prb = prb_window.astype(np.float32)\n",
    "    share = prb / np.float32(C_PRB)  # [W, U]\n",
    "\n",
    "    # basic stats\n",
    "    mean_share = share.mean(axis=0)         \n",
    "    min_share = share.min(axis=0)\n",
    "    max_share = share.max(axis=0)\n",
    "    std_share = share.std(axis=0)\n",
    "\n",
    "    # activity / starvation\n",
    "    active = (prb > 0).astype(np.float32)    \n",
    "    active_frac = active.mean(axis=0)        \n",
    "    zero_frac = (prb == 0).mean(axis=0)      \n",
    "    longest_zero = np.array(\n",
    "        [_longest_zero_run(prb[:, u]) for u in range(U)], dtype=np.float32\n",
    "    )                                        \n",
    "\n",
    "    # stack into [U, d_node]\n",
    "    feats = np.stack(\n",
    "        [\n",
    "            mean_share,\n",
    "            min_share,\n",
    "            max_share,\n",
    "            std_share,\n",
    "            active_frac,\n",
    "            zero_frac,\n",
    "            longest_zero,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    return feats  # [U, 7]\n",
    "\n",
    "\n",
    "# -------------------- Edge construction --------------------\n",
    "\n",
    "def build_edges(prb_window: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build undirected edges between UEs that are co-scheduled at least once.\n",
    "\n",
    "    prb_window: [W, U]\n",
    "    Returns: edge_index [2, E] (torch.long)\n",
    "    \"\"\"\n",
    "    prb = prb_window.astype(np.float32)\n",
    "    W, U = prb.shape\n",
    "    if U <= 1:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    edge_src = []\n",
    "    edge_dst = []\n",
    "\n",
    "    for i in range(U):\n",
    "        for j in range(i + 1, U):\n",
    "            both_active = (prb[:, i] > 0) & (prb[:, j] > 0)\n",
    "            if both_active.any():\n",
    "                edge_src.extend([i, j])\n",
    "                edge_dst.extend([j, i])\n",
    "\n",
    "    if not edge_src:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "# -------------------- Graph construction --------------------\n",
    "\n",
    "def build_graph_from_window(\n",
    "    df_run: pd.DataFrame,\n",
    "    ue_cols: List[str],\n",
    "    row_meta: pd.Series,\n",
    "    label_mode: str = \"binary\",\n",
    ") -> Tuple[Data, Dict]:\n",
    "    \"\"\"\n",
    "    Build a PyG Data graph from one window.\n",
    "\n",
    "    df_run: full run dataframe (all TTIs), index aligned so that start_tti/end_tti are valid iloc indices.\n",
    "    ue_cols: list of UE*_rb columns.\n",
    "    row_meta: single row from window meta parquet.\n",
    "    label_mode: \"binary\" (attack vs non-attack) or \"multiclass\" (0,1,2).\n",
    "\n",
    "    Returns:\n",
    "        (Data, small_meta_dict)\n",
    "    \"\"\"\n",
    "    start = int(row_meta[\"start_tti\"])\n",
    "    end = int(row_meta[\"end_tti\"])\n",
    "    run_id = str(row_meta.get(\"run_id\", \"\"))\n",
    "    source = str(row_meta.get(\"source\", \"\"))\n",
    "\n",
    "    df_w = df_run.iloc[start:end].reset_index(drop=True)\n",
    "    if df_w.empty:\n",
    "        # Return an empty graph; caller should skip if desired.\n",
    "        data = Data(\n",
    "            x=torch.zeros((0, 1), dtype=torch.float32),\n",
    "            edge_index=torch.empty((2, 0), dtype=torch.long),\n",
    "            y=torch.tensor(0, dtype=torch.long),\n",
    "        )\n",
    "        meta_small = {\n",
    "            \"run_id\": run_id,\n",
    "            \"source\": source,\n",
    "            \"start_tti\": start,\n",
    "            \"end_tti\": end,\n",
    "            \"label_raw\": int(row_meta.get(\"label\", 0)),\n",
    "            \"label_bin\": 0,\n",
    "        }\n",
    "        return data, meta_small\n",
    "\n",
    "    prb_window = df_w[ue_cols].to_numpy(dtype=np.float32)  # [W, U]\n",
    "    node_feats = build_node_features(prb_window)           # [U, d_node]\n",
    "    edge_index = build_edges(prb_window)                   # [2, E]\n",
    "\n",
    "    label_raw = int(row_meta.get(\"label\", 0))  # 0: benign, 1: proximal, 2: attack\n",
    "    if label_mode == \"binary\":\n",
    "        label_bin = 1 if label_raw == 2 else 0\n",
    "        y = torch.tensor(label_bin, dtype=torch.long)\n",
    "    elif label_mode == \"multiclass\":\n",
    "        y = torch.tensor(label_raw, dtype=torch.long)\n",
    "        label_bin = 1 if label_raw == 2 else 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_mode: {label_mode}\")\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.from_numpy(node_feats),  # [U, d_node]\n",
    "        edge_index=edge_index,\n",
    "        y=y,\n",
    "    )\n",
    "\n",
    "    meta_small = {\n",
    "        \"run_id\": run_id,\n",
    "        \"source\": source,\n",
    "        \"start_tti\": start,\n",
    "        \"end_tti\": end,\n",
    "        \"label_raw\": label_raw,\n",
    "        \"label_bin\": label_bin,\n",
    "        \"kind\": str(row_meta.get(\"kind\", \"\")),\n",
    "        \"stride\": int(row_meta.get(\"stride\", 0)),\n",
    "    }\n",
    "\n",
    "    return data, meta_small\n",
    "\n",
    "\n",
    "# -------------------- Build all graphs from meta + parquet --------------------\n",
    "\n",
    "def build_prb_graphs(\n",
    "    kind: str = \"short\",\n",
    "    meta_glob: Optional[str] = None,\n",
    "    parquet_root_attack: str = \"parquet/attack\",\n",
    "    parquet_root_benign: str = \"parquet/benign\",\n",
    "    label_mode: str = \"binary\",\n",
    "    restrict_sources: Optional[List[str]] = None,\n",
    "    max_windows: Optional[int] = None,\n",
    ") -> Tuple[List[Data], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build a list of PRB-GraphSAGE graphs from preprocessed meta + PF ledger parquet.\n",
    "\n",
    "    kind: \"short\" or \"long\" (matches the window kind in preprocessing)\n",
    "    meta_glob: optional override for meta file glob pattern; if None, uses f\"win_shards/{kind}_*_meta.parquet\"\n",
    "    parquet_root_attack / parquet_root_benign: roots for run-level PF ledgers\n",
    "    label_mode: \"binary\" or \"multiclass\"\n",
    "    restrict_sources: optional [\"attack\",\"benign\"] filter\n",
    "    max_windows: optional cap on number of windows (for dev / shims)\n",
    "    \"\"\"\n",
    "    if meta_glob is None:\n",
    "        meta_glob = f\"win_shards/{kind}_*_meta.parquet\"\n",
    "\n",
    "    meta_files = sorted(glob.glob(meta_glob))\n",
    "    if not meta_files:\n",
    "        raise FileNotFoundError(f\"No meta parquet files found for pattern: {meta_glob}\")\n",
    "\n",
    "    # Load and concatenate all meta rows\n",
    "    meta_rows = []\n",
    "    for mp in meta_files:\n",
    "        m = pd.read_parquet(mp)\n",
    "        if restrict_sources is not None and \"source\" in m.columns:\n",
    "            m = m[m[\"source\"].isin(restrict_sources)]\n",
    "        if len(m) == 0:\n",
    "            continue\n",
    "        m = m.copy()\n",
    "        m[\"meta_path\"] = mp\n",
    "        meta_rows.append(m)\n",
    "\n",
    "    if not meta_rows:\n",
    "        raise RuntimeError(\"No meta rows found after filtering.\")\n",
    "\n",
    "    meta_all = pd.concat(meta_rows, ignore_index=True)\n",
    "    graphs: List[Data] = []\n",
    "    gmeta_rows: List[Dict] = []\n",
    "\n",
    "    # Cache run-level DF + UE cols so we don't re-read parquet per window\n",
    "    run_cache: Dict[Tuple[str, str], Tuple[pd.DataFrame, List[str]]] = {}\n",
    "\n",
    "    for idx, row in meta_all.iterrows():\n",
    "        source = str(row.get(\"source\", \"\"))\n",
    "        run_id = str(row.get(\"run_id\", \"\"))\n",
    "\n",
    "        if restrict_sources is not None and source not in restrict_sources:\n",
    "            continue\n",
    "\n",
    "        if source == \"attack\":\n",
    "            root = parquet_root_attack\n",
    "        elif source == \"benign\":\n",
    "            root = parquet_root_benign\n",
    "        else:\n",
    "            # fallback; interpret non-attack as benign\n",
    "            root = parquet_root_benign\n",
    "\n",
    "        key = (source, run_id)\n",
    "        if key not in run_cache:\n",
    "            run_dir = os.path.join(root, f\"run_id={run_id}\")\n",
    "            if not os.path.isdir(run_dir) and not run_dir.endswith(\".parquet\"):\n",
    "                # try direct parquet file as well\n",
    "                if os.path.exists(run_dir + \".parquet\"):\n",
    "                    run_dir = run_dir + \".parquet\"\n",
    "            df_run = pd.read_parquet(run_dir)\n",
    "            df_run = df_run.reset_index(drop=True)\n",
    "            ue_cols = _detect_ue_rb_columns(df_run)\n",
    "            if not ue_cols:\n",
    "                # skip runs with no UE RB columns\n",
    "                continue\n",
    "            run_cache[key] = (df_run, ue_cols)\n",
    "\n",
    "        df_run, ue_cols = run_cache[key]\n",
    "\n",
    "        data, ms = build_graph_from_window(df_run, ue_cols, row, label_mode=label_mode)\n",
    "        # skip empty graphs (no UEs or no rows)\n",
    "        if data.x.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        graphs.append(data)\n",
    "        ms[\"graph_idx\"] = len(graphs) - 1\n",
    "        gmeta_rows.append(ms)\n",
    "\n",
    "        if max_windows is not None and len(graphs) >= max_windows:\n",
    "            break\n",
    "\n",
    "    if not graphs:\n",
    "        raise RuntimeError(\"No graphs were constructed; check meta and parquet roots.\")\n",
    "\n",
    "    graph_meta = pd.DataFrame(gmeta_rows).reset_index(drop=True)\n",
    "    return graphs, graph_meta\n",
    "\n",
    "\n",
    "# -------------------- Utility: detect node feature dimension --------------------\n",
    "\n",
    "def detect_node_dim(loader: GeoDataLoader) -> int:\n",
    "    \"\"\"\n",
    "    Inspect the first batch from a PyG DataLoader to infer node feature dimension.\n",
    "    \"\"\"\n",
    "    batch = next(iter(loader))\n",
    "    return batch.x.size(-1)\n",
    "\n",
    "\n",
    "# -------------------- High-level loader factory --------------------\n",
    "\n",
    "def make_prb_graph_loaders(\n",
    "    kind: str = \"short\",\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 0,\n",
    "    seed: int = 2025,\n",
    "    val_ratio: float = 0.2,\n",
    "    label_mode: str = \"binary\",\n",
    "    restrict_sources: Optional[List[str]] = None,\n",
    "    max_windows: Optional[int] = None,\n",
    ") -> Tuple[GeoDataLoader, GeoDataLoader, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Build train/val PyG DataLoaders for PRB-GraphSAGE.\n",
    "\n",
    "    Splits by run_id (grouped), so windows from the same run_id do not cross train/val.\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, graph_meta, splits_dict\n",
    "    \"\"\"\n",
    "    graphs, graph_meta = build_prb_graphs(\n",
    "        kind=kind,\n",
    "        label_mode=label_mode,\n",
    "        restrict_sources=restrict_sources,\n",
    "        max_windows=max_windows,\n",
    "    )\n",
    "\n",
    "    # Grouped split by run_id\n",
    "    run_ids = graph_meta[\"run_id\"].astype(str).unique().tolist()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(run_ids)\n",
    "\n",
    "    n_val_runs = max(1, int(len(run_ids) * val_ratio))\n",
    "    val_runs = set(run_ids[:n_val_runs])\n",
    "\n",
    "    train_idx: List[int] = []\n",
    "    val_idx: List[int] = []\n",
    "\n",
    "    for i, rid in enumerate(graph_meta[\"run_id\"].astype(str).tolist()):\n",
    "        if rid in val_runs:\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "\n",
    "    train_graphs = [graphs[i] for i in train_idx]\n",
    "    val_graphs = [graphs[i] for i in val_idx]\n",
    "\n",
    "    train_loader = GeoDataLoader(\n",
    "        train_graphs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = GeoDataLoader(\n",
    "        val_graphs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train_idx\": np.array(train_idx, dtype=np.int64),\n",
    "        \"val_idx\": np.array(val_idx, dtype=np.int64),\n",
    "        \"val_run_ids\": list(val_runs),\n",
    "        \"all_run_ids\": run_ids,\n",
    "    }\n",
    "\n",
    "    return train_loader, val_loader, graph_meta, splits\n",
    "\n",
    "train_loader, val_loader, graph_meta, splits = make_prb_graph_loaders(\n",
    "    kind=\"short\",\n",
    "    batch_size=GNN_HYP[\"batch_size\"],\n",
    "    val_ratio=0.2,\n",
    "    label_mode=\"binary\",   # attack vs non-attack\n",
    "    max_windows=None,      # or a small number for quick dev\n",
    ")\n",
    "\n",
    "D_NODE = detect_node_dim(train_loader)\n",
    "print(\"Node feature dim:\", D_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a4651-9ce3-4d5c-9178-84f6701ec6cb",
   "metadata": {},
   "source": [
    "### Base Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1bacf-e010-4743-a55b-04e003af9c1f",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE Architecture\n",
    "This code defines `PRBGraphSAGE`, a configurable PyTorch Geometric graph neural network for classifying UE-level contention graphs in a PRB starvation detection task. The model builds a GNN backbone with a user-specified number of layers (`num_layers`), hidden dimension, and convolution type (`SAGEConv`, `GCNConv`, `GraphConv`, or `GATv2Conv`), optionally followed by batch normalization and dropout after each layer. In the forward pass, node features `x` are iteratively updated using the chosen conv layers over edges `edge_index`, then aggregated into graph-level embeddings via a selectable global pooling strategy (`mean`, `max`, or concatenated `mean+max`). This pooled representation is passed through an MLP head whose depth and hidden size are also configurable, producing either a scalar logit per graph for binary classification (`num_classes == 1`) or a multi-dimensional logit vector for multi-class problems. The interface is designed to be hyperparameter-sweep friendly, with `edge_attr` included in the signature as a placeholder for future edge-aware enhancements, while the current implementation focuses on node features and graph-level pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd70b31c-f0ce-489f-92a2-f258c2632f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv,\n",
    "    GCNConv,\n",
    "    GraphConv,\n",
    "    GATv2Conv,\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    ")\n",
    "\n",
    "\n",
    "class PRBGraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    PRB-GraphSAGE: UE-level contention graph classifier for PRB starvation detection.\n",
    "\n",
    "    This model is designed to be easy to sweep over hyperparameters:\n",
    "      - in_dim:         node feature dimension\n",
    "      - hidden_dim:     hidden channel width\n",
    "      - num_layers:     number of GNN layers (>=1)\n",
    "      - conv_type:      GNN layer type: {\"sage\", \"gcn\", \"graph\", \"gat\"}\n",
    "      - aggr:           global pooling: {\"mean\", \"max\", \"mean+max\"}\n",
    "      - mlp_hidden_dim: hidden size of the final MLP head\n",
    "      - mlp_layers:     number of linear layers in the MLP head (>=1)\n",
    "      - dropout:        dropout probability applied after each conv & MLP layer\n",
    "      - num_classes:    1 for binary logit, >1 for multi-class logits\n",
    "\n",
    "    Forward signature:\n",
    "        logits = model(x, edge_index, batch, edge_attr=None)\n",
    "\n",
    "      - x:          [N, in_dim] node features\n",
    "      - edge_index: [2, E] COO edge indices\n",
    "      - batch:      [N] graph IDs for global pooling\n",
    "      - edge_attr:  [E, d_edge] (currently ignored; placeholder for future use)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        conv_type: str = \"sage\",\n",
    "        aggr: str = \"mean\",\n",
    "        mlp_hidden_dim: int = 64,\n",
    "        mlp_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 1,\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert num_layers >= 1, \"num_layers must be >= 1\"\n",
    "        assert mlp_layers >= 1, \"mlp_layers must be >= 1\"\n",
    "        assert conv_type in {\"sage\", \"gcn\", \"graph\", \"gat\"}, f\"Unknown conv_type: {conv_type}\"\n",
    "        assert aggr in {\"mean\", \"max\", \"mean+max\"}, f\"Unknown aggr: {aggr}\"\n",
    "        self.aggr = aggr\n",
    "        self.dropout = float(dropout)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.use_batchnorm = bool(use_batchnorm)\n",
    "\n",
    "        # ---- GNN backbone ----\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() if use_batchnorm else None\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(self._make_conv(conv_type, in_dim, hidden_dim))\n",
    "        if use_batchnorm:\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(self._make_conv(conv_type, hidden_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # ---- Readout (global pooling) ----\n",
    "        # mean+max concatenation doubles the channel dimension\n",
    "        readout_dim = hidden_dim if aggr in {\"mean\", \"max\"} else hidden_dim * 2\n",
    "\n",
    "        # ---- MLP classifier head ----\n",
    "        mlp_layers_list = []\n",
    "        in_mlp = readout_dim\n",
    "        for li in range(mlp_layers):\n",
    "            out_mlp = mlp_hidden_dim if li < mlp_layers - 1 else num_classes\n",
    "            mlp_layers_list.append(nn.Linear(in_mlp, out_mlp))\n",
    "            if li < mlp_layers - 1:\n",
    "                mlp_layers_list.append(nn.SiLU())\n",
    "                if dropout > 0:\n",
    "                    mlp_layers_list.append(nn.Dropout(dropout))\n",
    "            in_mlp = out_mlp\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_conv(conv_type: str, in_dim: int, out_dim: int):\n",
    "        \"\"\"Factory for different conv types to make hyperparameter sweeps easy.\"\"\"\n",
    "        if conv_type == \"sage\":\n",
    "            return SAGEConv(in_dim, out_dim)\n",
    "        if conv_type == \"gcn\":\n",
    "            return GCNConv(in_dim, out_dim)\n",
    "        if conv_type == \"graph\":\n",
    "            return GraphConv(in_dim, out_dim)\n",
    "        if conv_type == \"gat\":\n",
    "            return GATv2Conv(in_dim, out_dim, heads=1, concat=False)\n",
    "        raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        \"\"\"\n",
    "        x:          [N, in_dim]\n",
    "        edge_index: [2, E]\n",
    "        batch:      [N] graph IDs\n",
    "        edge_attr:  [E, d_edge] (ignored for now; reserved for future edge-aware layers)\n",
    "        \"\"\"\n",
    "        # ---- GNN backbone ----\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.silu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # ---- Global pooling ----\n",
    "        if self.aggr == \"mean\":\n",
    "            g = global_mean_pool(x, batch)\n",
    "        elif self.aggr == \"max\":\n",
    "            g = global_max_pool(x, batch)\n",
    "        else:  # \"mean+max\"\n",
    "            g_mean = global_mean_pool(x, batch)\n",
    "            g_max = global_max_pool(x, batch)\n",
    "            g = torch.cat([g_mean, g_max], dim=-1)\n",
    "\n",
    "        # ---- MLP head ----\n",
    "        logits = self.mlp(g)  # [num_graphs, num_classes]\n",
    "\n",
    "        # For binary classification, most training loops will want shape [num_graphs]\n",
    "        if self.num_classes == 1:\n",
    "            logits = logits.squeeze(-1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab57bc7-9215-4381-990f-435dcaf040ce",
   "metadata": {},
   "source": [
    "### Base Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f3d97-9ee8-48cc-8ab8-11d2d1b887ee",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "This code sets up initialization, configuration, and lightweight hyperparameter sweeping for a PRB-GraphSAGE GNN used in PRB starvation detection. It begins by seeding Python, NumPy, and PyTorch RNGs for reproducibility, choosing a CUDA or CPU device, and defining `D_NODE` as the node feature dimension (with a placeholder warning until real graph data is available). A `GNN_HYP` dictionary gathers all key hyperparameters in one place—training settings (batch size, epochs, learning rate and grid, weight decay), backbone architecture (hidden size, number of layers, dropout, conv type, pooling strategy, batchnorm), MLP head configuration, output dimension (binary vs multi-class), early-stopping criteria, and scheduler parameters (warmup and total steps). A helper `compute_pos_weight_from_labels` computes class weights for `BCEWithLogitsLoss` from imbalanced binary labels, with a default `pos_weight` of 1.0 until real labels are known. The `make_gnn_model` factory instantiates a `PRBGraphSAGE` model from a hyperparameter dict and moves it to the chosen device, while `count_params` reports trainable parameter count. Optimizer and scheduler builders create an AdamW optimizer and a LambdaLR scheduler implementing linear warmup followed by cosine decay over `total_steps`. The script then instantiates a default model, optimizer, scheduler, and a binary `BCEWithLogitsLoss` using the current `pos_weight`, printing summaries. Finally, `iter_gnn_configs` defines a simple grid-search generator over selected hyperparameters (learning rate, hidden size, number of layers, dropout, aggregation, and conv type), and the code demonstrates its use by instantiating and printing the first few candidate configurations along with their parameter counts, providing a ready-made loop for hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c6977c6-e5cc-4ed9-b8ed-935262806065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GNN Hyperparameters: {\n",
      "  \"batch_size\": 32,\n",
      "  \"epochs\": 40,\n",
      "  \"lr\": 0.0003,\n",
      "  \"lr_grid\": [\n",
      "    0.0002,\n",
      "    0.0003,\n",
      "    0.0005\n",
      "  ],\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"hidden_dim\": 64,\n",
      "  \"num_layers\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"conv_type\": \"sage\",\n",
      "  \"aggr\": \"mean\",\n",
      "  \"use_batchnorm\": true,\n",
      "  \"hidden_dim_grid\": [\n",
      "    32,\n",
      "    64\n",
      "  ],\n",
      "  \"num_layers_grid\": [\n",
      "    1,\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"dropout_grid\": [\n",
      "    0.0,\n",
      "    0.1,\n",
      "    0.3\n",
      "  ],\n",
      "  \"aggr_grid\": [\n",
      "    \"mean\",\n",
      "    \"mean+max\"\n",
      "  ],\n",
      "  \"conv_type_grid\": [\n",
      "    \"sage\",\n",
      "    \"gcn\"\n",
      "  ],\n",
      "  \"mlp_hidden_dim\": 64,\n",
      "  \"mlp_layers\": 2,\n",
      "  \"num_classes\": 1,\n",
      "  \"early_stop\": {\n",
      "    \"monitor\": \"f1\",\n",
      "    \"mode\": \"max\",\n",
      "    \"patience\": 5,\n",
      "    \"min_delta\": 0.001\n",
      "  },\n",
      "  \"warmup_steps\": 200,\n",
      "  \"total_steps\": 10000\n",
      "}\n",
      "Initial pos_weight (binary): 1.0\n",
      "GNN params: 14,849\n",
      "GNN model, optimizer, scheduler, and loss are initialized.\n",
      "\n",
      "Initialization\n",
      "  Config 0: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=sage, params=3,297\n",
      "  Config 1: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=gcn, params=2,785\n",
      "  Config 2: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean+max, conv_type=sage, params=5,345\n"
     ]
    }
   ],
   "source": [
    "# ==== CELL: GNN Model + Hyperparameter Initialization ====\n",
    "import os, math, json, random\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Reproducibility & device ----\n",
    "SEED = 2025\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_all(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "try:\n",
    "    D_NODE\n",
    "except NameError:\n",
    "    D_NODE = 16  # placeholder for later\n",
    "    print(\"WARNING: D_NODE is using a placeholder value (16). Set it to your actual node feature dim.\")\n",
    "\n",
    "# ---- GNN Hyperparameters (with small grids for sweeps) ----\n",
    "GNN_HYP: Dict = {\n",
    "    # ---- Training-level hparams ----\n",
    "    \"batch_size\": 32,              \n",
    "    \"epochs\": 40,                  \n",
    "    \"lr\": 3e-4,                    \n",
    "    \"lr_grid\": [2e-4, 3e-4, 5e-4], \n",
    "    \"weight_decay\": 1e-4,\n",
    "\n",
    "    # ---- Backbone architecture ----\n",
    "    # Base config (used if you don't sweep)\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"conv_type\": \"sage\",           \n",
    "    \"aggr\": \"mean\",                \n",
    "    \"use_batchnorm\": True,\n",
    "\n",
    "    # Grids for a compact but meaningful sweep\n",
    "    \"hidden_dim_grid\": [32, 64],           \n",
    "    \"num_layers_grid\": [1, 2, 3],          \n",
    "    \"dropout_grid\": [0.0, 0.1, 0.3],       \n",
    "    \"aggr_grid\": [\"mean\", \"mean+max\"],     \n",
    "    \"conv_type_grid\": [\"sage\", \"gcn\"],     \n",
    "\n",
    "    # ---- MLP head ----\n",
    "    \"mlp_hidden_dim\": 64,\n",
    "    \"mlp_layers\": 2,\n",
    "\n",
    "    # ---- Output ----\n",
    "    # 1 = binary logit (BCEWithLogits); >1 = multi-class (CrossEntropy)\n",
    "    \"num_classes\": 1,\n",
    "\n",
    "    # ---- Early-stop config (used by your training loop) ----\n",
    "    \"early_stop\": {\n",
    "        \"monitor\": \"f1\",            \n",
    "        \"mode\": \"max\",              \n",
    "        \"patience\": 5,              \n",
    "        \"min_delta\": 1e-3,          \n",
    "    },\n",
    "\n",
    "    # ---- Scheduler placeholders (updated once loaders are known) ----\n",
    "    \"warmup_steps\": 200,\n",
    "    \"total_steps\": 10000,\n",
    "}\n",
    "\n",
    "print(\"GNN Hyperparameters:\", json.dumps(GNN_HYP, indent=2))\n",
    "\n",
    "\n",
    "# ---- Optional class weights for binary BCEWithLogitsLoss ----\n",
    "# You can adapt this to your graph meta if you materialize labels in a DataFrame.\n",
    "def compute_pos_weight_from_labels(labels: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pos_weight for BCEWithLogitsLoss from a binary label vector.\n",
    "    pos_weight = (#neg / #pos)\n",
    "    \"\"\"\n",
    "    labels = labels.astype(np.int64).ravel()\n",
    "    pos = (labels == 1).sum()\n",
    "    neg = (labels == 0).sum()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0, dtype=torch.float32)\n",
    "    return torch.tensor(float(neg) / float(pos), dtype=torch.float32)\n",
    "\n",
    "pos_weight = torch.tensor(1.0, dtype=torch.float32)\n",
    "print(\"Initial pos_weight (binary):\", float(pos_weight))\n",
    "\n",
    "\n",
    "# ---- PRB-GraphSAGE factory ----\n",
    "def make_gnn_model(hyp: Dict, in_dim: int) -> PRBGraphSAGE:\n",
    "    \"\"\"\n",
    "    Instantiate a PRBGraphSAGE model from a hyperparameter dict.\n",
    "    \"\"\"\n",
    "    model = PRBGraphSAGE(\n",
    "        in_dim=in_dim,\n",
    "        hidden_dim=hyp[\"hidden_dim\"],\n",
    "        num_layers=hyp[\"num_layers\"],\n",
    "        conv_type=hyp[\"conv_type\"],      \n",
    "        aggr=hyp[\"aggr\"],                \n",
    "        mlp_hidden_dim=hyp[\"mlp_hidden_dim\"],\n",
    "        mlp_layers=hyp[\"mlp_layers\"],\n",
    "        dropout=hyp[\"dropout\"],\n",
    "        num_classes=hyp[\"num_classes\"],\n",
    "        use_batchnorm=hyp[\"use_batchnorm\"],\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# ---- Optimizer & Scheduler helpers ----\n",
    "def make_gnn_optimizer(model: nn.Module, hyp: Dict) -> torch.optim.Optimizer:\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hyp[\"lr\"],\n",
    "        weight_decay=hyp[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "def make_scheduler(optimizer, warmup_steps: int, total_steps: int):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "# ---- Instantiate a default GNN model + optimizer/scheduler ----\n",
    "gnn_model = make_gnn_model(GNN_HYP, D_NODE)\n",
    "print(f\"GNN params: {count_params(gnn_model):,}\")\n",
    "\n",
    "gnn_optim = make_gnn_optimizer(gnn_model, GNN_HYP)\n",
    "gnn_scheduler = make_scheduler(gnn_optim, GNN_HYP[\"warmup_steps\"], GNN_HYP[\"total_steps\"])\n",
    "\n",
    "# Binary classification by default (y ∈ {0,1})\n",
    "gnn_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "\n",
    "print(\"GNN model, optimizer, scheduler, and loss are initialized.\")\n",
    "\n",
    "\n",
    "# ---- Hyperparameter sweep helper (config generator only) ----\n",
    "def iter_gnn_configs(base: Dict) -> Iterable[Dict]:\n",
    "    \"\"\"\n",
    "    Simple grid over a few key hyperparameters.\n",
    "    You can extend/prune this as needed.\n",
    "    \"\"\"\n",
    "    for lr in base[\"lr_grid\"]:\n",
    "        for hidden in base[\"hidden_dim_grid\"]:\n",
    "            for num_layers in base[\"num_layers_grid\"]:\n",
    "                for dropout in base[\"dropout_grid\"]:\n",
    "                    for aggr in base[\"aggr_grid\"]:\n",
    "                        for conv_type in base.get(\"conv_type_grid\", [base[\"conv_type\"]]):\n",
    "                            cfg = dict(base)  # shallow copy\n",
    "                            cfg[\"lr\"] = lr\n",
    "                            cfg[\"hidden_dim\"] = hidden\n",
    "                            cfg[\"num_layers\"] = num_layers\n",
    "                            cfg[\"dropout\"] = dropout\n",
    "                            cfg[\"aggr\"] = aggr\n",
    "                            cfg[\"conv_type\"] = conv_type\n",
    "                            yield cfg\n",
    "\n",
    "print(\"\\nInitialization\")\n",
    "for i, cfg in enumerate(iter_gnn_configs(GNN_HYP)):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    m = make_gnn_model(cfg, D_NODE)\n",
    "    print(\n",
    "        f\"  Config {i}: lr={cfg['lr']}, hidden={cfg['hidden_dim']}, \"\n",
    "        f\"layers={cfg['num_layers']}, dropout={cfg['dropout']}, \"\n",
    "        f\"aggr={cfg['aggr']}, conv_type={cfg['conv_type']}, \"\n",
    "        f\"params={count_params(m):,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdb14c-0f22-4a7f-97fc-c06bf43e041d",
   "metadata": {},
   "source": [
    "### Model Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbff31-a9a3-4872-b2a6-effa69065f94",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4fd7f-efc1-4d51-b5bf-d124492e7d12",
   "metadata": {},
   "source": [
    "##### Helper Functions\n",
    "\n",
    "These helpers implement a compact training/evaluation loop for a binary graph classifier and standard performance metrics. compute_binary_metrics takes ground-truth binary labels and predicted probabilities (post-sigmoid), thresholds them into hard predictions, and computes accuracy, precision, recall, and F1 score with small numerical safeguards. run_epoch iterates over a PyTorch Geometric DataLoader for one epoch in either training or evaluation mode, moves each batch to the target device, runs the model forward on graph data (x, edge_index, batch), computes a scalar loss against flattened labels using a given criterion (e.g., BCEWithLogitsLoss), and in training mode performs backpropagation, optimizer steps, and optional scheduler updates. It accumulates loss weighted by batch size to return an average epoch loss, collects all sigmoid probabilities and labels across batches, and finally calls compute_binary_metrics to produce a metrics dictionary augmented with the averaged loss, or returns zeros if no samples were seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41fb7f5c-cae6-4811-8b28-7f1e8f0795b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_binary_metrics(y_true: np.ndarray,\n",
    "                           y_prob: np.ndarray,\n",
    "                           threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    y_true: [N] in {0,1}\n",
    "    y_prob: [N] in [0,1]  (sigmoid outputs)\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64).ravel()\n",
    "    y_prob = y_prob.astype(np.float32).ravel()\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64)\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    eps = 1e-9\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    acc       = (tp + tn) / max(1, len(y_true))\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "    }\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion: nn.Module,\n",
    "              device: torch.device,\n",
    "              scheduler=None,\n",
    "              train: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Runs one epoch over loader.\n",
    "    Returns (avg_loss, metrics_dict).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_probs: List[float] = []\n",
    "    all_labels: List[int] = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        # y: [num_graphs] or [num_graphs, 1]\n",
    "        y = batch.y.view(-1).float()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)  # [num_graphs] or [num_graphs,1]\n",
    "            logits = logits.view_as(y)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits.detach()).cpu().numpy()\n",
    "        labels = y.detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        return 0.0, {\"acc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    metrics = compute_binary_metrics(all_labels, all_probs, threshold=0.5)\n",
    "    metrics[\"loss\"] = float(avg_loss)\n",
    "    return avg_loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43f972-6356-4b02-984b-5012b5ad0c4c",
   "metadata": {},
   "source": [
    "##### Training Loop (with Early Stop)\n",
    "\n",
    "This code implements an early-stopping mechanism and a high-level training loop for a single GNN configuration. The `EarlyStopper` class tracks a chosen validation metric (e.g., F1) over epochs, deciding whether to stop training when the metric fails to improve by at least `min_delta` for a specified number of epochs (`patience`), and it keeps a deep copy of the best model state encountered. The `train_gnn_model` function wires everything together for one run: it constructs a GNN using `make_gnn_model`, sets up an AdamW optimizer and cosine–decay scheduler via `make_gnn_optimizer` and `make_scheduler`, and uses a shared loss (`gnn_criterion`, typically `BCEWithLogitsLoss` for binary classification). For each epoch, it calls `run_epoch` on both the training and validation loaders, capturing losses and metrics, logs progress with train/val loss and F1, appends snapshots into a `history` dict, and feeds validation metrics into `EarlyStopper` to determine if early stopping should trigger. When training ends—either by exhausting the epoch budget or by early stop—the best-performing weights (if any) are restored onto the model, and a result dictionary is returned containing the trained model, metric history, the best monitored metric value, its name, and the corresponding `state_dict` for downstream evaluation or checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63299bb3-02b7-45cb-b437-23dd9b996ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, monitor: str = \"f1\", mode: str = \"max\",\n",
    "                 patience: int = 5, min_delta: float = 1e-3):\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.best = -float(\"inf\")\n",
    "        elif mode == \"min\":\n",
    "            self.best = float(\"inf\")\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'max' or 'min'\")\n",
    "\n",
    "        self.num_bad = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metrics: Dict[str, float], model: nn.Module) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if we should stop (patience exhausted).\n",
    "        \"\"\"\n",
    "        current = metrics.get(self.monitor, None)\n",
    "        if current is None:\n",
    "            return False\n",
    "\n",
    "        improved = False\n",
    "        if self.mode == \"max\":\n",
    "            if current > self.best + self.min_delta:\n",
    "                improved = True\n",
    "        else:  # 'min'\n",
    "            if current < self.best - self.min_delta:\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            self.best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "\n",
    "        return self.num_bad >= self.patience\n",
    "\n",
    "\n",
    "def train_gnn_model(\n",
    "    hyp: Dict,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    in_dim: int,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    High-level training loop for a single GNN config.\n",
    "    Returns a dict with best metrics, history, and best_state_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    model = make_gnn_model(hyp, in_dim=in_dim)\n",
    "    optimizer = make_gnn_optimizer(model, hyp)\n",
    "    scheduler = make_scheduler(optimizer, hyp[\"warmup_steps\"], hyp[\"total_steps\"])\n",
    "\n",
    "    criterion = gnn_criterion \n",
    "\n",
    "    es_cfg = hyp[\"early_stop\"]\n",
    "    early_stopper = EarlyStopper(\n",
    "        monitor=es_cfg.get(\"monitor\", \"f1\"),\n",
    "        mode=es_cfg.get(\"mode\", \"max\"),\n",
    "        patience=es_cfg.get(\"patience\", 5),\n",
    "        min_delta=es_cfg.get(\"min_delta\", 1e-3),\n",
    "    )\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    for epoch in range(1, hyp[\"epochs\"] + 1):\n",
    "        train_loss, train_metrics = run_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, scheduler, train=True\n",
    "        )\n",
    "        val_loss, val_metrics = run_epoch(\n",
    "            model, val_loader, optimizer, criterion, device, scheduler=None, train=False\n",
    "        )\n",
    "\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:03d}] \"\n",
    "            f\"train_loss={train_metrics['loss']:.4f}, train_f1={train_metrics['f1']:.3f} | \"\n",
    "            f\"val_loss={val_metrics['loss']:.4f}, val_f1={val_metrics['f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "        stop = early_stopper.step(val_metrics, model)\n",
    "        if stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Best {early_stopper.monitor} = {early_stopper.best:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights if we got any improvement\n",
    "    if early_stopper.best_state is not None:\n",
    "        model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    result = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"best_metric\": early_stopper.best,\n",
    "        \"best_monitor\": early_stopper.monitor,\n",
    "        \"best_state_dict\": early_stopper.best_state,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fdd40-69fd-44b7-b30e-41a905f649d2",
   "metadata": {},
   "source": [
    "##### Initiate Training\n",
    "\n",
    "This code performs a simple grid search over GNN hyperparameters and tracks the best-performing configuration based on validation metrics. It iterates through each candidate config generated by `iter_gnn_configs(GNN_HYP)`, prints a short summary of that config (convolution type, hidden dimension, number of layers, dropout, aggregation, and learning rate), and trains a model with `train_gnn_model` on the given train/validation loaders and node feature dimension. The result `out` from each run includes the best monitored metric (e.g., validation F1) for that configuration; the loop keeps `best_overall` and `best_cfg` updated whenever a configuration surpasses the current best metric. After all configurations have been evaluated, it prints the best hyperparameter dictionary and the corresponding best validation score, effectively selecting the strongest model configuration discovered during the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae6d6496-d571-4b5e-acde-c5c36ba86087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7448, train_f1=0.000 | val_loss=0.7011, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7092, train_f1=0.000 | val_loss=0.6753, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6385, train_f1=0.000 | val_loss=0.5761, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5223, train_f1=0.000 | val_loss=0.4332, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3577, train_f1=0.000 | val_loss=0.2211, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2160, train_f1=0.000 | val_loss=0.1255, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6901, train_f1=0.000 | val_loss=0.7423, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6493, train_f1=0.000 | val_loss=0.7568, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5687, train_f1=0.000 | val_loss=0.7092, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4417, train_f1=0.000 | val_loss=0.6321, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2875, train_f1=0.000 | val_loss=0.5438, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1779, train_f1=0.000 | val_loss=0.4415, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0002 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ml4g/term_project/.venv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] train_loss=0.7084, train_f1=0.000 | val_loss=0.6589, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6442, train_f1=0.000 | val_loss=0.4521, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5044, train_f1=0.000 | val_loss=0.3314, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2873, train_f1=0.000 | val_loss=0.2197, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1336, train_f1=0.000 | val_loss=0.1256, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0684, train_f1=0.000 | val_loss=0.0845, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7043, train_f1=0.000 | val_loss=0.6631, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6468, train_f1=0.000 | val_loss=0.5306, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5238, train_f1=0.000 | val_loss=0.3470, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3157, train_f1=0.000 | val_loss=0.1584, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1412, train_f1=0.000 | val_loss=0.0554, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0711, train_f1=0.000 | val_loss=0.0236, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6247, train_f1=0.000 | val_loss=0.6951, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5915, train_f1=0.000 | val_loss=0.6869, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5179, train_f1=0.000 | val_loss=0.5778, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4017, train_f1=0.000 | val_loss=0.4366, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2643, train_f1=0.000 | val_loss=0.2550, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1611, train_f1=0.000 | val_loss=0.1602, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7195, train_f1=0.000 | val_loss=0.7245, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6857, train_f1=0.000 | val_loss=0.6799, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6192, train_f1=0.000 | val_loss=0.5983, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5070, train_f1=0.000 | val_loss=0.4566, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3580, train_f1=0.000 | val_loss=0.3299, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2262, train_f1=0.000 | val_loss=0.2131, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7653, train_f1=0.000 | val_loss=0.7700, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7141, train_f1=0.000 | val_loss=0.7544, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6070, train_f1=0.000 | val_loss=0.6845, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4102, train_f1=0.000 | val_loss=0.5931, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2107, train_f1=0.000 | val_loss=0.4561, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1132, train_f1=0.000 | val_loss=0.3085, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.8184, train_f1=0.000 | val_loss=0.7451, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7452, train_f1=0.000 | val_loss=0.7383, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6285, train_f1=0.000 | val_loss=0.6131, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4522, train_f1=0.000 | val_loss=0.3939, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2464, train_f1=0.000 | val_loss=0.1682, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1270, train_f1=0.000 | val_loss=0.0672, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7272, train_f1=0.000 | val_loss=0.7327, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6913, train_f1=0.000 | val_loss=0.7350, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6302, train_f1=0.000 | val_loss=0.6850, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5332, train_f1=0.000 | val_loss=0.6065, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.4023, train_f1=0.000 | val_loss=0.5207, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2750, train_f1=0.000 | val_loss=0.4368, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7097, train_f1=0.000 | val_loss=0.7022, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6772, train_f1=0.000 | val_loss=0.6761, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6212, train_f1=0.000 | val_loss=0.6106, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5389, train_f1=0.000 | val_loss=0.5311, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.4193, train_f1=0.000 | val_loss=0.4256, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2937, train_f1=0.000 | val_loss=0.3385, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7468, train_f1=0.000 | val_loss=0.7119, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7021, train_f1=0.000 | val_loss=0.6747, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6204, train_f1=0.000 | val_loss=0.5903, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4585, train_f1=0.000 | val_loss=0.4236, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2559, train_f1=0.000 | val_loss=0.2305, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1281, train_f1=0.000 | val_loss=0.0961, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7561, train_f1=0.000 | val_loss=0.7954, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7061, train_f1=0.000 | val_loss=0.9038, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5869, train_f1=0.000 | val_loss=0.7884, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4033, train_f1=0.000 | val_loss=0.5371, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2332, train_f1=0.000 | val_loss=0.3302, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1441, train_f1=0.000 | val_loss=0.2118, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7063, train_f1=0.000 | val_loss=0.7337, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6620, train_f1=0.000 | val_loss=0.6641, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5723, train_f1=0.000 | val_loss=0.5293, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4246, train_f1=0.000 | val_loss=0.3424, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2452, train_f1=0.000 | val_loss=0.1805, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1301, train_f1=0.000 | val_loss=0.0831, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7154, train_f1=0.000 | val_loss=0.6048, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6725, train_f1=0.000 | val_loss=0.6258, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5888, train_f1=0.000 | val_loss=0.5825, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4484, train_f1=0.000 | val_loss=0.4709, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2585, train_f1=0.000 | val_loss=0.3012, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1361, train_f1=0.000 | val_loss=0.1364, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7141, train_f1=0.000 | val_loss=0.6931, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6315, train_f1=0.000 | val_loss=0.4163, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4791, train_f1=0.000 | val_loss=0.2866, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2717, train_f1=0.000 | val_loss=0.1094, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1162, train_f1=0.000 | val_loss=0.0443, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0524, train_f1=0.000 | val_loss=0.0170, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6742, train_f1=0.000 | val_loss=0.6664, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6097, train_f1=0.000 | val_loss=0.5230, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4785, train_f1=0.000 | val_loss=0.3932, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2865, train_f1=0.000 | val_loss=0.1884, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1341, train_f1=0.000 | val_loss=0.1062, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0667, train_f1=0.000 | val_loss=0.0408, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6568, train_f1=0.000 | val_loss=0.6776, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6188, train_f1=0.000 | val_loss=0.5514, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5408, train_f1=0.000 | val_loss=0.4662, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4105, train_f1=0.000 | val_loss=0.3098, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2508, train_f1=0.000 | val_loss=0.2058, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1403, train_f1=0.000 | val_loss=0.0707, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6884, train_f1=0.000 | val_loss=0.6882, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6496, train_f1=0.000 | val_loss=0.6189, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5764, train_f1=0.000 | val_loss=0.5321, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4443, train_f1=0.000 | val_loss=0.4020, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2859, train_f1=0.000 | val_loss=0.2245, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1685, train_f1=0.000 | val_loss=0.1127, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6944, train_f1=0.000 | val_loss=0.7288, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6383, train_f1=0.000 | val_loss=0.5862, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4992, train_f1=0.000 | val_loss=0.4259, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2848, train_f1=0.000 | val_loss=0.1908, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1306, train_f1=0.000 | val_loss=0.0668, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0670, train_f1=0.000 | val_loss=0.0177, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7369, train_f1=0.000 | val_loss=0.7176, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6868, train_f1=0.000 | val_loss=0.7211, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5819, train_f1=0.000 | val_loss=0.5324, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4006, train_f1=0.000 | val_loss=0.2481, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2040, train_f1=0.000 | val_loss=0.0782, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0989, train_f1=0.000 | val_loss=0.0258, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7460, train_f1=0.000 | val_loss=0.6939, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7098, train_f1=0.000 | val_loss=0.6946, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6658, train_f1=0.000 | val_loss=0.6382, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.6064, train_f1=0.000 | val_loss=0.5548, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.5139, train_f1=0.000 | val_loss=0.4638, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.4051, train_f1=0.000 | val_loss=0.3199, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7254, train_f1=0.000 | val_loss=0.7960, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6997, train_f1=0.000 | val_loss=0.6905, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6418, train_f1=0.000 | val_loss=0.6012, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5689, train_f1=0.000 | val_loss=0.4816, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.4731, train_f1=0.000 | val_loss=0.3683, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.3670, train_f1=0.000 | val_loss=0.2475, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7292, train_f1=0.000 | val_loss=0.6920, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6898, train_f1=0.000 | val_loss=0.6020, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6092, train_f1=0.000 | val_loss=0.5156, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4675, train_f1=0.000 | val_loss=0.3574, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2832, train_f1=0.000 | val_loss=0.1906, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1550, train_f1=0.000 | val_loss=0.0991, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7009, train_f1=0.000 | val_loss=0.7235, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6689, train_f1=0.000 | val_loss=0.7085, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5926, train_f1=0.000 | val_loss=0.6483, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4702, train_f1=0.000 | val_loss=0.4878, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3010, train_f1=0.000 | val_loss=0.2311, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1728, train_f1=0.000 | val_loss=0.0919, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6439, train_f1=0.000 | val_loss=0.6261, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5910, train_f1=0.000 | val_loss=0.5848, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4985, train_f1=0.000 | val_loss=0.4677, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3822, train_f1=0.000 | val_loss=0.3873, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2446, train_f1=0.000 | val_loss=0.2116, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1424, train_f1=0.000 | val_loss=0.1541, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6585, train_f1=0.000 | val_loss=0.7350, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6152, train_f1=0.000 | val_loss=0.5607, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5328, train_f1=0.000 | val_loss=0.4377, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3866, train_f1=0.000 | val_loss=0.2508, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2068, train_f1=0.000 | val_loss=0.1207, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1102, train_f1=0.000 | val_loss=0.0758, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7243, train_f1=0.000 | val_loss=0.7166, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6413, train_f1=0.000 | val_loss=0.5922, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5041, train_f1=0.000 | val_loss=0.4179, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2987, train_f1=0.000 | val_loss=0.1811, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1257, train_f1=0.000 | val_loss=0.0923, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0560, train_f1=0.000 | val_loss=0.0502, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.8262, train_f1=0.000 | val_loss=0.6968, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7453, train_f1=0.000 | val_loss=0.6860, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5842, train_f1=0.000 | val_loss=0.4650, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3465, train_f1=0.000 | val_loss=0.1881, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1516, train_f1=0.000 | val_loss=0.0629, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0674, train_f1=0.000 | val_loss=0.0323, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6267, train_f1=0.000 | val_loss=0.5878, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5990, train_f1=0.000 | val_loss=0.6314, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5328, train_f1=0.000 | val_loss=0.5394, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4279, train_f1=0.000 | val_loss=0.4209, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2810, train_f1=0.000 | val_loss=0.2310, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1652, train_f1=0.000 | val_loss=0.1243, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7294, train_f1=0.000 | val_loss=0.8034, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7047, train_f1=0.000 | val_loss=0.7777, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6481, train_f1=0.000 | val_loss=0.7068, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5455, train_f1=0.000 | val_loss=0.5948, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3819, train_f1=0.000 | val_loss=0.4401, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2332, train_f1=0.000 | val_loss=0.2613, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6819, train_f1=0.000 | val_loss=0.6743, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6357, train_f1=0.000 | val_loss=0.6101, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5297, train_f1=0.000 | val_loss=0.4819, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3434, train_f1=0.000 | val_loss=0.1900, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1606, train_f1=0.000 | val_loss=0.0687, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.000 | val_loss=0.0376, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6951, train_f1=0.000 | val_loss=0.6769, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6588, train_f1=0.000 | val_loss=0.5690, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5758, train_f1=0.000 | val_loss=0.3705, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3724, train_f1=0.000 | val_loss=0.1482, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1548, train_f1=0.000 | val_loss=0.0372, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0611, train_f1=0.000 | val_loss=0.0190, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6504, train_f1=0.000 | val_loss=0.6676, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6281, train_f1=0.000 | val_loss=0.6740, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5907, train_f1=0.000 | val_loss=0.6587, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5342, train_f1=0.000 | val_loss=0.6052, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.4508, train_f1=0.000 | val_loss=0.5314, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.3526, train_f1=0.000 | val_loss=0.4297, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7777, train_f1=0.000 | val_loss=0.6858, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7534, train_f1=0.000 | val_loss=0.6629, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.7094, train_f1=0.000 | val_loss=0.6061, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.6420, train_f1=0.000 | val_loss=0.5234, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.5532, train_f1=0.000 | val_loss=0.3746, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.4527, train_f1=0.000 | val_loss=0.2605, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6929, train_f1=0.000 | val_loss=0.6474, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6580, train_f1=0.000 | val_loss=0.6193, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5991, train_f1=0.000 | val_loss=0.5504, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5024, train_f1=0.000 | val_loss=0.4208, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3448, train_f1=0.000 | val_loss=0.1892, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1983, train_f1=0.000 | val_loss=0.1009, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6792, train_f1=0.000 | val_loss=0.6134, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6559, train_f1=0.000 | val_loss=0.5431, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5910, train_f1=0.000 | val_loss=0.5197, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4972, train_f1=0.000 | val_loss=0.4504, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3564, train_f1=0.000 | val_loss=0.3512, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2287, train_f1=0.000 | val_loss=0.2620, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6293, train_f1=0.000 | val_loss=0.6464, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5701, train_f1=0.000 | val_loss=0.5243, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4358, train_f1=0.000 | val_loss=0.3935, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2428, train_f1=0.000 | val_loss=0.1658, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1106, train_f1=0.000 | val_loss=0.0789, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0576, train_f1=0.000 | val_loss=0.0528, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6425, train_f1=0.000 | val_loss=0.6480, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5745, train_f1=0.000 | val_loss=0.5200, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4297, train_f1=0.000 | val_loss=0.3439, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2341, train_f1=0.000 | val_loss=0.1508, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1089, train_f1=0.000 | val_loss=0.0536, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0534, train_f1=0.000 | val_loss=0.0291, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7485, train_f1=0.000 | val_loss=0.7774, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6403, train_f1=0.000 | val_loss=0.6756, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3885, train_f1=0.000 | val_loss=0.4371, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1549, train_f1=0.000 | val_loss=0.2038, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0637, train_f1=0.000 | val_loss=0.1100, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0289, train_f1=0.000 | val_loss=0.0659, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6912, train_f1=0.000 | val_loss=0.6639, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6012, train_f1=0.000 | val_loss=0.5595, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3874, train_f1=0.000 | val_loss=0.3485, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1584, train_f1=0.000 | val_loss=0.1365, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0607, train_f1=0.000 | val_loss=0.0674, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0294, train_f1=0.000 | val_loss=0.0251, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6848, train_f1=0.000 | val_loss=0.6031, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6260, train_f1=0.000 | val_loss=0.5076, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5106, train_f1=0.000 | val_loss=0.4218, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3343, train_f1=0.000 | val_loss=0.2995, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1665, train_f1=0.000 | val_loss=0.1735, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0906, train_f1=0.000 | val_loss=0.0854, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6860, train_f1=0.000 | val_loss=0.6747, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6220, train_f1=0.000 | val_loss=0.6737, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4984, train_f1=0.000 | val_loss=0.5398, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3153, train_f1=0.000 | val_loss=0.3359, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1574, train_f1=0.000 | val_loss=0.1443, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0850, train_f1=0.000 | val_loss=0.0512, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6164, train_f1=0.000 | val_loss=0.6735, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5211, train_f1=0.000 | val_loss=0.5448, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3189, train_f1=0.000 | val_loss=0.2839, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1310, train_f1=0.000 | val_loss=0.0637, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0463, train_f1=0.000 | val_loss=0.0304, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0251, train_f1=0.000 | val_loss=0.0100, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6389, train_f1=0.000 | val_loss=0.6166, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5450, train_f1=0.000 | val_loss=0.3923, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3532, train_f1=0.000 | val_loss=0.2051, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1454, train_f1=0.000 | val_loss=0.0584, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0565, train_f1=0.000 | val_loss=0.0299, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0308, train_f1=0.000 | val_loss=0.0137, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6092, train_f1=0.000 | val_loss=0.6584, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5756, train_f1=0.000 | val_loss=0.6000, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4915, train_f1=0.000 | val_loss=0.5033, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3512, train_f1=0.000 | val_loss=0.3442, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1899, train_f1=0.000 | val_loss=0.1826, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1017, train_f1=0.000 | val_loss=0.0784, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7383, train_f1=0.000 | val_loss=0.6916, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6858, train_f1=0.000 | val_loss=0.6389, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5818, train_f1=0.000 | val_loss=0.5556, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4085, train_f1=0.000 | val_loss=0.4047, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2194, train_f1=0.000 | val_loss=0.2304, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1172, train_f1=0.000 | val_loss=0.1553, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7357, train_f1=0.000 | val_loss=0.6806, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6379, train_f1=0.000 | val_loss=0.5904, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4514, train_f1=0.000 | val_loss=0.4711, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2339, train_f1=0.000 | val_loss=0.2393, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1053, train_f1=0.000 | val_loss=0.1212, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0588, train_f1=0.000 | val_loss=0.0549, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7005, train_f1=0.000 | val_loss=0.6333, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6344, train_f1=0.000 | val_loss=0.5570, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4980, train_f1=0.000 | val_loss=0.3730, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2753, train_f1=0.000 | val_loss=0.1677, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1230, train_f1=0.000 | val_loss=0.0558, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0673, train_f1=0.000 | val_loss=0.0264, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7696, train_f1=0.000 | val_loss=0.8046, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6636, train_f1=0.000 | val_loss=0.7540, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4824, train_f1=0.000 | val_loss=0.5163, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2824, train_f1=0.000 | val_loss=0.2397, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1322, train_f1=0.000 | val_loss=0.0954, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0660, train_f1=0.000 | val_loss=0.0495, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7304, train_f1=0.000 | val_loss=0.7287, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6493, train_f1=0.000 | val_loss=0.6039, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4651, train_f1=0.000 | val_loss=0.3145, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2291, train_f1=0.000 | val_loss=0.1169, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0959, train_f1=0.000 | val_loss=0.0659, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0496, train_f1=0.000 | val_loss=0.0361, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7362, train_f1=0.000 | val_loss=0.7368, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5854, train_f1=0.000 | val_loss=0.4973, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3082, train_f1=0.000 | val_loss=0.1814, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1063, train_f1=0.000 | val_loss=0.0446, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0423, train_f1=0.000 | val_loss=0.0111, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0251, train_f1=0.000 | val_loss=0.0086, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7033, train_f1=0.000 | val_loss=0.6449, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5591, train_f1=0.000 | val_loss=0.3653, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2939, train_f1=0.000 | val_loss=0.1197, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0982, train_f1=0.000 | val_loss=0.0246, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0351, train_f1=0.000 | val_loss=0.0083, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0198, train_f1=0.000 | val_loss=0.0037, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7411, train_f1=0.000 | val_loss=0.6539, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6654, train_f1=0.000 | val_loss=0.5071, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5052, train_f1=0.000 | val_loss=0.3051, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2819, train_f1=0.000 | val_loss=0.1285, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1245, train_f1=0.000 | val_loss=0.0404, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0672, train_f1=0.000 | val_loss=0.0297, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6996, train_f1=0.000 | val_loss=0.6954, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6484, train_f1=0.000 | val_loss=0.7091, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5283, train_f1=0.000 | val_loss=0.6308, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3230, train_f1=0.000 | val_loss=0.3609, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1396, train_f1=0.000 | val_loss=0.0907, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0688, train_f1=0.000 | val_loss=0.0751, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7049, train_f1=0.000 | val_loss=0.6066, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6002, train_f1=0.000 | val_loss=0.4052, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3568, train_f1=0.000 | val_loss=0.1407, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1361, train_f1=0.000 | val_loss=0.0291, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0471, train_f1=0.000 | val_loss=0.0140, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0284, train_f1=0.000 | val_loss=0.0082, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6526, train_f1=0.000 | val_loss=0.6534, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5649, train_f1=0.000 | val_loss=0.4297, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3761, train_f1=0.000 | val_loss=0.2054, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1548, train_f1=0.000 | val_loss=0.0582, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0587, train_f1=0.000 | val_loss=0.0201, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0306, train_f1=0.000 | val_loss=0.0108, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7727, train_f1=0.000 | val_loss=0.7192, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7303, train_f1=0.000 | val_loss=0.7206, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6556, train_f1=0.000 | val_loss=0.6041, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4923, train_f1=0.000 | val_loss=0.3843, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2787, train_f1=0.000 | val_loss=0.1606, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1417, train_f1=0.000 | val_loss=0.0631, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6149, train_f1=0.000 | val_loss=0.5899, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5830, train_f1=0.000 | val_loss=0.5434, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5123, train_f1=0.000 | val_loss=0.4856, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3786, train_f1=0.000 | val_loss=0.3582, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2153, train_f1=0.000 | val_loss=0.2132, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1089, train_f1=0.000 | val_loss=0.0914, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7220, train_f1=0.000 | val_loss=0.7705, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6538, train_f1=0.000 | val_loss=0.5945, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4793, train_f1=0.000 | val_loss=0.3473, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2374, train_f1=0.000 | val_loss=0.0869, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1029, train_f1=0.000 | val_loss=0.0185, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0535, train_f1=0.000 | val_loss=0.0070, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6583, train_f1=0.000 | val_loss=0.6125, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6135, train_f1=0.000 | val_loss=0.4110, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5211, train_f1=0.000 | val_loss=0.3169, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3182, train_f1=0.000 | val_loss=0.1251, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1378, train_f1=0.000 | val_loss=0.0518, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0625, train_f1=0.000 | val_loss=0.0204, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6259, train_f1=0.000 | val_loss=0.6008, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5299, train_f1=0.000 | val_loss=0.3680, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3884, train_f1=0.000 | val_loss=0.2235, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2219, train_f1=0.000 | val_loss=0.1156, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0995, train_f1=0.000 | val_loss=0.0517, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0487, train_f1=0.000 | val_loss=0.0275, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7615, train_f1=0.000 | val_loss=0.8922, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6680, train_f1=0.000 | val_loss=0.6977, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4979, train_f1=0.000 | val_loss=0.4828, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2616, train_f1=0.000 | val_loss=0.1698, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0992, train_f1=0.000 | val_loss=0.0841, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0462, train_f1=0.000 | val_loss=0.0483, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6742, train_f1=0.000 | val_loss=0.6509, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5158, train_f1=0.000 | val_loss=0.2497, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2563, train_f1=0.000 | val_loss=0.0986, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0851, train_f1=0.000 | val_loss=0.0329, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0329, train_f1=0.000 | val_loss=0.0242, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0144, train_f1=0.000 | val_loss=0.0084, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6475, train_f1=0.000 | val_loss=0.6563, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5198, train_f1=0.000 | val_loss=0.3639, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2745, train_f1=0.000 | val_loss=0.1633, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0898, train_f1=0.000 | val_loss=0.0550, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0316, train_f1=0.000 | val_loss=0.0230, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0175, train_f1=0.000 | val_loss=0.0140, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6398, train_f1=0.000 | val_loss=0.7201, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5477, train_f1=0.000 | val_loss=0.4854, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3779, train_f1=0.000 | val_loss=0.2122, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1940, train_f1=0.000 | val_loss=0.0829, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0877, train_f1=0.000 | val_loss=0.0340, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0492, train_f1=0.000 | val_loss=0.0235, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6887, train_f1=0.000 | val_loss=0.7141, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6433, train_f1=0.000 | val_loss=0.6433, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5162, train_f1=0.000 | val_loss=0.4456, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2983, train_f1=0.000 | val_loss=0.1408, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1201, train_f1=0.000 | val_loss=0.0495, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0556, train_f1=0.000 | val_loss=0.0264, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6992, train_f1=0.000 | val_loss=0.6732, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6032, train_f1=0.000 | val_loss=0.6185, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4004, train_f1=0.000 | val_loss=0.3679, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1624, train_f1=0.000 | val_loss=0.0930, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0576, train_f1=0.000 | val_loss=0.0181, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0291, train_f1=0.000 | val_loss=0.0144, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7182, train_f1=0.000 | val_loss=0.6346, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6520, train_f1=0.000 | val_loss=0.5276, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4856, train_f1=0.000 | val_loss=0.3987, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2094, train_f1=0.000 | val_loss=0.1072, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0622, train_f1=0.000 | val_loss=0.0715, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0313, train_f1=0.000 | val_loss=0.0267, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6314, train_f1=0.000 | val_loss=0.5806, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6035, train_f1=0.000 | val_loss=0.4971, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5421, train_f1=0.000 | val_loss=0.4446, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4240, train_f1=0.000 | val_loss=0.4291, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2710, train_f1=0.000 | val_loss=0.3760, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1586, train_f1=0.000 | val_loss=0.3075, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.6777, train_f1=0.000 | val_loss=0.6604, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6518, train_f1=0.000 | val_loss=0.5615, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5947, train_f1=0.000 | val_loss=0.4988, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5021, train_f1=0.000 | val_loss=0.4295, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3475, train_f1=0.000 | val_loss=0.3154, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1957, train_f1=0.000 | val_loss=0.1627, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7322, train_f1=0.000 | val_loss=0.6836, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6855, train_f1=0.000 | val_loss=0.7034, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5945, train_f1=0.000 | val_loss=0.5500, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3597, train_f1=0.000 | val_loss=0.2276, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1391, train_f1=0.000 | val_loss=0.0481, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0605, train_f1=0.000 | val_loss=0.0224, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.7000, train_f1=0.000 | val_loss=0.6768, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6684, train_f1=0.000 | val_loss=0.6114, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5953, train_f1=0.000 | val_loss=0.5371, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4497, train_f1=0.000 | val_loss=0.3904, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2384, train_f1=0.000 | val_loss=0.1954, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1090, train_f1=0.000 | val_loss=0.0750, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7113, train_f1=0.000 | val_loss=0.7227, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6537, train_f1=0.000 | val_loss=0.6211, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5152, train_f1=0.000 | val_loss=0.4552, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2901, train_f1=0.000 | val_loss=0.2280, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1237, train_f1=0.000 | val_loss=0.0637, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0636, train_f1=0.000 | val_loss=0.0329, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6423, train_f1=0.000 | val_loss=0.6268, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5874, train_f1=0.000 | val_loss=0.5876, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4675, train_f1=0.000 | val_loss=0.5010, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2738, train_f1=0.000 | val_loss=0.3346, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1202, train_f1=0.000 | val_loss=0.2224, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0582, train_f1=0.000 | val_loss=0.1297, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7511, train_f1=0.000 | val_loss=0.7218, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6604, train_f1=0.000 | val_loss=0.5672, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4736, train_f1=0.000 | val_loss=0.3087, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2164, train_f1=0.000 | val_loss=0.0900, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0738, train_f1=0.000 | val_loss=0.0252, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0361, train_f1=0.000 | val_loss=0.0125, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7002, train_f1=0.000 | val_loss=0.7543, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6129, train_f1=0.000 | val_loss=0.7379, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4352, train_f1=0.000 | val_loss=0.4993, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1982, train_f1=0.000 | val_loss=0.1673, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0689, train_f1=0.000 | val_loss=0.0602, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0328, train_f1=0.000 | val_loss=0.0214, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7833, train_f1=0.000 | val_loss=0.7314, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7271, train_f1=0.000 | val_loss=0.6310, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6139, train_f1=0.000 | val_loss=0.5516, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4258, train_f1=0.000 | val_loss=0.3464, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2174, train_f1=0.000 | val_loss=0.1317, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1018, train_f1=0.000 | val_loss=0.0531, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6614, train_f1=0.000 | val_loss=0.6544, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6139, train_f1=0.000 | val_loss=0.5869, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5102, train_f1=0.000 | val_loss=0.4509, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3506, train_f1=0.000 | val_loss=0.2695, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1775, train_f1=0.000 | val_loss=0.1808, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0886, train_f1=0.000 | val_loss=0.0880, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6624, train_f1=0.000 | val_loss=0.6907, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5872, train_f1=0.000 | val_loss=0.5997, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4357, train_f1=0.000 | val_loss=0.3793, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2233, train_f1=0.000 | val_loss=0.1856, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0845, train_f1=0.000 | val_loss=0.0672, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0418, train_f1=0.000 | val_loss=0.0317, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7311, train_f1=0.000 | val_loss=0.6729, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6416, train_f1=0.000 | val_loss=0.4412, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4648, train_f1=0.000 | val_loss=0.2545, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2350, train_f1=0.000 | val_loss=0.1000, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0927, train_f1=0.000 | val_loss=0.0218, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0480, train_f1=0.000 | val_loss=0.0050, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7523, train_f1=0.000 | val_loss=0.6860, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6938, train_f1=0.000 | val_loss=0.5226, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5855, train_f1=0.000 | val_loss=0.4077, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4094, train_f1=0.000 | val_loss=0.2660, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2252, train_f1=0.000 | val_loss=0.1330, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1270, train_f1=0.000 | val_loss=0.0618, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6874, train_f1=0.000 | val_loss=0.7477, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6448, train_f1=0.000 | val_loss=0.7036, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5484, train_f1=0.000 | val_loss=0.6157, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3956, train_f1=0.000 | val_loss=0.4704, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2333, train_f1=0.000 | val_loss=0.2654, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1282, train_f1=0.000 | val_loss=0.1818, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7012, train_f1=0.000 | val_loss=0.6414, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6266, train_f1=0.000 | val_loss=0.5915, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4736, train_f1=0.000 | val_loss=0.4782, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2423, train_f1=0.000 | val_loss=0.2727, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1022, train_f1=0.000 | val_loss=0.1411, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0542, train_f1=0.000 | val_loss=0.0634, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6333, train_f1=0.000 | val_loss=0.6383, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5594, train_f1=0.000 | val_loss=0.5189, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3987, train_f1=0.000 | val_loss=0.2411, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1898, train_f1=0.000 | val_loss=0.0779, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0812, train_f1=0.000 | val_loss=0.0225, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0419, train_f1=0.000 | val_loss=0.0085, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7391, train_f1=0.000 | val_loss=0.7267, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6616, train_f1=0.000 | val_loss=0.4631, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5148, train_f1=0.000 | val_loss=0.3208, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2984, train_f1=0.000 | val_loss=0.1429, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1215, train_f1=0.000 | val_loss=0.0443, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0576, train_f1=0.000 | val_loss=0.0268, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6457, train_f1=0.000 | val_loss=0.6240, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5872, train_f1=0.000 | val_loss=0.5552, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4428, train_f1=0.000 | val_loss=0.3783, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2355, train_f1=0.000 | val_loss=0.1332, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0940, train_f1=0.000 | val_loss=0.0518, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0449, train_f1=0.000 | val_loss=0.0208, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6630, train_f1=0.000 | val_loss=0.6244, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5614, train_f1=0.000 | val_loss=0.3591, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3468, train_f1=0.000 | val_loss=0.2315, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1269, train_f1=0.000 | val_loss=0.0563, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0459, train_f1=0.000 | val_loss=0.0212, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0222, train_f1=0.000 | val_loss=0.0111, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7527, train_f1=0.000 | val_loss=0.6730, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6726, train_f1=0.000 | val_loss=0.4386, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4939, train_f1=0.000 | val_loss=0.2187, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2242, train_f1=0.000 | val_loss=0.0946, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0705, train_f1=0.000 | val_loss=0.0362, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0309, train_f1=0.000 | val_loss=0.0153, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6684, train_f1=0.000 | val_loss=0.6794, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6194, train_f1=0.000 | val_loss=0.6194, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5268, train_f1=0.000 | val_loss=0.5105, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3659, train_f1=0.000 | val_loss=0.2976, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1871, train_f1=0.000 | val_loss=0.1467, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0960, train_f1=0.000 | val_loss=0.0808, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7219, train_f1=0.000 | val_loss=0.6472, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6698, train_f1=0.000 | val_loss=0.5140, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5647, train_f1=0.000 | val_loss=0.4328, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3748, train_f1=0.000 | val_loss=0.2415, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1806, train_f1=0.000 | val_loss=0.1066, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0867, train_f1=0.000 | val_loss=0.0364, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6843, train_f1=0.000 | val_loss=0.6358, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6100, train_f1=0.000 | val_loss=0.4900, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4589, train_f1=0.000 | val_loss=0.3871, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2359, train_f1=0.000 | val_loss=0.2807, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0889, train_f1=0.000 | val_loss=0.1364, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0402, train_f1=0.000 | val_loss=0.0929, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7594, train_f1=0.000 | val_loss=0.7085, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6855, train_f1=0.000 | val_loss=0.7025, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5296, train_f1=0.000 | val_loss=0.5640, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2770, train_f1=0.000 | val_loss=0.3145, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1094, train_f1=0.000 | val_loss=0.1198, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0469, train_f1=0.000 | val_loss=0.0478, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7201, train_f1=0.000 | val_loss=0.7167, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6764, train_f1=0.000 | val_loss=0.6995, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5953, train_f1=0.000 | val_loss=0.6543, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4518, train_f1=0.000 | val_loss=0.4747, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2722, train_f1=0.000 | val_loss=0.2042, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1502, train_f1=0.000 | val_loss=0.0561, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6969, train_f1=0.000 | val_loss=0.7083, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6582, train_f1=0.000 | val_loss=0.6758, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6015, train_f1=0.000 | val_loss=0.6080, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4900, train_f1=0.000 | val_loss=0.5292, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3354, train_f1=0.000 | val_loss=0.3859, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1894, train_f1=0.000 | val_loss=0.2167, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6610, train_f1=0.000 | val_loss=0.6688, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6134, train_f1=0.000 | val_loss=0.5835, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5060, train_f1=0.000 | val_loss=0.4778, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3269, train_f1=0.000 | val_loss=0.3261, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1537, train_f1=0.000 | val_loss=0.1445, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0723, train_f1=0.000 | val_loss=0.0717, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7150, train_f1=0.000 | val_loss=0.6788, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6724, train_f1=0.000 | val_loss=0.5877, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5667, train_f1=0.000 | val_loss=0.4858, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3814, train_f1=0.000 | val_loss=0.2490, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1828, train_f1=0.000 | val_loss=0.0721, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0859, train_f1=0.000 | val_loss=0.0129, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7216, train_f1=0.000 | val_loss=0.7580, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6384, train_f1=0.000 | val_loss=0.5021, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4851, train_f1=0.000 | val_loss=0.3771, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2618, train_f1=0.000 | val_loss=0.1409, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0941, train_f1=0.000 | val_loss=0.0662, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0446, train_f1=0.000 | val_loss=0.0423, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6115, train_f1=0.000 | val_loss=0.5228, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5455, train_f1=0.000 | val_loss=0.5063, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4115, train_f1=0.000 | val_loss=0.3209, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2345, train_f1=0.000 | val_loss=0.0945, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1019, train_f1=0.000 | val_loss=0.0341, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0444, train_f1=0.000 | val_loss=0.0201, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7740, train_f1=0.000 | val_loss=0.8076, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6636, train_f1=0.000 | val_loss=0.6446, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4166, train_f1=0.000 | val_loss=0.2977, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1396, train_f1=0.000 | val_loss=0.0569, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0502, train_f1=0.000 | val_loss=0.0189, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0242, train_f1=0.000 | val_loss=0.0093, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7648, train_f1=0.000 | val_loss=0.6859, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6658, train_f1=0.000 | val_loss=0.4982, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4750, train_f1=0.000 | val_loss=0.3634, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2027, train_f1=0.000 | val_loss=0.1805, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0633, train_f1=0.000 | val_loss=0.0655, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0258, train_f1=0.000 | val_loss=0.0420, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7346, train_f1=0.000 | val_loss=0.6878, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6660, train_f1=0.000 | val_loss=0.4648, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5448, train_f1=0.000 | val_loss=0.3341, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3667, train_f1=0.000 | val_loss=0.1424, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1866, train_f1=0.000 | val_loss=0.0704, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0945, train_f1=0.000 | val_loss=0.0257, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7373, train_f1=0.000 | val_loss=0.6352, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6875, train_f1=0.000 | val_loss=0.5488, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5909, train_f1=0.000 | val_loss=0.4330, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4063, train_f1=0.000 | val_loss=0.2402, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1848, train_f1=0.000 | val_loss=0.0835, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0761, train_f1=0.000 | val_loss=0.0268, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6886, train_f1=0.000 | val_loss=0.7663, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6196, train_f1=0.000 | val_loss=0.6568, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4518, train_f1=0.000 | val_loss=0.3619, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2055, train_f1=0.000 | val_loss=0.0730, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0707, train_f1=0.000 | val_loss=0.0283, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0338, train_f1=0.000 | val_loss=0.0111, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6716, train_f1=0.000 | val_loss=0.6538, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6138, train_f1=0.000 | val_loss=0.5772, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4670, train_f1=0.000 | val_loss=0.3470, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2154, train_f1=0.000 | val_loss=0.1144, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0701, train_f1=0.000 | val_loss=0.0236, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0336, train_f1=0.000 | val_loss=0.0106, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7537, train_f1=0.000 | val_loss=0.7353, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7245, train_f1=0.000 | val_loss=0.7133, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6585, train_f1=0.000 | val_loss=0.6676, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5552, train_f1=0.000 | val_loss=0.5487, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.3951, train_f1=0.000 | val_loss=0.3451, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2303, train_f1=0.000 | val_loss=0.1508, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7472, train_f1=0.000 | val_loss=0.7140, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7214, train_f1=0.000 | val_loss=0.6735, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6595, train_f1=0.000 | val_loss=0.5832, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.5536, train_f1=0.000 | val_loss=0.4145, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.4044, train_f1=0.000 | val_loss=0.1998, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.2236, train_f1=0.000 | val_loss=0.0683, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7063, train_f1=0.000 | val_loss=0.7443, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6511, train_f1=0.000 | val_loss=0.7186, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5449, train_f1=0.000 | val_loss=0.6432, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3558, train_f1=0.000 | val_loss=0.4334, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1761, train_f1=0.000 | val_loss=0.2113, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0809, train_f1=0.000 | val_loss=0.0591, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6762, train_f1=0.000 | val_loss=0.6274, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6340, train_f1=0.000 | val_loss=0.5843, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5609, train_f1=0.000 | val_loss=0.5470, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4344, train_f1=0.000 | val_loss=0.4768, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2491, train_f1=0.000 | val_loss=0.2467, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1117, train_f1=0.000 | val_loss=0.1283, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6084, train_f1=0.000 | val_loss=0.5892, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5344, train_f1=0.000 | val_loss=0.3234, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3584, train_f1=0.000 | val_loss=0.1696, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1472, train_f1=0.000 | val_loss=0.0439, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0516, train_f1=0.000 | val_loss=0.0142, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0261, train_f1=0.000 | val_loss=0.0029, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7423, train_f1=0.000 | val_loss=0.6894, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6454, train_f1=0.000 | val_loss=0.5535, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4508, train_f1=0.000 | val_loss=0.3153, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2040, train_f1=0.000 | val_loss=0.0887, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0726, train_f1=0.000 | val_loss=0.0185, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0340, train_f1=0.000 | val_loss=0.0099, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6542, train_f1=0.000 | val_loss=0.6383, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5043, train_f1=0.000 | val_loss=0.3622, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2194, train_f1=0.000 | val_loss=0.0755, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0658, train_f1=0.000 | val_loss=0.0124, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0268, train_f1=0.000 | val_loss=0.0044, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0135, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6084, train_f1=0.000 | val_loss=0.6170, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4433, train_f1=0.000 | val_loss=0.3792, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1836, train_f1=0.000 | val_loss=0.1092, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0585, train_f1=0.000 | val_loss=0.0221, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0259, train_f1=0.000 | val_loss=0.0084, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0125, train_f1=0.000 | val_loss=0.0040, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6047, train_f1=0.000 | val_loss=0.6955, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5120, train_f1=0.000 | val_loss=0.6130, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3264, train_f1=0.000 | val_loss=0.3374, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1372, train_f1=0.000 | val_loss=0.0945, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0548, train_f1=0.000 | val_loss=0.0504, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0277, train_f1=0.000 | val_loss=0.0203, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6773, train_f1=0.000 | val_loss=0.6320, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5891, train_f1=0.000 | val_loss=0.4669, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4008, train_f1=0.000 | val_loss=0.2590, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1718, train_f1=0.000 | val_loss=0.0633, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0642, train_f1=0.000 | val_loss=0.0248, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0289, train_f1=0.000 | val_loss=0.0109, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7167, train_f1=0.000 | val_loss=0.7056, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5838, train_f1=0.000 | val_loss=0.5196, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3370, train_f1=0.000 | val_loss=0.2086, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1293, train_f1=0.000 | val_loss=0.0404, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0468, train_f1=0.000 | val_loss=0.0102, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0249, train_f1=0.000 | val_loss=0.0064, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6940, train_f1=0.000 | val_loss=0.7357, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5511, train_f1=0.000 | val_loss=0.6365, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2772, train_f1=0.000 | val_loss=0.3910, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0968, train_f1=0.000 | val_loss=0.1846, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0331, train_f1=0.000 | val_loss=0.0798, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0176, train_f1=0.000 | val_loss=0.0496, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6850, train_f1=0.000 | val_loss=0.6827, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5979, train_f1=0.000 | val_loss=0.6422, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4202, train_f1=0.000 | val_loss=0.5443, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2157, train_f1=0.000 | val_loss=0.3645, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0963, train_f1=0.000 | val_loss=0.2319, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0490, train_f1=0.000 | val_loss=0.1249, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7302, train_f1=0.000 | val_loss=0.7220, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6528, train_f1=0.000 | val_loss=0.6707, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4880, train_f1=0.000 | val_loss=0.5197, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2569, train_f1=0.000 | val_loss=0.2357, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1096, train_f1=0.000 | val_loss=0.1022, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0567, train_f1=0.000 | val_loss=0.0494, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7188, train_f1=0.000 | val_loss=0.7285, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5715, train_f1=0.000 | val_loss=0.5658, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2843, train_f1=0.000 | val_loss=0.2483, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0983, train_f1=0.000 | val_loss=0.0435, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0366, train_f1=0.000 | val_loss=0.0178, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0217, train_f1=0.000 | val_loss=0.0050, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6030, train_f1=0.000 | val_loss=0.6392, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4862, train_f1=0.000 | val_loss=0.3674, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2441, train_f1=0.000 | val_loss=0.1225, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.000 | val_loss=0.0203, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0300, train_f1=0.000 | val_loss=0.0100, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0157, train_f1=0.000 | val_loss=0.0035, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6207, train_f1=0.000 | val_loss=0.5833, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5064, train_f1=0.000 | val_loss=0.3869, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2970, train_f1=0.000 | val_loss=0.1654, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1154, train_f1=0.000 | val_loss=0.0509, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0447, train_f1=0.000 | val_loss=0.0128, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0202, train_f1=0.000 | val_loss=0.0044, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7155, train_f1=0.000 | val_loss=0.7729, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5809, train_f1=0.000 | val_loss=0.4589, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3272, train_f1=0.000 | val_loss=0.1715, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1157, train_f1=0.000 | val_loss=0.0408, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0357, train_f1=0.000 | val_loss=0.0123, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0191, train_f1=0.000 | val_loss=0.0123, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6826, train_f1=0.000 | val_loss=0.6413, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4817, train_f1=0.000 | val_loss=0.2778, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1779, train_f1=0.000 | val_loss=0.0690, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0483, train_f1=0.000 | val_loss=0.0106, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0188, train_f1=0.000 | val_loss=0.0078, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0109, train_f1=0.000 | val_loss=0.0034, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6206, train_f1=0.000 | val_loss=0.6321, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4629, train_f1=0.000 | val_loss=0.3984, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2099, train_f1=0.000 | val_loss=0.1393, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0626, train_f1=0.000 | val_loss=0.0251, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0199, train_f1=0.000 | val_loss=0.0085, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0104, train_f1=0.000 | val_loss=0.0043, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6661, train_f1=0.000 | val_loss=0.7017, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5371, train_f1=0.000 | val_loss=0.4995, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3024, train_f1=0.000 | val_loss=0.2228, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1088, train_f1=0.000 | val_loss=0.0569, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0414, train_f1=0.000 | val_loss=0.0173, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0217, train_f1=0.000 | val_loss=0.0133, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6704, train_f1=0.000 | val_loss=0.6954, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5585, train_f1=0.000 | val_loss=0.4930, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3445, train_f1=0.000 | val_loss=0.3259, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1458, train_f1=0.000 | val_loss=0.1096, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0533, train_f1=0.000 | val_loss=0.0449, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0278, train_f1=0.000 | val_loss=0.0207, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6993, train_f1=0.000 | val_loss=0.7034, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5502, train_f1=0.000 | val_loss=0.4817, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2737, train_f1=0.000 | val_loss=0.1274, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0786, train_f1=0.000 | val_loss=0.0283, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0289, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0157, train_f1=0.000 | val_loss=0.0048, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7219, train_f1=0.000 | val_loss=0.6094, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5901, train_f1=0.000 | val_loss=0.3924, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2827, train_f1=0.000 | val_loss=0.0897, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0722, train_f1=0.000 | val_loss=0.0138, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0292, train_f1=0.000 | val_loss=0.0072, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0136, train_f1=0.000 | val_loss=0.0028, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7075, train_f1=0.000 | val_loss=0.7052, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6489, train_f1=0.000 | val_loss=0.6805, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5283, train_f1=0.000 | val_loss=0.5812, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3272, train_f1=0.000 | val_loss=0.3551, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1478, train_f1=0.000 | val_loss=0.1184, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0660, train_f1=0.000 | val_loss=0.0520, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6637, train_f1=0.000 | val_loss=0.6446, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6146, train_f1=0.000 | val_loss=0.5567, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4915, train_f1=0.000 | val_loss=0.4559, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3029, train_f1=0.000 | val_loss=0.2209, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1359, train_f1=0.000 | val_loss=0.0748, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0578, train_f1=0.000 | val_loss=0.0312, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.8078, train_f1=0.000 | val_loss=0.7434, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7194, train_f1=0.000 | val_loss=0.6775, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5378, train_f1=0.000 | val_loss=0.4818, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2427, train_f1=0.000 | val_loss=0.1215, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.000 | val_loss=0.0364, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0421, train_f1=0.000 | val_loss=0.0069, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6716, train_f1=0.000 | val_loss=0.6903, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5986, train_f1=0.000 | val_loss=0.6056, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4275, train_f1=0.000 | val_loss=0.3982, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1871, train_f1=0.000 | val_loss=0.1286, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0629, train_f1=0.000 | val_loss=0.0281, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0329, train_f1=0.000 | val_loss=0.0113, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6986, train_f1=0.000 | val_loss=0.7071, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5543, train_f1=0.000 | val_loss=0.5528, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3425, train_f1=0.000 | val_loss=0.2670, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1498, train_f1=0.000 | val_loss=0.0443, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0497, train_f1=0.000 | val_loss=0.0185, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0195, train_f1=0.000 | val_loss=0.0182, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6992, train_f1=0.000 | val_loss=0.7040, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5813, train_f1=0.000 | val_loss=0.4024, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3781, train_f1=0.000 | val_loss=0.1745, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1451, train_f1=0.000 | val_loss=0.0603, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0485, train_f1=0.000 | val_loss=0.0388, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0211, train_f1=0.000 | val_loss=0.0254, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6233, train_f1=0.000 | val_loss=0.6240, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4490, train_f1=0.000 | val_loss=0.3198, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1761, train_f1=0.000 | val_loss=0.0556, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0389, train_f1=0.000 | val_loss=0.0234, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0154, train_f1=0.000 | val_loss=0.0058, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0069, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6536, train_f1=0.000 | val_loss=0.6914, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4872, train_f1=0.000 | val_loss=0.2217, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1697, train_f1=0.000 | val_loss=0.0611, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0322, train_f1=0.000 | val_loss=0.0122, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0115, train_f1=0.000 | val_loss=0.0059, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0068, train_f1=0.000 | val_loss=0.0039, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7362, train_f1=0.000 | val_loss=0.7362, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6481, train_f1=0.000 | val_loss=0.6284, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4477, train_f1=0.000 | val_loss=0.4578, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1944, train_f1=0.000 | val_loss=0.1761, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0643, train_f1=0.000 | val_loss=0.0307, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0291, train_f1=0.000 | val_loss=0.0130, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7587, train_f1=0.000 | val_loss=0.8077, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6924, train_f1=0.000 | val_loss=0.6718, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5341, train_f1=0.000 | val_loss=0.4215, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2483, train_f1=0.000 | val_loss=0.1182, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0771, train_f1=0.000 | val_loss=0.0226, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0357, train_f1=0.000 | val_loss=0.0102, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7494, train_f1=0.000 | val_loss=0.7706, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5850, train_f1=0.000 | val_loss=0.6690, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2713, train_f1=0.000 | val_loss=0.2033, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.000 | val_loss=0.0318, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0306, train_f1=0.000 | val_loss=0.0200, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0148, train_f1=0.000 | val_loss=0.0093, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.6411, train_f1=0.000 | val_loss=0.5501, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5464, train_f1=0.000 | val_loss=0.4783, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2901, train_f1=0.000 | val_loss=0.1373, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0693, train_f1=0.000 | val_loss=0.0193, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0199, train_f1=0.000 | val_loss=0.0073, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0105, train_f1=0.000 | val_loss=0.0042, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7432, train_f1=0.000 | val_loss=0.7149, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6978, train_f1=0.000 | val_loss=0.6742, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5963, train_f1=0.000 | val_loss=0.5567, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3700, train_f1=0.000 | val_loss=0.2562, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1425, train_f1=0.000 | val_loss=0.0526, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0596, train_f1=0.000 | val_loss=0.0121, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7143, train_f1=0.000 | val_loss=0.6793, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6738, train_f1=0.000 | val_loss=0.6355, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5986, train_f1=0.000 | val_loss=0.5234, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4365, train_f1=0.000 | val_loss=0.3449, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2127, train_f1=0.000 | val_loss=0.1217, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0914, train_f1=0.000 | val_loss=0.0297, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7009, train_f1=0.000 | val_loss=0.6670, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6359, train_f1=0.000 | val_loss=0.5999, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4491, train_f1=0.000 | val_loss=0.2949, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1614, train_f1=0.000 | val_loss=0.0465, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0506, train_f1=0.000 | val_loss=0.0115, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0246, train_f1=0.000 | val_loss=0.0036, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.7077, train_f1=0.000 | val_loss=0.6517, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6561, train_f1=0.000 | val_loss=0.5991, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5367, train_f1=0.000 | val_loss=0.4570, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2828, train_f1=0.000 | val_loss=0.1157, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0864, train_f1=0.000 | val_loss=0.0229, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0386, train_f1=0.000 | val_loss=0.0075, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6176, train_f1=0.000 | val_loss=0.6033, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5342, train_f1=0.000 | val_loss=0.5401, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3482, train_f1=0.000 | val_loss=0.3852, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1319, train_f1=0.000 | val_loss=0.2210, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0446, train_f1=0.000 | val_loss=0.0997, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0169, train_f1=0.000 | val_loss=0.0674, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6155, train_f1=0.000 | val_loss=0.7219, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5354, train_f1=0.000 | val_loss=0.7503, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3531, train_f1=0.000 | val_loss=0.5173, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1414, train_f1=0.000 | val_loss=0.2101, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0432, train_f1=0.000 | val_loss=0.0671, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0185, train_f1=0.000 | val_loss=0.0296, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6923, train_f1=0.000 | val_loss=0.7100, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5323, train_f1=0.000 | val_loss=0.4567, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2234, train_f1=0.000 | val_loss=0.1905, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0586, train_f1=0.000 | val_loss=0.0340, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0186, train_f1=0.000 | val_loss=0.0142, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0092, train_f1=0.000 | val_loss=0.0047, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6390, train_f1=0.000 | val_loss=0.5926, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4939, train_f1=0.000 | val_loss=0.4724, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2244, train_f1=0.000 | val_loss=0.2110, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0572, train_f1=0.000 | val_loss=0.0546, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0185, train_f1=0.000 | val_loss=0.0200, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0108, train_f1=0.000 | val_loss=0.0117, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7766, train_f1=0.000 | val_loss=0.7500, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6851, train_f1=0.000 | val_loss=0.6367, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4844, train_f1=0.000 | val_loss=0.3395, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1981, train_f1=0.000 | val_loss=0.0722, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0561, train_f1=0.000 | val_loss=0.0160, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0265, train_f1=0.000 | val_loss=0.0062, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7459, train_f1=0.000 | val_loss=0.7951, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6421, train_f1=0.000 | val_loss=0.7400, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4444, train_f1=0.000 | val_loss=0.5581, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1932, train_f1=0.000 | val_loss=0.2813, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0632, train_f1=0.000 | val_loss=0.1161, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0306, train_f1=0.000 | val_loss=0.1060, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7094, train_f1=0.000 | val_loss=0.6523, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5780, train_f1=0.000 | val_loss=0.3837, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2900, train_f1=0.000 | val_loss=0.1268, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0747, train_f1=0.000 | val_loss=0.0215, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0290, train_f1=0.000 | val_loss=0.0047, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0123, train_f1=0.000 | val_loss=0.0025, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7147, train_f1=0.000 | val_loss=0.6362, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5904, train_f1=0.000 | val_loss=0.4064, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2916, train_f1=0.000 | val_loss=0.0930, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0749, train_f1=0.000 | val_loss=0.0148, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0247, train_f1=0.000 | val_loss=0.0045, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0153, train_f1=0.000 | val_loss=0.0011, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7068, train_f1=0.000 | val_loss=0.6582, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6278, train_f1=0.000 | val_loss=0.5585, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4801, train_f1=0.000 | val_loss=0.4981, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2449, train_f1=0.000 | val_loss=0.3938, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1010, train_f1=0.000 | val_loss=0.2597, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0478, train_f1=0.000 | val_loss=0.1836, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6408, train_f1=0.000 | val_loss=0.7133, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5638, train_f1=0.000 | val_loss=0.5694, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4028, train_f1=0.000 | val_loss=0.4486, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1917, train_f1=0.000 | val_loss=0.2012, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0751, train_f1=0.000 | val_loss=0.0772, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0367, train_f1=0.000 | val_loss=0.0407, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6880, train_f1=0.000 | val_loss=0.6926, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5709, train_f1=0.000 | val_loss=0.5094, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3103, train_f1=0.000 | val_loss=0.2417, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0975, train_f1=0.000 | val_loss=0.0493, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0292, train_f1=0.000 | val_loss=0.0145, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0159, train_f1=0.000 | val_loss=0.0055, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7300, train_f1=0.000 | val_loss=0.6177, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6063, train_f1=0.000 | val_loss=0.4301, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3669, train_f1=0.000 | val_loss=0.2176, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1302, train_f1=0.000 | val_loss=0.0646, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0459, train_f1=0.000 | val_loss=0.0327, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0235, train_f1=0.000 | val_loss=0.0185, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6651, train_f1=0.000 | val_loss=0.7311, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5524, train_f1=0.000 | val_loss=0.4819, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3186, train_f1=0.000 | val_loss=0.1732, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0938, train_f1=0.000 | val_loss=0.0209, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0264, train_f1=0.000 | val_loss=0.0061, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0139, train_f1=0.000 | val_loss=0.0051, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6191, train_f1=0.000 | val_loss=0.7327, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5099, train_f1=0.000 | val_loss=0.6701, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3079, train_f1=0.000 | val_loss=0.4007, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1118, train_f1=0.000 | val_loss=0.0683, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0319, train_f1=0.000 | val_loss=0.0121, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0156, train_f1=0.000 | val_loss=0.0090, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.5868, train_f1=0.000 | val_loss=0.5956, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4257, train_f1=0.000 | val_loss=0.4584, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1676, train_f1=0.000 | val_loss=0.0775, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0398, train_f1=0.000 | val_loss=0.0202, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0146, train_f1=0.000 | val_loss=0.0049, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0080, train_f1=0.000 | val_loss=0.0024, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6288, train_f1=0.000 | val_loss=0.7262, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4745, train_f1=0.000 | val_loss=0.4774, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1970, train_f1=0.000 | val_loss=0.1418, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0519, train_f1=0.000 | val_loss=0.0363, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0182, train_f1=0.000 | val_loss=0.0154, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0089, train_f1=0.000 | val_loss=0.0113, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6998, train_f1=0.000 | val_loss=0.7104, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6057, train_f1=0.000 | val_loss=0.5700, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4144, train_f1=0.000 | val_loss=0.3242, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1696, train_f1=0.000 | val_loss=0.0473, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0482, train_f1=0.000 | val_loss=0.0104, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0230, train_f1=0.000 | val_loss=0.0051, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6646, train_f1=0.000 | val_loss=0.6049, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5818, train_f1=0.000 | val_loss=0.4420, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4061, train_f1=0.000 | val_loss=0.2802, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1736, train_f1=0.000 | val_loss=0.0992, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0519, train_f1=0.000 | val_loss=0.0197, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0250, train_f1=0.000 | val_loss=0.0152, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6270, train_f1=0.000 | val_loss=0.6057, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4646, train_f1=0.000 | val_loss=0.3777, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2020, train_f1=0.000 | val_loss=0.1548, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0576, train_f1=0.000 | val_loss=0.0341, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0195, train_f1=0.000 | val_loss=0.0077, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0099, train_f1=0.000 | val_loss=0.0064, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7404, train_f1=0.000 | val_loss=0.7294, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6167, train_f1=0.000 | val_loss=0.5778, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3204, train_f1=0.000 | val_loss=0.2123, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0847, train_f1=0.000 | val_loss=0.0219, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0266, train_f1=0.000 | val_loss=0.0057, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0146, train_f1=0.000 | val_loss=0.0043, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6027, train_f1=0.000 | val_loss=0.6518, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5490, train_f1=0.000 | val_loss=0.5324, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3993, train_f1=0.000 | val_loss=0.3182, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1934, train_f1=0.000 | val_loss=0.0426, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0701, train_f1=0.000 | val_loss=0.0072, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0356, train_f1=0.000 | val_loss=0.0012, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7811, train_f1=0.000 | val_loss=0.8047, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7196, train_f1=0.000 | val_loss=0.8219, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5904, train_f1=0.000 | val_loss=0.6692, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3371, train_f1=0.000 | val_loss=0.3290, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1321, train_f1=0.000 | val_loss=0.0791, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0546, train_f1=0.000 | val_loss=0.0275, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6600, train_f1=0.000 | val_loss=0.6964, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5738, train_f1=0.000 | val_loss=0.6357, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3594, train_f1=0.000 | val_loss=0.4018, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1159, train_f1=0.000 | val_loss=0.1178, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0340, train_f1=0.000 | val_loss=0.0429, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0184, train_f1=0.000 | val_loss=0.0231, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7972, train_f1=0.000 | val_loss=0.7339, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7114, train_f1=0.000 | val_loss=0.7063, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5256, train_f1=0.000 | val_loss=0.4750, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2184, train_f1=0.000 | val_loss=0.0589, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0631, train_f1=0.000 | val_loss=0.0120, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0298, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6775, train_f1=0.000 | val_loss=0.6399, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5614, train_f1=0.000 | val_loss=0.4398, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3178, train_f1=0.000 | val_loss=0.1355, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0898, train_f1=0.000 | val_loss=0.0220, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0275, train_f1=0.000 | val_loss=0.0051, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0124, train_f1=0.000 | val_loss=0.0063, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6864, train_f1=0.000 | val_loss=0.5990, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5870, train_f1=0.000 | val_loss=0.5848, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3553, train_f1=0.000 | val_loss=0.2764, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1075, train_f1=0.000 | val_loss=0.0578, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0273, train_f1=0.000 | val_loss=0.0205, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0136, train_f1=0.000 | val_loss=0.0129, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7367, train_f1=0.000 | val_loss=0.7134, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5687, train_f1=0.000 | val_loss=0.5853, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2776, train_f1=0.000 | val_loss=0.0934, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0600, train_f1=0.000 | val_loss=0.0119, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0149, train_f1=0.000 | val_loss=0.0042, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0086, train_f1=0.000 | val_loss=0.0029, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7600, train_f1=0.000 | val_loss=0.5981, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6035, train_f1=0.000 | val_loss=0.4645, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2879, train_f1=0.000 | val_loss=0.1172, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0611, train_f1=0.000 | val_loss=0.0217, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0169, train_f1=0.000 | val_loss=0.0074, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0095, train_f1=0.000 | val_loss=0.0042, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6690, train_f1=0.000 | val_loss=0.6262, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5768, train_f1=0.000 | val_loss=0.3695, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3926, train_f1=0.000 | val_loss=0.1453, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1692, train_f1=0.000 | val_loss=0.0654, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0506, train_f1=0.000 | val_loss=0.0261, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0259, train_f1=0.000 | val_loss=0.0086, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6204, train_f1=0.000 | val_loss=0.7116, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5292, train_f1=0.000 | val_loss=0.5860, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3442, train_f1=0.000 | val_loss=0.2477, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1376, train_f1=0.000 | val_loss=0.0498, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0427, train_f1=0.000 | val_loss=0.0083, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0201, train_f1=0.000 | val_loss=0.0025, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6774, train_f1=0.000 | val_loss=0.6315, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5571, train_f1=0.000 | val_loss=0.4329, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2701, train_f1=0.000 | val_loss=0.1236, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0723, train_f1=0.000 | val_loss=0.0305, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0250, train_f1=0.000 | val_loss=0.0167, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0118, train_f1=0.000 | val_loss=0.0031, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6561, train_f1=0.000 | val_loss=0.5398, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5695, train_f1=0.000 | val_loss=0.3753, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3236, train_f1=0.000 | val_loss=0.1763, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0745, train_f1=0.000 | val_loss=0.0376, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0205, train_f1=0.000 | val_loss=0.0162, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0087, train_f1=0.000 | val_loss=0.0069, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7333, train_f1=0.000 | val_loss=0.6725, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6765, train_f1=0.000 | val_loss=0.5794, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5571, train_f1=0.000 | val_loss=0.4740, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.3743, train_f1=0.000 | val_loss=0.3143, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.1609, train_f1=0.000 | val_loss=0.0938, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0667, train_f1=0.000 | val_loss=0.0247, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7939, train_f1=0.000 | val_loss=0.7417, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7364, train_f1=0.000 | val_loss=0.7426, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.6374, train_f1=0.000 | val_loss=0.6228, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.4681, train_f1=0.000 | val_loss=0.3942, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.2471, train_f1=0.000 | val_loss=0.1243, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.1029, train_f1=0.000 | val_loss=0.0627, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7710, train_f1=0.000 | val_loss=0.6972, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.7027, train_f1=0.000 | val_loss=0.7022, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.5442, train_f1=0.000 | val_loss=0.5903, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2468, train_f1=0.000 | val_loss=0.1585, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0649, train_f1=0.000 | val_loss=0.0088, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0252, train_f1=0.000 | val_loss=0.0034, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6451, train_f1=0.000 | val_loss=0.6574, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5913, train_f1=0.000 | val_loss=0.6521, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4643, train_f1=0.000 | val_loss=0.5247, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.2424, train_f1=0.000 | val_loss=0.2297, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0750, train_f1=0.000 | val_loss=0.0744, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0297, train_f1=0.000 | val_loss=0.0263, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6531, train_f1=0.000 | val_loss=0.6360, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5105, train_f1=0.000 | val_loss=0.3862, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2211, train_f1=0.000 | val_loss=0.1955, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0565, train_f1=0.000 | val_loss=0.0647, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0198, train_f1=0.000 | val_loss=0.0286, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0101, train_f1=0.000 | val_loss=0.0103, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6659, train_f1=0.000 | val_loss=0.6460, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5229, train_f1=0.000 | val_loss=0.4579, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2333, train_f1=0.000 | val_loss=0.1217, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0641, train_f1=0.000 | val_loss=0.0219, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0206, train_f1=0.000 | val_loss=0.0075, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0135, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6334, train_f1=0.000 | val_loss=0.6769, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4033, train_f1=0.000 | val_loss=0.2504, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1039, train_f1=0.000 | val_loss=0.0396, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0208, train_f1=0.000 | val_loss=0.0190, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0092, train_f1=0.000 | val_loss=0.0060, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0048, train_f1=0.000 | val_loss=0.0069, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6806, train_f1=0.000 | val_loss=0.6730, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.3940, train_f1=0.000 | val_loss=0.3531, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1020, train_f1=0.000 | val_loss=0.0857, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0262, train_f1=0.000 | val_loss=0.0134, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0096, train_f1=0.000 | val_loss=0.0074, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0050, train_f1=0.000 | val_loss=0.0036, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6915, train_f1=0.000 | val_loss=0.7581, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5296, train_f1=0.000 | val_loss=0.5329, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2093, train_f1=0.000 | val_loss=0.1015, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0481, train_f1=0.000 | val_loss=0.0119, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0219, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0101, train_f1=0.000 | val_loss=0.0045, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6674, train_f1=0.000 | val_loss=0.6616, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5143, train_f1=0.000 | val_loss=0.3794, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2203, train_f1=0.000 | val_loss=0.1064, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0563, train_f1=0.000 | val_loss=0.0231, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0234, train_f1=0.000 | val_loss=0.0065, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0099, train_f1=0.000 | val_loss=0.0039, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6770, train_f1=0.000 | val_loss=0.7585, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4649, train_f1=0.000 | val_loss=0.4828, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1518, train_f1=0.000 | val_loss=0.0633, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0369, train_f1=0.000 | val_loss=0.0168, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0141, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0073, train_f1=0.000 | val_loss=0.0028, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7034, train_f1=0.000 | val_loss=0.7333, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4781, train_f1=0.000 | val_loss=0.4049, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1323, train_f1=0.000 | val_loss=0.0668, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0280, train_f1=0.000 | val_loss=0.0087, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0102, train_f1=0.000 | val_loss=0.0038, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0058, train_f1=0.000 | val_loss=0.0071, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7580, train_f1=0.000 | val_loss=0.7045, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6324, train_f1=0.000 | val_loss=0.5693, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3846, train_f1=0.000 | val_loss=0.3497, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1262, train_f1=0.000 | val_loss=0.1541, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0382, train_f1=0.000 | val_loss=0.0520, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0193, train_f1=0.000 | val_loss=0.0358, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6958, train_f1=0.000 | val_loss=0.6695, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5774, train_f1=0.000 | val_loss=0.5473, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3657, train_f1=0.000 | val_loss=0.3334, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1317, train_f1=0.000 | val_loss=0.0397, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0464, train_f1=0.000 | val_loss=0.0161, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0238, train_f1=0.000 | val_loss=0.0080, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6186, train_f1=0.000 | val_loss=0.6137, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4613, train_f1=0.000 | val_loss=0.4011, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1646, train_f1=0.000 | val_loss=0.1127, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0395, train_f1=0.000 | val_loss=0.0314, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0146, train_f1=0.000 | val_loss=0.0157, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0081, train_f1=0.000 | val_loss=0.0085, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6635, train_f1=0.000 | val_loss=0.6235, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4479, train_f1=0.000 | val_loss=0.3273, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1426, train_f1=0.000 | val_loss=0.0644, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0334, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0150, train_f1=0.000 | val_loss=0.0020, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0072, train_f1=0.000 | val_loss=0.0007, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7153, train_f1=0.000 | val_loss=0.8296, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5171, train_f1=0.000 | val_loss=0.5130, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2403, train_f1=0.000 | val_loss=0.2632, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0681, train_f1=0.000 | val_loss=0.0951, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0211, train_f1=0.000 | val_loss=0.0461, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0113, train_f1=0.000 | val_loss=0.0346, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7447, train_f1=0.000 | val_loss=0.7836, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5436, train_f1=0.000 | val_loss=0.5104, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2262, train_f1=0.000 | val_loss=0.2765, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0570, train_f1=0.000 | val_loss=0.0767, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0184, train_f1=0.000 | val_loss=0.0357, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0084, train_f1=0.000 | val_loss=0.0261, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7129, train_f1=0.000 | val_loss=0.6612, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4202, train_f1=0.000 | val_loss=0.2566, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1128, train_f1=0.000 | val_loss=0.0380, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0312, train_f1=0.000 | val_loss=0.0101, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0128, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0068, train_f1=0.000 | val_loss=0.0019, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6600, train_f1=0.000 | val_loss=0.6885, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4307, train_f1=0.000 | val_loss=0.4872, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1242, train_f1=0.000 | val_loss=0.1914, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0280, train_f1=0.000 | val_loss=0.0706, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0083, train_f1=0.000 | val_loss=0.0386, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0054, train_f1=0.000 | val_loss=0.0227, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7009, train_f1=0.000 | val_loss=0.6738, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5450, train_f1=0.000 | val_loss=0.4063, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2608, train_f1=0.000 | val_loss=0.0778, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0695, train_f1=0.000 | val_loss=0.0133, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0251, train_f1=0.000 | val_loss=0.0049, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0108, train_f1=0.000 | val_loss=0.0025, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6928, train_f1=0.000 | val_loss=0.7660, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5518, train_f1=0.000 | val_loss=0.6294, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2386, train_f1=0.000 | val_loss=0.1497, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0558, train_f1=0.000 | val_loss=0.0135, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0164, train_f1=0.000 | val_loss=0.0045, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0097, train_f1=0.000 | val_loss=0.0035, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6142, train_f1=0.000 | val_loss=0.5904, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.3302, train_f1=0.000 | val_loss=0.0572, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0691, train_f1=0.000 | val_loss=0.0060, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0195, train_f1=0.000 | val_loss=0.0021, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0077, train_f1=0.000 | val_loss=0.0007, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0061, train_f1=0.000 | val_loss=0.0004, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6151, train_f1=0.000 | val_loss=0.6327, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.3763, train_f1=0.000 | val_loss=0.3320, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0929, train_f1=0.000 | val_loss=0.0263, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0246, train_f1=0.000 | val_loss=0.0048, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0098, train_f1=0.000 | val_loss=0.0015, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0051, train_f1=0.000 | val_loss=0.0004, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7403, train_f1=0.000 | val_loss=0.6883, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6195, train_f1=0.000 | val_loss=0.5310, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3669, train_f1=0.000 | val_loss=0.2942, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1309, train_f1=0.000 | val_loss=0.1087, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0433, train_f1=0.000 | val_loss=0.0465, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0213, train_f1=0.000 | val_loss=0.0175, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7064, train_f1=0.000 | val_loss=0.6472, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6416, train_f1=0.000 | val_loss=0.5755, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4597, train_f1=0.000 | val_loss=0.3342, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1653, train_f1=0.000 | val_loss=0.0452, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0458, train_f1=0.000 | val_loss=0.0076, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0219, train_f1=0.000 | val_loss=0.0011, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6313, train_f1=0.000 | val_loss=0.6429, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4946, train_f1=0.000 | val_loss=0.3780, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1865, train_f1=0.000 | val_loss=0.0351, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0448, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0153, train_f1=0.000 | val_loss=0.0021, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0090, train_f1=0.000 | val_loss=0.0012, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6634, train_f1=0.000 | val_loss=0.6424, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5298, train_f1=0.000 | val_loss=0.5260, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2388, train_f1=0.000 | val_loss=0.1233, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0512, train_f1=0.000 | val_loss=0.0181, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0146, train_f1=0.000 | val_loss=0.0029, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0081, train_f1=0.000 | val_loss=0.0017, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6311, train_f1=0.000 | val_loss=0.6134, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4101, train_f1=0.000 | val_loss=0.3021, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1636, train_f1=0.000 | val_loss=0.0802, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0430, train_f1=0.000 | val_loss=0.0247, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0144, train_f1=0.000 | val_loss=0.0054, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0065, train_f1=0.000 | val_loss=0.0042, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6170, train_f1=0.000 | val_loss=0.6072, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4128, train_f1=0.000 | val_loss=0.3493, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1408, train_f1=0.000 | val_loss=0.0821, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0349, train_f1=0.000 | val_loss=0.0145, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0120, train_f1=0.000 | val_loss=0.0072, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0064, train_f1=0.000 | val_loss=0.0076, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6664, train_f1=0.000 | val_loss=0.5754, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.3246, train_f1=0.000 | val_loss=0.0819, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0573, train_f1=0.000 | val_loss=0.0130, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0140, train_f1=0.000 | val_loss=0.0042, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0054, train_f1=0.000 | val_loss=0.0022, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0045, train_f1=0.000 | val_loss=0.0013, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7149, train_f1=0.000 | val_loss=0.7303, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4290, train_f1=0.000 | val_loss=0.1991, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1215, train_f1=0.000 | val_loss=0.0307, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0284, train_f1=0.000 | val_loss=0.0068, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0094, train_f1=0.000 | val_loss=0.0030, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0047, train_f1=0.000 | val_loss=0.0019, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6112, train_f1=0.000 | val_loss=0.6003, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4196, train_f1=0.000 | val_loss=0.2094, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1244, train_f1=0.000 | val_loss=0.0287, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0323, train_f1=0.000 | val_loss=0.0058, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0118, train_f1=0.000 | val_loss=0.0032, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0062, train_f1=0.000 | val_loss=0.0017, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7115, train_f1=0.000 | val_loss=0.7096, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6091, train_f1=0.000 | val_loss=0.4892, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.3000, train_f1=0.000 | val_loss=0.1268, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0673, train_f1=0.000 | val_loss=0.0185, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0231, train_f1=0.000 | val_loss=0.0097, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0087, train_f1=0.000 | val_loss=0.0055, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6001, train_f1=0.000 | val_loss=0.5885, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.3712, train_f1=0.000 | val_loss=0.2366, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0852, train_f1=0.000 | val_loss=0.0291, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0204, train_f1=0.000 | val_loss=0.0043, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0078, train_f1=0.000 | val_loss=0.0033, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0046, train_f1=0.000 | val_loss=0.0010, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6401, train_f1=0.000 | val_loss=0.7432, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4217, train_f1=0.000 | val_loss=0.2752, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.1181, train_f1=0.000 | val_loss=0.0579, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0229, train_f1=0.000 | val_loss=0.0164, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0106, train_f1=0.000 | val_loss=0.0090, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0044, train_f1=0.000 | val_loss=0.0046, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.5762, train_f1=0.000 | val_loss=0.6311, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.4961, train_f1=0.000 | val_loss=0.4946, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2944, train_f1=0.000 | val_loss=0.2146, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0951, train_f1=0.000 | val_loss=0.0333, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0282, train_f1=0.000 | val_loss=0.0060, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0140, train_f1=0.000 | val_loss=0.0018, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.6392, train_f1=0.000 | val_loss=0.6956, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.5883, train_f1=0.000 | val_loss=0.6350, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4347, train_f1=0.000 | val_loss=0.3564, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.1693, train_f1=0.000 | val_loss=0.0473, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0430, train_f1=0.000 | val_loss=0.0118, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0182, train_f1=0.000 | val_loss=0.0065, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7500, train_f1=0.000 | val_loss=0.6943, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6272, train_f1=0.000 | val_loss=0.6209, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.2893, train_f1=0.000 | val_loss=0.0863, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0577, train_f1=0.000 | val_loss=0.0058, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0169, train_f1=0.000 | val_loss=0.0020, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0080, train_f1=0.000 | val_loss=0.0008, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.7403, train_f1=0.000 | val_loss=0.7129, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.6641, train_f1=0.000 | val_loss=0.5988, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.4219, train_f1=0.000 | val_loss=0.1416, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0890, train_f1=0.000 | val_loss=0.0152, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0217, train_f1=0.000 | val_loss=0.0041, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0112, train_f1=0.000 | val_loss=0.0018, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "Best config: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.0, 'conv_type': 'sage', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best validation f1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "best_overall = None\n",
    "best_cfg = None\n",
    "\n",
    "for cfg in iter_gnn_configs(GNN_HYP):\n",
    "    print(\"\\n=== Config:\", cfg[\"conv_type\"], cfg[\"hidden_dim\"], cfg[\"num_layers\"],\n",
    "          cfg[\"dropout\"], cfg[\"aggr\"], \"lr\", cfg[\"lr\"], \"===\")\n",
    "    out = train_gnn_model(cfg, train_loader, val_loader, in_dim=D_NODE, device=DEVICE)\n",
    "\n",
    "    if best_overall is None or out[\"best_metric\"] > best_overall[\"best_metric\"]:\n",
    "        best_overall = out\n",
    "        best_cfg = cfg\n",
    "\n",
    "print(\"\\nBest config:\", best_cfg)\n",
    "print(\"Best validation\", best_overall[\"best_monitor\"], \"=\", best_overall[\"best_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107dc2c-c14c-46d9-a86d-dfdc6f7a80ba",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f1eea-8bf1-4d6b-8620-b3567760851d",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c2a31-c14e-4dac-a7fb-cef109a5d9b4",
   "metadata": {},
   "source": [
    "##### Helper Functions\n",
    "\n",
    "This code provides evaluation utilities for a trained PRB-GraphSAGE model, computing metrics and confusion statistics on a given dataset. The `evaluate_gnn_model` function runs in no-grad mode, sets the model to eval, and iterates over a PyTorch Geometric loader, moving each batch to the target device, forwarding node features and graph structure through the model to obtain logits, and computing a loss with the same `gnn_criterion` used during training. It accumulates batch-weighted loss, collects sigmoid-transformed probabilities and true labels, and at the end concatenates them to compute average loss and standard binary classification metrics (accuracy, precision, recall, F1) via the existing `compute_binary_metrics` helper, returning both the metrics dictionary and the full `y_true` and `y_prob` arrays. The `summarize_eval_split` helper prints these metrics in a compact, human-readable line for a named split (e.g., “train”, “val”, “test”). Finally, `confusion_from_probs` turns true labels and predicted probabilities into a small 2×2 confusion matrix (tp, tn, fp, fn) using a configurable threshold, making it easy to inspect the model’s error profile on any evaluation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "499402f4-f608-4b91-bdd8-a2a11fd4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: PRB-GraphSAGE Evaluation (Best Model) ====\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gnn_model(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    device: torch.device,\n",
    "    threshold: float = 0.5,\n",
    ") -> Tuple[Dict[str, float], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained PRB-GraphSAGE model on a given loader.\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict with loss, acc, precision, recall, f1\n",
    "        y_true:  np.ndarray of true labels (0/1)\n",
    "        y_prob:  np.ndarray of predicted probabilities p(y=1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = batch.y.view(-1).float()            # [B]\n",
    "\n",
    "        logits = model(batch.x, batch.edge_index, batch.batch)  # [B] or [B,1]\n",
    "        logits = logits.view_as(y)\n",
    "        loss = gnn_criterion(logits, y)\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        metrics = {\"loss\": 0.0, \"acc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        return metrics, np.array([]), np.array([])\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    # Reuse the helper you already defined earlier\n",
    "    metrics = compute_binary_metrics(all_labels, all_probs, threshold=threshold)\n",
    "    metrics[\"loss\"] = float(avg_loss)\n",
    "    return metrics, all_labels, all_probs\n",
    "\n",
    "\n",
    "def summarize_eval_split(name: str, metrics: Dict[str, float]):\n",
    "    print(\n",
    "        f\"[{name}] \"\n",
    "        f\"loss={metrics['loss']:.4f}  \"\n",
    "        f\"acc={metrics['acc']:.3f}  \"\n",
    "        f\"precision={metrics['precision']:.3f}  \"\n",
    "        f\"recall={metrics['recall']:.3f}  \"\n",
    "        f\"f1={metrics['f1']:.3f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def confusion_from_probs(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Tiny 2x2 confusion matrix helper for reporting.\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64).ravel()\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64).ravel()\n",
    "\n",
    "    tp = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    tn = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    fp = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "    fn = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "\n",
    "    return {\"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5041e3-36bb-4d10-af3a-ce861facec6c",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "\n",
    "This code evaluates the best-performing PRB-GraphSAGE model found during the hyperparameter sweep on both the training and validation splits to assess fit and generalization. It first retrieves the model with its restored best weights from `best_overall`, moves it to the active device, switches it to evaluation mode, and prints the corresponding best hyperparameter configuration and monitored metric (e.g., validation F1). It then calls `evaluate_gnn_model` on the training DataLoader to compute loss and binary metrics, uses `summarize_eval_split` to print a concise summary line, and derives a 2×2 confusion matrix with `confusion_from_probs` to inspect true/false positives and negatives. The same process is repeated on the validation DataLoader, with the validation metrics and confusion matrix serving as the primary performance numbers to report in experiments or papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c40e0633-d5f0-49b5-96ae-3672e7621312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.0, 'conv_type': 'sage', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best monitored metric (f1): 0.0000\n",
      "\n",
      "[train] loss=0.7396  acc=0.270  precision=0.000  recall=0.000  f1=0.000\n",
      "  train confusion: {'tp': 0, 'tn': 444, 'fp': 1198, 'fn': 0}\n",
      "[val] loss=0.7011  acc=0.156  precision=0.000  recall=0.000  f1=0.000\n",
      "  val confusion: {'tp': 0, 'tn': 515, 'fp': 2793, 'fn': 0}\n"
     ]
    }
   ],
   "source": [
    "# Grab the best model (weights are already restored in train_gnn_model)\n",
    "best_model = best_overall[\"model\"].to(DEVICE)\n",
    "best_model.eval()\n",
    "\n",
    "print(\"Best config:\", best_cfg)\n",
    "print(f\"Best monitored metric ({best_overall['best_monitor']}): {best_overall['best_metric']:.4f}\\n\")\n",
    "\n",
    "# Evaluate on training split (to see overfit / underfit)\n",
    "train_metrics, train_y, train_p = evaluate_gnn_model(best_model, train_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"train\", train_metrics)\n",
    "train_cm = confusion_from_probs(train_y, train_p, threshold=0.5)\n",
    "print(\"  train confusion:\", train_cm)\n",
    "\n",
    "# Evaluate on validation split (primary number for the report)\n",
    "val_metrics, val_y, val_p = evaluate_gnn_model(best_model, val_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"val\", val_metrics)\n",
    "val_cm = confusion_from_probs(val_y, val_p, threshold=0.5)\n",
    "print(\"  val confusion:\", val_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
