{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c319e0-eeb6-448d-9167-59928fb81e0d",
   "metadata": {},
   "source": [
    "# PRB-GraphSAGE: Resource Management Reconstruction GNN  \n",
    "## CPSC 8810 — ML4G Term Project\n",
    "### Authored by Ryan Barker\n",
    "\n",
    "This notebook implements a minimal viable **PRB-GraphSAGE** model: an offline, UE-level graph neural network that reconstructs resource-management behavior from noisy per-TTI proportional-fair (PF) scheduler logs. Using the existing TTI-Trust Anamoly Detector + Classifier preprocessing pipeline, each sliding window of TTIs is converted into a **UE-contention graph** whose nodes represent UEs active in the window and whose edges capture co-scheduling (PRB contention) relationships. Node features summarize PRB usage statistics, fairness/PF surrogates, and starvation patterns, while a compact **GraphSAGE → global pooling → MLP** architecture performs **graph-level classification** to distinguish benign scheduling behavior from PF-based PRB starvation indicative of attack conditions. This MVP focuses on methodology rather than headline accuracy, acknowledging the dataset’s timing noise and simulator artifacts, and serves as a prototype for future training on GPU-native, slot-accurate Aerial/cuMAC traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f84dda-2fe7-4959-80fa-d8a0d1fb75c6",
   "metadata": {},
   "source": [
    "### preproc_tti_trust.py\n",
    "\n",
    "This script is a preprocessing scaffold for the TTI-Trust project that converts lightweight attack/benign CSV “shims” into parquet datasets, derives scheduler-native, identity-agnostic features, and emits windowed shards plus cross-validation splits for PyTorch. It first materializes CSVs into partitioned parquet (by `run_id`), adding a relative time axis `sec_rel` if needed, then for each run it auto-detects UE resource-block columns, computes `total_rb`, normalizes RB usage into per-UE PRB shares, and derives a Jain’s fairness index `J`. On top of these per-UE shares it builds “roles” by tracking an exponential-moving-average of the shares, selecting the top-K dominant UEs per TTI, and aggregating everyone else into a `rest_share` and corresponding “small RB” indicators, plus 1-second presence flags to capture coarse duty cycle behavior. Using these role-based features and the `phase` labels, it slides long and short windows (e.g., 240- and 64-TTI) with multiple strides, labels each window as benign, attack, or ambiguous via a dominance test on the top role (with thresholds like `ATTACK_DOMINANCE`, `OTHERS_MAX`, and `DUTY_KEEP`), and batches them into compressed NumPy `.npz` shards alongside parquet meta tables that record window boundaries, labels, and summary statistics. The script writes enriched per-run parquet files, then populates a `win_shards` directory with both long and short window shards for attack and benign sources. Finally, it builds grouped K-fold splits where entire `run_id`s are kept together in either train or validation, returning index pairs for each fold and printing a brief summary so downstream training can stream shards or assemble cross-validation sets directly from the generated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb1b65f-ab4d-4e4b-a8d5-44c615516c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[attack] Found 1 run_id partitions.\n",
      "[attack] run_id=040933 → attack_enriched_040933.parquet  (rows=8,001)\n",
      "[benign] Found 1 run_id partitions.\n",
      "[benign] run_id=004912 → benign_enriched_004912.parquet  (rows=4,001)\n",
      "\n",
      "[long] meta=(2403, 11), groups=2, folds=5\n",
      "[short] meta=(4950, 11), groups=2, folds=5\n",
      "\n",
      "Preprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\n"
     ]
    }
   ],
   "source": [
    "# TTI-Trust preprocessing scaffold for PyTorch: identity-agnostic feature composer,\n",
    "# phase-aware windowing, and grouped K-fold splits.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, warnings, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.csv as pv, pyarrow.parquet as pq\n",
    "\n",
    "# -------------------- DATASETS --------------------\n",
    "ATTACK_FN = \"data/shims/attack_shim_16000_24000.csv\" # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "BENIGN_FN = \"data/shims/benign_shim_8000_12000.csv\"  # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "\n",
    "# -------------------- Constants aligned to paper --------------------\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "\n",
    "W_LONG  = 240\n",
    "W_SHORT = 64\n",
    "\n",
    "STRIDES_LONG  = [8, 12]\n",
    "STRIDES_SHORT = [4, 6]\n",
    "\n",
    "TOP_K = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "ATTACK_DOMINANCE = 0.90\n",
    "OTHERS_MAX       = 0.10\n",
    "DUTY_KEEP        = 0.90\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def csv_to_parquet(src_csv: str, dest_root: str):\n",
    "    os.makedirs(dest_root, exist_ok=True)\n",
    "    for _root, _dirs, files in os.walk(dest_root):\n",
    "        if any(f.endswith(\".parquet\") for f in files):\n",
    "            return\n",
    "\n",
    "    convert_cols = {\"run_id\": pa.large_string(), \"phase\": pa.large_string()}\n",
    "    read_opts  = pv.ReadOptions(block_size=1<<26)\n",
    "    conv_opts  = pv.ConvertOptions(column_types=convert_cols, strings_can_be_null=True)\n",
    "    t = pv.read_csv(src_csv, read_options=read_opts, convert_options=conv_opts)\n",
    "\n",
    "    if \"run_id\" not in t.schema.names:\n",
    "        stem = os.path.splitext(os.path.basename(src_csv))[0]\n",
    "        run_id = pa.array(np.full(t.num_rows, stem), type=pa.large_string())\n",
    "        t = t.append_column(\"run_id\", run_id)\n",
    "\n",
    "    if \"sec_rel\" not in t.schema.names:\n",
    "        n = t.num_rows\n",
    "        sec_rel = pa.array((np.arange(n, dtype=np.int32) * np.float32(TTI_SEC)), type=pa.float32())\n",
    "        t = t.append_column(\"sec_rel\", sec_rel)\n",
    "\n",
    "    pq.write_to_dataset(t, root_path=dest_root, partition_cols=[\"run_id\"],\n",
    "                        compression=\"zstd\", use_dictionary=True)\n",
    "\n",
    "def _detect_ue_rb_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cand = [c for c in df.columns if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in df.columns if c.lower().endswith('_rb')]\n",
    "    def _key(c):\n",
    "        m = re.search(r'(\\d+)', c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "def _ensure_total_rb(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    if 'total_rb' not in df.columns:\n",
    "        df['total_rb'] = df[ue_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def _compute_shares_and_fairness(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    shares = (df[ue_cols].clip(lower=0).astype('float32') / np.float32(C_PRB))\n",
    "    shares.columns = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    for c in shares.columns:\n",
    "        df[c] = shares[c]\n",
    "    x = shares.to_numpy(dtype='float32', copy=False)\n",
    "    sum_x  = x.sum(axis=1, dtype='float32')\n",
    "    sum_x2 = (x*x).sum(axis=1, dtype='float32')\n",
    "    n = np.float32(max(1, len(ue_cols)))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        J = (sum_x*sum_x) / (n * sum_x2)\n",
    "    df['J'] = np.nan_to_num(J, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    return df\n",
    "\n",
    "def _compose_roles(df: pd.DataFrame, ue_cols: List[str], top_k: int = TOP_K) -> pd.DataFrame:\n",
    "    share_cols = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    alpha = 2.0 / (20 + 1.0)\n",
    "    for c in share_cols:\n",
    "        df[c+\"_ema\"] = df[c].astype('float32').ewm(alpha=alpha, adjust=False).mean().astype('float32')\n",
    "\n",
    "    S   = df[share_cols].to_numpy(dtype='float32', copy=False)\n",
    "    E   = df[[c+\"_ema\" for c in share_cols]].to_numpy(dtype='float32', copy=False)\n",
    "    PRB = df[ue_cols].to_numpy(dtype='int32',    copy=False)\n",
    "\n",
    "    T, U = S.shape\n",
    "    k = min(top_k, U)\n",
    "    order  = np.argsort(-E, axis=1)\n",
    "    top_ix = order[:, :k]\n",
    "\n",
    "    roles       = np.take_along_axis(S,   top_ix, axis=1).astype('float32')\n",
    "    roles_small = np.take_along_axis((PRB < SMALL_RB_EPS).astype('float32'), top_ix, axis=1)\n",
    "\n",
    "    sum_all    = S.sum(axis=1, dtype='float32')\n",
    "    sum_topk   = roles.sum(axis=1, dtype='float32')\n",
    "    rest_share = (sum_all - sum_topk).clip(min=0).astype('float32')\n",
    "\n",
    "    if U > k:\n",
    "        row_ix = np.arange(T)[:, None]\n",
    "        mask = np.ones_like(S, dtype=bool); mask[row_ix, top_ix] = False\n",
    "        cnt  = mask.sum(axis=1)\n",
    "        rest_small = ((PRB < SMALL_RB_EPS).astype('float32') * mask).sum(axis=1) / np.maximum(cnt, 1)\n",
    "    else:\n",
    "        rest_small = np.zeros(T, dtype='float32')\n",
    "\n",
    "    for i in range(k):\n",
    "        df[f'role{i+1}_share']   = roles[:, i]\n",
    "        df[f'role{i+1}_smallrb'] = roles_small[:, i]\n",
    "    for i in range(k, top_k):\n",
    "        df[f'role{i+1}_share']   = np.float32(0.0)\n",
    "        df[f'role{i+1}_smallrb'] = np.float32(0.0)\n",
    "    df['rest_share']   = rest_share\n",
    "    df['rest_smallrb'] = rest_small\n",
    "\n",
    "    sec_bin = np.floor(df['sec_rel'].to_numpy(dtype='float32')).astype('int32')\n",
    "    max_sec = int(sec_bin.max(initial=0))\n",
    "    def sec_presence(vec_bool: np.ndarray) -> np.ndarray:\n",
    "        agg = np.zeros(max_sec + 1, dtype='uint8')\n",
    "        np.maximum.at(agg, sec_bin, vec_bool.astype('uint8'))\n",
    "        return agg[sec_bin]\n",
    "    for i in range(top_k):\n",
    "        df[f'role{i+1}_present_1s'] = sec_presence((df[f'role{i+1}_share'].to_numpy() > 0))\n",
    "    df['rest_present_1s'] = sec_presence((df['rest_share'].to_numpy() > 0))\n",
    "    return df\n",
    "\n",
    "def _label_window(shares_mat: np.ndarray) -> bool:\n",
    "    if shares_mat.size == 0:\n",
    "        return False\n",
    "    top    = shares_mat[:, 0]\n",
    "    others = shares_mat[:, 1:]\n",
    "    ok = (top >= ATTACK_DOMINANCE) & (np.max(others, axis=1) <= OTHERS_MAX)\n",
    "    return (ok.mean() >= DUTY_KEEP)\n",
    "\n",
    "@dataclass\n",
    "class WindowSpec:\n",
    "    length_tti: int\n",
    "    stride_tti: int\n",
    "\n",
    "def iter_windows(df: pd.DataFrame, run_id: str, spec: WindowSpec, role_cols: List[str],\n",
    "                 phases_attack={\"attack\"}, phases_benign={\"baseline\",\"recovery\",\"benign\"},\n",
    "                 batch_windows: int = 8192):\n",
    "    W, S = spec.length_tti, spec.stride_tti\n",
    "    R = df[role_cols].to_numpy(dtype='float32', copy=False)\n",
    "    phase = df['phase'].astype(str).to_numpy()\n",
    "    T = len(df)\n",
    "    starts = np.arange(0, max(0, T - W + 1), S, dtype=np.int64)\n",
    "\n",
    "    bufX, rows = [], []\n",
    "    for s in starts:\n",
    "        e = s + W\n",
    "        blk = R[s:e, :]\n",
    "        if blk.shape[0] != W:\n",
    "            continue\n",
    "        win_phase = phase[s:e]\n",
    "        frac_attack = np.isin(win_phase, list(phases_attack)).mean()\n",
    "        frac_benign = np.isin(win_phase, list(phases_benign)).mean()\n",
    "        looks_attacky = _label_window(blk)\n",
    "        if (frac_attack >= 0.90) and looks_attacky: y = 2\n",
    "        elif (frac_benign >= 0.90):                y = 0\n",
    "        else:                                      y = 1\n",
    "\n",
    "        bufX.append(blk)\n",
    "        rows.append((run_id, int(s), int(e), int(W), int(y),\n",
    "                     float(frac_attack), float(frac_benign), int(looks_attacky)))\n",
    "\n",
    "        if len(bufX) == batch_windows:\n",
    "            X = np.stack(bufX, axis=0)\n",
    "            meta = pd.DataFrame.from_records(\n",
    "                rows,\n",
    "                columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                         \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "            )\n",
    "            yield X, meta\n",
    "            bufX.clear(); rows.clear()\n",
    "\n",
    "    if bufX:\n",
    "        X = np.stack(bufX, axis=0)\n",
    "        meta = pd.DataFrame.from_records(\n",
    "            rows,\n",
    "            columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                     \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "        )\n",
    "        yield X, meta\n",
    "\n",
    "# -------------------- Main entry: build datasets (partitioned, low RAM) --------------------\n",
    "\n",
    "csv_to_parquet(ATTACK_FN, \"parquet/attack\")\n",
    "csv_to_parquet(BENIGN_FN, \"parquet/benign\")\n",
    "\n",
    "def iter_run_partitions(root: str):\n",
    "    for run_dir in sorted(glob(os.path.join(root, \"run_id=*\"))):\n",
    "        rid = os.path.basename(run_dir).split(\"=\", 1)[1]\n",
    "        df = pd.read_parquet(run_dir, engine=\"pyarrow\")\n",
    "        if 'run_id' not in df.columns:\n",
    "            df['run_id'] = rid\n",
    "        if 'phase' not in df.columns:\n",
    "            df['phase'] = 'unknown'\n",
    "        if 'sec_rel' not in df.columns:\n",
    "            df['sec_rel'] = (np.arange(len(df), dtype=np.int32) * np.float32(TTI_SEC)).astype('float32')\n",
    "        yield rid, df\n",
    "\n",
    "def process_source(root: str, src_name: str):\n",
    "    if not os.path.isdir(root):\n",
    "        print(f\"[{src_name}] No parquet root found at {root} (skipping).\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"win_shards\", exist_ok=True)\n",
    "    shard_counter = itertools.count(0)\n",
    "\n",
    "    runs = list(iter_run_partitions(root))\n",
    "    if not runs:\n",
    "        print(f\"[{src_name}] No run_id partitions found in {root}.\")\n",
    "        return\n",
    "    print(f\"[{src_name}] Found {len(runs)} run_id partitions.\")\n",
    "\n",
    "    for rid, df in runs:\n",
    "        ue_cols = _detect_ue_rb_columns(df)\n",
    "        if not ue_cols:\n",
    "            warnings.warn(f\"[{src_name}] run_id={rid}: no UE *_rb columns; skipping\")\n",
    "            continue\n",
    "\n",
    "        df = _ensure_total_rb(df, ue_cols)\n",
    "        df = _compute_shares_and_fairness(df, ue_cols)\n",
    "        df = _compose_roles(df, ue_cols, top_k=TOP_K)\n",
    "\n",
    "        keep = [f'role{i}_share' for i in range(1, TOP_K+1)] + ['rest_share','J','sec_rel','run_id','phase']\n",
    "        enriched_out = f\"{src_name}_enriched_{rid}.parquet\"\n",
    "        df[keep].to_parquet(enriched_out, index=False)\n",
    "        print(f\"[{src_name}] run_id={rid} → {enriched_out}  (rows={len(df):,})\")\n",
    "\n",
    "        role_cols = [f'role{k}_share' for k in range(1, TOP_K+1)] + ['rest_share']\n",
    "        g = df.reset_index(drop=True)\n",
    "\n",
    "        for stride in STRIDES_LONG:\n",
    "            spec = WindowSpec(length_tti=W_LONG, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"long\"\n",
    "                meta.to_parquet(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "        for stride in STRIDES_SHORT:\n",
    "            spec = WindowSpec(length_tti=W_SHORT, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"short\"\n",
    "                meta.to_parquet(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "process_source(\"parquet/attack\", \"attack\")\n",
    "process_source(\"parquet/benign\", \"benign\")\n",
    "\n",
    "def load_all_meta(kind: str) -> pd.DataFrame:\n",
    "    metas = [pd.read_parquet(p) for p in glob(f\"win_shards/{kind}_*_meta.parquet\")]\n",
    "    return pd.concat(metas, ignore_index=True) if metas else pd.DataFrame()\n",
    "\n",
    "M_long  = load_all_meta(\"long\")\n",
    "M_short = load_all_meta(\"short\")\n",
    "\n",
    "def grouped_kfold(meta: pd.DataFrame, K: int = 5, seed: int = 42) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    if meta.empty:\n",
    "        return []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    run_ids = meta['run_id'].astype(str).unique().tolist()\n",
    "    rng.shuffle(run_ids)\n",
    "    folds = [set() for _ in range(K)]\n",
    "    for i, rid in enumerate(run_ids):\n",
    "        folds[i % K].add(rid)\n",
    "    splits = []\n",
    "    for i in range(K):\n",
    "        val_rids = folds[i]\n",
    "        train_idx = meta.index[~meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        val_idx   = meta.index[ meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        splits.append((train_idx, val_idx))\n",
    "    return splits\n",
    "\n",
    "splits_long  = grouped_kfold(M_long,  K=5, seed=2025)\n",
    "splits_short = grouped_kfold(M_short, K=5, seed=2025)\n",
    "\n",
    "if len(M_long):\n",
    "    print(f\"\\n[long] meta={M_long.shape}, groups={M_long['run_id'].nunique()}, folds={len(splits_long)}\")\n",
    "if len(M_short):\n",
    "    print(f\"[short] meta={M_short.shape}, groups={M_short['run_id'].nunique()}, folds={len(splits_short)}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb5f5-a951-47f5-99df-6b9d5a8601e9",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "This code defines a high-throughput PyTorch data pipeline for the TTI-Trust project that streams scheduler-native, identity-agnostic features directly from precomputed NPZ shards and their parquet metadata. It provides a set of NumPy-based feature helpers to compute Jain’s fairness index over time, causal rolling minima/medians, run-lengths of “small RB” streaks, approximate PRB plateau histograms, benign-grant fractions, and contiguous zero-run statistics across non-dominant UEs. The `TTINPZShardBatches` `IterableDataset` discovers shard files under `win_shards`, loads each shard’s `[N, W, D]` role-based share tensor and labels, and for each batch constructs a rich sequential feature tensor `X_seq` by concatenating the original shares, temporal deltas, fairness trajectories (instant, min, median), and small-RB run-lengths across the window. In parallel, it builds a per-window auxiliary feature vector `X_aux` that captures plateau occupancy, how often other UEs receive grants, zero-run lengths, and placeholder “radio hint” slots, while also packaging minimal metadata (run ID, source, stride, window bounds) into a Python dict list. The `make_loader` factory wraps this iterable in a PyTorch `DataLoader` configured for streaming (no secondary batching), with multiple workers, pinned memory, persistent workers, and prefetching tuned for large datasets. A small smoke test at the end attempts to instantiate short-window loaders, pull the first batch, and print tensor shapes and sample metadata, illustrating how downstream models should consume `(xb_seq, xb_aux, yb, mb)` in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c496f167-721e-4197-9c1e-961bf4260af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming loader ready. First batch shape:\n",
      "X_seq: (1024, 64, 24) X_aux: (1024, 44) y: (1024,) meta[0]: {'run_id': '040933', 'source': 'attack', 'stride': 4, 'window_len': 64, 'start_tti': 0, 'end_tti': 64}\n",
      "\n",
      "Use: for xb_seq, xb_aux, yb, mb in make_loader('short', ...): model(xb_seq, xb_aux) ...\n"
     ]
    }
   ],
   "source": [
    "# PyTorch feature space & data loader for TTI-Trust (scheduler-native, identity-agnostic)\n",
    "# Optimized to stream large datasets from shards: win_shards/{long,short}_s*_... .npz + *_meta.parquet\n",
    "#\n",
    "# Emits batches directly (IterableDataset) to avoid in-memory concatenation or per-item pandas operations.\n",
    "\n",
    "import os, re, glob, math\n",
    "from typing import Dict, List, Tuple, Optional, Iterator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "# --- Shared constants (align with preprocessing) ---\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "TOP_K   = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "PLATEAUS = np.array([16, 31, 46, 61, 76, 91, 106], dtype=np.int32)\n",
    "\n",
    "# ---------- Feature helpers (NumPy; no pandas in the hot path) ----------\n",
    "\n",
    "def jains_fairness_seq(shares: np.ndarray) -> np.ndarray:\n",
    "    # shares: [N, W, D] or [W, D]\n",
    "    if shares.ndim == 2:\n",
    "        shares = shares[None, ...]\n",
    "    sum_x  = shares.sum(axis=2, dtype=np.float32)                     # [N, W]\n",
    "    sum_x2 = (shares * shares).sum(axis=2, dtype=np.float32)          # [N, W]\n",
    "    n = shares.shape[2]\n",
    "    J = (sum_x * sum_x) / (np.float32(n) * sum_x2 + 1e-9)\n",
    "    return J.squeeze(0).astype(np.float32) if J.shape[0] == 1 else J.astype(np.float32)\n",
    "\n",
    "def rolling_min_med_causal(x: np.ndarray, win: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # x: [W] (per-window). W<=240 → simple loop is fine and cache-friendly.\n",
    "    W = x.shape[0]\n",
    "    rmin = np.empty(W, dtype=np.float32)\n",
    "    rmed = np.empty(W, dtype=np.float32)\n",
    "    for t in range(W):\n",
    "        a = max(0, t - win + 1)\n",
    "        sl = x[a:t+1]\n",
    "        rmin[t] = sl.min() if sl.size else 0.0\n",
    "        rmed[t] = np.median(sl) if sl.size else 0.0\n",
    "    return rmin, rmed\n",
    "\n",
    "def small_rb_runlength_1d(streak_bin: np.ndarray) -> np.ndarray:\n",
    "    out = np.zeros_like(streak_bin, dtype=np.float32)\n",
    "    c = 0.0\n",
    "    for i, b in enumerate(streak_bin.astype(bool)):\n",
    "        c = c + 1.0 if b else 0.0\n",
    "        out[i] = c\n",
    "    return out\n",
    "\n",
    "def build_c_runs(srb_bin: np.ndarray) -> np.ndarray:\n",
    "    # srb_bin: [W, D] 0/1 → run-length per channel\n",
    "    return np.stack([small_rb_runlength_1d(srb_bin[:, j]) for j in range(srb_bin.shape[1])], axis=1).astype(np.float32)\n",
    "\n",
    "def approx_plateau_hist(share_seq: np.ndarray) -> np.ndarray:\n",
    "    # share_seq: [W, D] → PRBs → closest plateau histogram normalized\n",
    "    prbs = np.rint(share_seq * C_PRB).astype(np.int32)\n",
    "    idx  = np.abs(prbs[:, :, None] - PLATEAUS[None, None, :]).argmin(axis=2)  # [W, D]\n",
    "    hist = np.bincount(idx.ravel(), minlength=len(PLATEAUS)).astype(np.float32)\n",
    "    if hist.sum() > 0: hist /= hist.sum()\n",
    "    return hist\n",
    "\n",
    "def benign_grant_fraction(share_seq: np.ndarray) -> float:\n",
    "    others = share_seq[:, 1:]\n",
    "    return float((others > 0).any(axis=1).mean())\n",
    "\n",
    "def contiguous_zero_runs(share_seq: np.ndarray) -> Tuple[float, float]:\n",
    "    others_all_zero = (share_seq[:, 1:] == 0).all(axis=1).astype(np.int32)  # [W]\n",
    "    runs, c = [], 0\n",
    "    for b in others_all_zero:\n",
    "        if b: c += 1\n",
    "        else:\n",
    "            if c: runs.append(c); c = 0\n",
    "    if c: runs.append(c)\n",
    "    return (0.0, 0.0) if not runs else (float(max(runs)), float(np.mean(runs)))\n",
    "\n",
    "# ---------- IterableDataset over shard files ----------\n",
    "\n",
    "class TTINPZShardBatches(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streams batches from NPZ shards produced by preprocessing.\n",
    "    Each shard file has X: [N, W, D] where D = TOP_K+1 (roles incl. rest), and a paired *_meta.parquet.\n",
    "\n",
    "    Yields (X_seq, X_aux, y, meta_list) where:\n",
    "      X_seq: [B, W, Dseq]  (roles, d_roles, J, J_min, J_med, smallRB runs)\n",
    "      X_aux: [B, Daux]     (plateau hist, benign frac, zero-run stats, radio hints if available)\n",
    "      y:     [B] {0,1,2}\n",
    "      meta_list: list of dicts with run_id/source/stride/window_len/start_tti/end_tti\n",
    "    \"\"\"\n",
    "    def __init__(self, kind: str, batch_size: int = 1024, shard_glob: Optional[str] = None,\n",
    "                 restrict_sources: Optional[List[str]] = None):\n",
    "        assert kind in (\"long\", \"short\")\n",
    "        self.kind = kind\n",
    "        self.batch_size = int(batch_size)\n",
    "        # Discover shards\n",
    "        patt = shard_glob or f\"win_shards/{kind}_*.npz\"\n",
    "        self.shard_paths = sorted(glob.glob(patt))\n",
    "        if not self.shard_paths:\n",
    "            raise FileNotFoundError(f\"No shards found for pattern: {patt}. Run preprocessing to generate win_shards.\")\n",
    "        self.restrict_sources = set(restrict_sources) if restrict_sources else None\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        for npz_path in self.shard_paths:\n",
    "            meta_path = npz_path.replace(\".npz\", \"_meta.parquet\")\n",
    "            if not os.path.exists(meta_path):\n",
    "                continue\n",
    "            meta = pd.read_parquet(meta_path)\n",
    "            if self.restrict_sources is not None:\n",
    "                if \"source\" in meta.columns:\n",
    "                    if not any(src in self.restrict_sources for src in meta[\"source\"].unique().tolist()):\n",
    "                        continue\n",
    "            X = np.load(npz_path)[\"X\"]  # [N, W, D], float32 from preprocessing\n",
    "            # Labels\n",
    "            if \"label\" not in meta.columns:\n",
    "                raise ValueError(f\"Meta {meta_path} missing 'label' column.\")\n",
    "            y_all = meta[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "            # Optional radio hints (rare). If present as per-TTI per-role, we’d read a second array.\n",
    "            have_radio = False  # keep simple; emit zeros + flag\n",
    "\n",
    "            N, W, D = X.shape\n",
    "            assert D == (TOP_K + 1), f\"Expected D=TOP_K+1, got D={D}\"\n",
    "\n",
    "            # Batch over this shard\n",
    "            for i0 in range(0, N, self.batch_size):\n",
    "                i1 = min(N, i0 + self.batch_size)\n",
    "                seq = X[i0:i1]                                  # [B, W, D]\n",
    "                # Δshares\n",
    "                dseq = np.diff(seq, axis=1, prepend=seq[:, 0:1, :])  # [B, W, D]\n",
    "\n",
    "                # Jain's J(t) + causal rolling stats (per sample)\n",
    "                J_list, Jmin_list, Jmed_list = [], [], []\n",
    "                for b in range(seq.shape[0]):\n",
    "                    Jb = jains_fairness_seq(seq[b])                   # [W]\n",
    "                    Jmin, Jmed = rolling_min_med_causal(Jb, win=min(60, Jb.shape[0]))\n",
    "                    J_list.append(Jb); Jmin_list.append(Jmin); Jmed_list.append(Jmed)\n",
    "                J   = np.stack(J_list,    axis=0)[:, :, None]         # [B, W, 1]\n",
    "                Jmn = np.stack(Jmin_list, axis=0)[:, :, None]         # [B, W, 1]\n",
    "                Jmd = np.stack(Jmed_list, axis=0)[:, :, None]         # [B, W, 1]\n",
    "\n",
    "                # small-RB run-lengths (approx from shares threshold)\n",
    "                prb = np.rint(seq * C_PRB)                            # [B, W, D]\n",
    "                srb = (prb < SMALL_RB_EPS).astype(np.float32)\n",
    "                c_runs = np.stack([build_c_runs(srb[b]) for b in range(srb.shape[0])], axis=0)  # [B, W, D]\n",
    "\n",
    "                # Time-step channels\n",
    "                X_seq = np.concatenate([seq, dseq, J, Jmn, Jmd, c_runs], axis=2).astype(np.float32)  # [B, W, Dseq]\n",
    "\n",
    "                # Aux features per window\n",
    "                aux_list = []\n",
    "                for b in range(seq.shape[0]):\n",
    "                    ph = approx_plateau_hist(seq[b])\n",
    "                    bf = benign_grant_fraction(seq[b])\n",
    "                    zmax, zmean = contiguous_zero_runs(seq[b])\n",
    "                    if have_radio:\n",
    "                        radio_aux = np.zeros(33, dtype=np.float32)\n",
    "                        radio_flag = 1.0\n",
    "                    else:\n",
    "                        radio_aux = np.zeros(33, dtype=np.float32)\n",
    "                        radio_flag = 0.0\n",
    "                    aux = np.concatenate([ph, np.array([bf, zmax, zmean], dtype=np.float32),\n",
    "                                          radio_aux, np.array([radio_flag], dtype=np.float32)])\n",
    "                    aux_list.append(aux)\n",
    "                X_aux = np.stack(aux_list, axis=0).astype(np.float32)  # [B, 44]\n",
    "\n",
    "                y = torch.from_numpy(y_all[i0:i1])                     # [B], int64\n",
    "\n",
    "                # Build meta dicts list (keep only essentials)\n",
    "                meta_batch = []\n",
    "                cols = meta.columns\n",
    "                for j in range(i0, i1):\n",
    "                    row = meta.iloc[j]\n",
    "                    meta_batch.append({\n",
    "                        \"run_id\": str(row.get(\"run_id\", \"\")),\n",
    "                        \"source\": str(row.get(\"source\", \"\")),\n",
    "                        \"stride\": int(row.get(\"stride\", 0)),\n",
    "                        \"window_len\": int(row.get(\"window_len\", seq.shape[1])),\n",
    "                        \"start_tti\": int(row.get(\"start_tti\", 0)),\n",
    "                        \"end_tti\": int(row.get(\"end_tti\", 0)),\n",
    "                    })\n",
    "\n",
    "                # Zero-copy to torch for X_seq/X_aux\n",
    "                yield torch.from_numpy(X_seq), torch.from_numpy(X_aux), y, meta_batch\n",
    "\n",
    "# ---------- High-throughput DataLoader factory ----------\n",
    "\n",
    "def make_loader(kind: str, batch_size: int = 1024, num_workers: int = 4,\n",
    "                restrict_sources: Optional[List[str]] = None) -> DataLoader:\n",
    "    ds = TTINPZShardBatches(kind=kind, batch_size=batch_size, restrict_sources=restrict_sources)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=None,               # dataset already yields batches\n",
    "        num_workers=num_workers,       # tune: 2–8 based on CPU/IO\n",
    "        pin_memory=True,               # faster H2D copies\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# ---------- Smoke test (requires win_shards present) ----------\n",
    "\n",
    "try:\n",
    "    train_loader = make_loader(kind=\"short\", batch_size=1024, num_workers=4)\n",
    "    val_loader   = make_loader(kind=\"short\", batch_size=1024, num_workers=4)\n",
    "    print(\"Streaming loader ready. First batch shape:\")\n",
    "    xb_seq, xb_aux, yb, mb = next(iter(train_loader))\n",
    "    print(\"X_seq:\", tuple(xb_seq.shape), \"X_aux:\", tuple(xb_aux.shape), \"y:\", tuple(yb.shape), \"meta[0]:\", mb[0])\n",
    "except Exception as e:\n",
    "    print(\"Note:\", str(e))\n",
    "\n",
    "print(\"\\nUse: for xb_seq, xb_aux, yb, mb in make_loader('short', ...): model(xb_seq, xb_aux) ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a4651-9ce3-4d5c-9178-84f6701ec6cb",
   "metadata": {},
   "source": [
    "### Base Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1bacf-e010-4743-a55b-04e003af9c1f",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE Architecture\n",
    "This code defines `PRBGraphSAGE`, a configurable PyTorch Geometric graph neural network for classifying UE-level contention graphs in a PRB starvation detection task. The model builds a GNN backbone with a user-specified number of layers (`num_layers`), hidden dimension, and convolution type (`SAGEConv`, `GCNConv`, `GraphConv`, or `GATv2Conv`), optionally followed by batch normalization and dropout after each layer. In the forward pass, node features `x` are iteratively updated using the chosen conv layers over edges `edge_index`, then aggregated into graph-level embeddings via a selectable global pooling strategy (`mean`, `max`, or concatenated `mean+max`). This pooled representation is passed through an MLP head whose depth and hidden size are also configurable, producing either a scalar logit per graph for binary classification (`num_classes == 1`) or a multi-dimensional logit vector for multi-class problems. The interface is designed to be hyperparameter-sweep friendly, with `edge_attr` included in the signature as a placeholder for future edge-aware enhancements, while the current implementation focuses on node features and graph-level pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd70b31c-f0ce-489f-92a2-f258c2632f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv,\n",
    "    GCNConv,\n",
    "    GraphConv,\n",
    "    GATv2Conv,\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    ")\n",
    "\n",
    "\n",
    "class PRBGraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    PRB-GraphSAGE: UE-level contention graph classifier for PRB starvation detection.\n",
    "\n",
    "    This model is designed to be easy to sweep over hyperparameters:\n",
    "      - in_dim:         node feature dimension\n",
    "      - hidden_dim:     hidden channel width\n",
    "      - num_layers:     number of GNN layers (>=1)\n",
    "      - conv_type:      GNN layer type: {\"sage\", \"gcn\", \"graph\", \"gat\"}\n",
    "      - aggr:           global pooling: {\"mean\", \"max\", \"mean+max\"}\n",
    "      - mlp_hidden_dim: hidden size of the final MLP head\n",
    "      - mlp_layers:     number of linear layers in the MLP head (>=1)\n",
    "      - dropout:        dropout probability applied after each conv & MLP layer\n",
    "      - num_classes:    1 for binary logit, >1 for multi-class logits\n",
    "\n",
    "    Forward signature:\n",
    "        logits = model(x, edge_index, batch, edge_attr=None)\n",
    "\n",
    "      - x:          [N, in_dim] node features\n",
    "      - edge_index: [2, E] COO edge indices\n",
    "      - batch:      [N] graph IDs for global pooling\n",
    "      - edge_attr:  [E, d_edge] (currently ignored; placeholder for future use)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        conv_type: str = \"sage\",\n",
    "        aggr: str = \"mean\",\n",
    "        mlp_hidden_dim: int = 64,\n",
    "        mlp_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 1,\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert num_layers >= 1, \"num_layers must be >= 1\"\n",
    "        assert mlp_layers >= 1, \"mlp_layers must be >= 1\"\n",
    "        assert conv_type in {\"sage\", \"gcn\", \"graph\", \"gat\"}, f\"Unknown conv_type: {conv_type}\"\n",
    "        assert aggr in {\"mean\", \"max\", \"mean+max\"}, f\"Unknown aggr: {aggr}\"\n",
    "        self.aggr = aggr\n",
    "        self.dropout = float(dropout)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.use_batchnorm = bool(use_batchnorm)\n",
    "\n",
    "        # ---- GNN backbone ----\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() if use_batchnorm else None\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(self._make_conv(conv_type, in_dim, hidden_dim))\n",
    "        if use_batchnorm:\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(self._make_conv(conv_type, hidden_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # ---- Readout (global pooling) ----\n",
    "        # mean+max concatenation doubles the channel dimension\n",
    "        readout_dim = hidden_dim if aggr in {\"mean\", \"max\"} else hidden_dim * 2\n",
    "\n",
    "        # ---- MLP classifier head ----\n",
    "        mlp_layers_list = []\n",
    "        in_mlp = readout_dim\n",
    "        for li in range(mlp_layers):\n",
    "            out_mlp = mlp_hidden_dim if li < mlp_layers - 1 else num_classes\n",
    "            mlp_layers_list.append(nn.Linear(in_mlp, out_mlp))\n",
    "            if li < mlp_layers - 1:\n",
    "                mlp_layers_list.append(nn.SiLU())\n",
    "                if dropout > 0:\n",
    "                    mlp_layers_list.append(nn.Dropout(dropout))\n",
    "            in_mlp = out_mlp\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_conv(conv_type: str, in_dim: int, out_dim: int):\n",
    "        \"\"\"Factory for different conv types to make hyperparameter sweeps easy.\"\"\"\n",
    "        if conv_type == \"sage\":\n",
    "            return SAGEConv(in_dim, out_dim)\n",
    "        if conv_type == \"gcn\":\n",
    "            return GCNConv(in_dim, out_dim)\n",
    "        if conv_type == \"graph\":\n",
    "            return GraphConv(in_dim, out_dim)\n",
    "        if conv_type == \"gat\":\n",
    "            return GATv2Conv(in_dim, out_dim, heads=1, concat=False)\n",
    "        raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        \"\"\"\n",
    "        x:          [N, in_dim]\n",
    "        edge_index: [2, E]\n",
    "        batch:      [N] graph IDs\n",
    "        edge_attr:  [E, d_edge] (ignored for now; reserved for future edge-aware layers)\n",
    "        \"\"\"\n",
    "        # ---- GNN backbone ----\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.silu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # ---- Global pooling ----\n",
    "        if self.aggr == \"mean\":\n",
    "            g = global_mean_pool(x, batch)\n",
    "        elif self.aggr == \"max\":\n",
    "            g = global_max_pool(x, batch)\n",
    "        else:  # \"mean+max\"\n",
    "            g_mean = global_mean_pool(x, batch)\n",
    "            g_max = global_max_pool(x, batch)\n",
    "            g = torch.cat([g_mean, g_max], dim=-1)\n",
    "\n",
    "        # ---- MLP head ----\n",
    "        logits = self.mlp(g)  # [num_graphs, num_classes]\n",
    "\n",
    "        # For binary classification, most training loops will want shape [num_graphs]\n",
    "        if self.num_classes == 1:\n",
    "            logits = logits.squeeze(-1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab57bc7-9215-4381-990f-435dcaf040ce",
   "metadata": {},
   "source": [
    "### Base Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f3d97-9ee8-48cc-8ab8-11d2d1b887ee",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "This code sets up initialization, configuration, and lightweight hyperparameter sweeping for a PRB-GraphSAGE GNN used in PRB starvation detection. It begins by seeding Python, NumPy, and PyTorch RNGs for reproducibility, choosing a CUDA or CPU device, and defining `D_NODE` as the node feature dimension (with a placeholder warning until real graph data is available). A `GNN_HYP` dictionary gathers all key hyperparameters in one place—training settings (batch size, epochs, learning rate and grid, weight decay), backbone architecture (hidden size, number of layers, dropout, conv type, pooling strategy, batchnorm), MLP head configuration, output dimension (binary vs multi-class), early-stopping criteria, and scheduler parameters (warmup and total steps). A helper `compute_pos_weight_from_labels` computes class weights for `BCEWithLogitsLoss` from imbalanced binary labels, with a default `pos_weight` of 1.0 until real labels are known. The `make_gnn_model` factory instantiates a `PRBGraphSAGE` model from a hyperparameter dict and moves it to the chosen device, while `count_params` reports trainable parameter count. Optimizer and scheduler builders create an AdamW optimizer and a LambdaLR scheduler implementing linear warmup followed by cosine decay over `total_steps`. The script then instantiates a default model, optimizer, scheduler, and a binary `BCEWithLogitsLoss` using the current `pos_weight`, printing summaries. Finally, `iter_gnn_configs` defines a simple grid-search generator over selected hyperparameters (learning rate, hidden size, number of layers, dropout, aggregation, and conv type), and the code demonstrates its use by instantiating and printing the first few candidate configurations along with their parameter counts, providing a ready-made loop for hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c6977c6-e5cc-4ed9-b8ed-935262806065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GNN Hyperparameters: {\n",
      "  \"batch_size\": 32,\n",
      "  \"epochs\": 40,\n",
      "  \"lr\": 0.0003,\n",
      "  \"lr_grid\": [\n",
      "    0.0002,\n",
      "    0.0003,\n",
      "    0.0005\n",
      "  ],\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"hidden_dim\": 64,\n",
      "  \"num_layers\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"conv_type\": \"sage\",\n",
      "  \"aggr\": \"mean\",\n",
      "  \"use_batchnorm\": true,\n",
      "  \"hidden_dim_grid\": [\n",
      "    32,\n",
      "    64\n",
      "  ],\n",
      "  \"num_layers_grid\": [\n",
      "    1,\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"dropout_grid\": [\n",
      "    0.0,\n",
      "    0.1,\n",
      "    0.3\n",
      "  ],\n",
      "  \"aggr_grid\": [\n",
      "    \"mean\",\n",
      "    \"mean+max\"\n",
      "  ],\n",
      "  \"conv_type_grid\": [\n",
      "    \"sage\",\n",
      "    \"gcn\"\n",
      "  ],\n",
      "  \"mlp_hidden_dim\": 64,\n",
      "  \"mlp_layers\": 2,\n",
      "  \"num_classes\": 1,\n",
      "  \"early_stop\": {\n",
      "    \"monitor\": \"f1\",\n",
      "    \"mode\": \"max\",\n",
      "    \"patience\": 5,\n",
      "    \"min_delta\": 0.001\n",
      "  },\n",
      "  \"warmup_steps\": 200,\n",
      "  \"total_steps\": 10000\n",
      "}\n",
      "Initial pos_weight (binary): 1.0\n",
      "GNN params: 14,849\n",
      "GNN model, optimizer, scheduler, and loss are initialized.\n",
      "\n",
      "Initialization\n",
      "  Config 0: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=sage, params=3,297\n",
      "  Config 1: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=gcn, params=2,785\n",
      "  Config 2: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean+max, conv_type=sage, params=5,345\n"
     ]
    }
   ],
   "source": [
    "# ==== CELL: GNN Model + Hyperparameter Initialization ====\n",
    "import os, math, json, random\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Reproducibility & device ----\n",
    "SEED = 2025\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_all(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "try:\n",
    "    D_NODE\n",
    "except NameError:\n",
    "    D_NODE = 16  # placeholder for later\n",
    "    print(\"WARNING: D_NODE is using a placeholder value (16). Set it to your actual node feature dim.\")\n",
    "\n",
    "# ---- GNN Hyperparameters (with small grids for sweeps) ----\n",
    "GNN_HYP: Dict = {\n",
    "    # ---- Training-level hparams ----\n",
    "    \"batch_size\": 32,              \n",
    "    \"epochs\": 40,                  \n",
    "    \"lr\": 3e-4,                    \n",
    "    \"lr_grid\": [2e-4, 3e-4, 5e-4], \n",
    "    \"weight_decay\": 1e-4,\n",
    "\n",
    "    # ---- Backbone architecture ----\n",
    "    # Base config (used if you don't sweep)\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"conv_type\": \"sage\",           \n",
    "    \"aggr\": \"mean\",                \n",
    "    \"use_batchnorm\": True,\n",
    "\n",
    "    # Grids for a compact but meaningful sweep\n",
    "    \"hidden_dim_grid\": [32, 64],           \n",
    "    \"num_layers_grid\": [1, 2, 3],          \n",
    "    \"dropout_grid\": [0.0, 0.1, 0.3],       \n",
    "    \"aggr_grid\": [\"mean\", \"mean+max\"],     \n",
    "    \"conv_type_grid\": [\"sage\", \"gcn\"],     \n",
    "\n",
    "    # ---- MLP head ----\n",
    "    \"mlp_hidden_dim\": 64,\n",
    "    \"mlp_layers\": 2,\n",
    "\n",
    "    # ---- Output ----\n",
    "    # 1 = binary logit (BCEWithLogits); >1 = multi-class (CrossEntropy)\n",
    "    \"num_classes\": 1,\n",
    "\n",
    "    # ---- Early-stop config (used by your training loop) ----\n",
    "    \"early_stop\": {\n",
    "        \"monitor\": \"f1\",            \n",
    "        \"mode\": \"max\",              \n",
    "        \"patience\": 5,              \n",
    "        \"min_delta\": 1e-3,          \n",
    "    },\n",
    "\n",
    "    # ---- Scheduler placeholders (updated once loaders are known) ----\n",
    "    \"warmup_steps\": 200,\n",
    "    \"total_steps\": 10000,\n",
    "}\n",
    "\n",
    "print(\"GNN Hyperparameters:\", json.dumps(GNN_HYP, indent=2))\n",
    "\n",
    "\n",
    "# ---- Optional class weights for binary BCEWithLogitsLoss ----\n",
    "# You can adapt this to your graph meta if you materialize labels in a DataFrame.\n",
    "def compute_pos_weight_from_labels(labels: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pos_weight for BCEWithLogitsLoss from a binary label vector.\n",
    "    pos_weight = (#neg / #pos)\n",
    "    \"\"\"\n",
    "    labels = labels.astype(np.int64).ravel()\n",
    "    pos = (labels == 1).sum()\n",
    "    neg = (labels == 0).sum()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0, dtype=torch.float32)\n",
    "    return torch.tensor(float(neg) / float(pos), dtype=torch.float32)\n",
    "\n",
    "pos_weight = torch.tensor(1.0, dtype=torch.float32)\n",
    "print(\"Initial pos_weight (binary):\", float(pos_weight))\n",
    "\n",
    "\n",
    "# ---- PRB-GraphSAGE factory ----\n",
    "def make_gnn_model(hyp: Dict, in_dim: int) -> PRBGraphSAGE:\n",
    "    \"\"\"\n",
    "    Instantiate a PRBGraphSAGE model from a hyperparameter dict.\n",
    "    \"\"\"\n",
    "    model = PRBGraphSAGE(\n",
    "        in_dim=in_dim,\n",
    "        hidden_dim=hyp[\"hidden_dim\"],\n",
    "        num_layers=hyp[\"num_layers\"],\n",
    "        conv_type=hyp[\"conv_type\"],      \n",
    "        aggr=hyp[\"aggr\"],                \n",
    "        mlp_hidden_dim=hyp[\"mlp_hidden_dim\"],\n",
    "        mlp_layers=hyp[\"mlp_layers\"],\n",
    "        dropout=hyp[\"dropout\"],\n",
    "        num_classes=hyp[\"num_classes\"],\n",
    "        use_batchnorm=hyp[\"use_batchnorm\"],\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# ---- Optimizer & Scheduler helpers ----\n",
    "def make_gnn_optimizer(model: nn.Module, hyp: Dict) -> torch.optim.Optimizer:\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hyp[\"lr\"],\n",
    "        weight_decay=hyp[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "def make_scheduler(optimizer, warmup_steps: int, total_steps: int):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "# ---- Instantiate a default GNN model + optimizer/scheduler ----\n",
    "gnn_model = make_gnn_model(GNN_HYP, D_NODE)\n",
    "print(f\"GNN params: {count_params(gnn_model):,}\")\n",
    "\n",
    "gnn_optim = make_gnn_optimizer(gnn_model, GNN_HYP)\n",
    "gnn_scheduler = make_scheduler(gnn_optim, GNN_HYP[\"warmup_steps\"], GNN_HYP[\"total_steps\"])\n",
    "\n",
    "# Binary classification by default (y ∈ {0,1})\n",
    "gnn_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "\n",
    "print(\"GNN model, optimizer, scheduler, and loss are initialized.\")\n",
    "\n",
    "\n",
    "# ---- Hyperparameter sweep helper (config generator only) ----\n",
    "def iter_gnn_configs(base: Dict) -> Iterable[Dict]:\n",
    "    \"\"\"\n",
    "    Simple grid over a few key hyperparameters.\n",
    "    You can extend/prune this as needed.\n",
    "    \"\"\"\n",
    "    for lr in base[\"lr_grid\"]:\n",
    "        for hidden in base[\"hidden_dim_grid\"]:\n",
    "            for num_layers in base[\"num_layers_grid\"]:\n",
    "                for dropout in base[\"dropout_grid\"]:\n",
    "                    for aggr in base[\"aggr_grid\"]:\n",
    "                        for conv_type in base.get(\"conv_type_grid\", [base[\"conv_type\"]]):\n",
    "                            cfg = dict(base)  # shallow copy\n",
    "                            cfg[\"lr\"] = lr\n",
    "                            cfg[\"hidden_dim\"] = hidden\n",
    "                            cfg[\"num_layers\"] = num_layers\n",
    "                            cfg[\"dropout\"] = dropout\n",
    "                            cfg[\"aggr\"] = aggr\n",
    "                            cfg[\"conv_type\"] = conv_type\n",
    "                            yield cfg\n",
    "\n",
    "print(\"\\nInitialization\")\n",
    "for i, cfg in enumerate(iter_gnn_configs(GNN_HYP)):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    m = make_gnn_model(cfg, D_NODE)\n",
    "    print(\n",
    "        f\"  Config {i}: lr={cfg['lr']}, hidden={cfg['hidden_dim']}, \"\n",
    "        f\"layers={cfg['num_layers']}, dropout={cfg['dropout']}, \"\n",
    "        f\"aggr={cfg['aggr']}, conv_type={cfg['conv_type']}, \"\n",
    "        f\"params={count_params(m):,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdb14c-0f22-4a7f-97fc-c06bf43e041d",
   "metadata": {},
   "source": [
    "### Model Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbff31-a9a3-4872-b2a6-effa69065f94",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4fd7f-efc1-4d51-b5bf-d124492e7d12",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41fb7f5c-cae6-4811-8b28-7f1e8f0795b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_binary_metrics(y_true: np.ndarray,\n",
    "                           y_prob: np.ndarray,\n",
    "                           threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    y_true: [N] in {0,1}\n",
    "    y_prob: [N] in [0,1]  (sigmoid outputs)\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64).ravel()\n",
    "    y_prob = y_prob.astype(np.float32).ravel()\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64)\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    eps = 1e-9\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    acc       = (tp + tn) / max(1, len(y_true))\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "    }\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion: nn.Module,\n",
    "              device: torch.device,\n",
    "              scheduler=None,\n",
    "              train: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Runs one epoch over loader.\n",
    "    Returns (avg_loss, metrics_dict).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_probs: List[float] = []\n",
    "    all_labels: List[int] = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        # y: [num_graphs] or [num_graphs, 1]\n",
    "        y = batch.y.view(-1).float()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)  # [num_graphs] or [num_graphs,1]\n",
    "            logits = logits.view_as(y)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits.detach()).cpu().numpy()\n",
    "        labels = y.detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        return 0.0, {\"acc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    metrics = compute_binary_metrics(all_labels, all_probs, threshold=0.5)\n",
    "    metrics[\"loss\"] = float(avg_loss)\n",
    "    return avg_loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43f972-6356-4b02-984b-5012b5ad0c4c",
   "metadata": {},
   "source": [
    "##### Training Loop (with Early Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63299bb3-02b7-45cb-b437-23dd9b996ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, monitor: str = \"f1\", mode: str = \"max\",\n",
    "                 patience: int = 5, min_delta: float = 1e-3):\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.best = -float(\"inf\")\n",
    "        elif mode == \"min\":\n",
    "            self.best = float(\"inf\")\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'max' or 'min'\")\n",
    "\n",
    "        self.num_bad = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metrics: Dict[str, float], model: nn.Module) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if we should stop (patience exhausted).\n",
    "        \"\"\"\n",
    "        current = metrics.get(self.monitor, None)\n",
    "        if current is None:\n",
    "            return False\n",
    "\n",
    "        improved = False\n",
    "        if self.mode == \"max\":\n",
    "            if current > self.best + self.min_delta:\n",
    "                improved = True\n",
    "        else:  # 'min'\n",
    "            if current < self.best - self.min_delta:\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            self.best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "\n",
    "        return self.num_bad >= self.patience\n",
    "\n",
    "\n",
    "def train_gnn_model(\n",
    "    hyp: Dict,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    in_dim: int,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    High-level training loop for a single GNN config.\n",
    "    Returns a dict with best metrics, history, and best_state_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    model = make_gnn_model(hyp, in_dim=in_dim)\n",
    "    optimizer = make_gnn_optimizer(model, hyp)\n",
    "    scheduler = make_scheduler(optimizer, hyp[\"warmup_steps\"], hyp[\"total_steps\"])\n",
    "\n",
    "    criterion = gnn_criterion \n",
    "\n",
    "    es_cfg = hyp[\"early_stop\"]\n",
    "    early_stopper = EarlyStopper(\n",
    "        monitor=es_cfg.get(\"monitor\", \"f1\"),\n",
    "        mode=es_cfg.get(\"mode\", \"max\"),\n",
    "        patience=es_cfg.get(\"patience\", 5),\n",
    "        min_delta=es_cfg.get(\"min_delta\", 1e-3),\n",
    "    )\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    for epoch in range(1, hyp[\"epochs\"] + 1):\n",
    "        train_loss, train_metrics = run_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, scheduler, train=True\n",
    "        )\n",
    "        val_loss, val_metrics = run_epoch(\n",
    "            model, val_loader, optimizer, criterion, device, scheduler=None, train=False\n",
    "        )\n",
    "\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:03d}] \"\n",
    "            f\"train_loss={train_metrics['loss']:.4f}, train_f1={train_metrics['f1']:.3f} | \"\n",
    "            f\"val_loss={val_metrics['loss']:.4f}, val_f1={val_metrics['f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "        stop = early_stopper.step(val_metrics, model)\n",
    "        if stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Best {early_stopper.monitor} = {early_stopper.best:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights if we got any improvement\n",
    "    if early_stopper.best_state is not None:\n",
    "        model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    result = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"best_metric\": early_stopper.best,\n",
    "        \"best_monitor\": early_stopper.monitor,\n",
    "        \"best_state_dict\": early_stopper.best_state,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fdd40-69fd-44b7-b30e-41a905f649d2",
   "metadata": {},
   "source": [
    "##### Initiate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae6d6496-d571-4b5e-acde-c5c36ba86087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0002 ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m iter_gnn_configs(GNN_HYP):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Config:\u001b[39m\u001b[33m\"\u001b[39m, cfg[\u001b[33m\"\u001b[39m\u001b[33mconv_type\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mhidden_dim\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_layers\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m           cfg[\u001b[33m\"\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33maggr\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m, cfg[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     out = \u001b[43mtrain_gnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD_NODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m best_overall \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m out[\u001b[33m\"\u001b[39m\u001b[33mbest_metric\u001b[39m\u001b[33m\"\u001b[39m] > best_overall[\u001b[33m\"\u001b[39m\u001b[33mbest_metric\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     10\u001b[39m         best_overall = out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain_gnn_model\u001b[39m\u001b[34m(hyp, train_loader, val_loader, in_dim, device)\u001b[39m\n\u001b[32m     71\u001b[39m history = {\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, hyp[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     train_loss, train_metrics = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     val_loss, val_metrics = run_epoch(\n\u001b[32m     78\u001b[39m         model, val_loader, optimizer, criterion, device, scheduler=\u001b[38;5;28;01mNone\u001b[39;00m, train=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     79\u001b[39m     )\n\u001b[32m     81\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append(train_metrics)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, scheduler, train)\u001b[39m\n\u001b[32m     55\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     batch = \u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# y: [num_graphs] or [num_graphs, 1]\u001b[39;00m\n\u001b[32m     60\u001b[39m     y = batch.y.view(-\u001b[32m1\u001b[39m).float()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "best_overall = None\n",
    "best_cfg = None\n",
    "\n",
    "for cfg in iter_gnn_configs(GNN_HYP):\n",
    "    print(\"\\n=== Config:\", cfg[\"conv_type\"], cfg[\"hidden_dim\"], cfg[\"num_layers\"],\n",
    "          cfg[\"dropout\"], cfg[\"aggr\"], \"lr\", cfg[\"lr\"], \"===\")\n",
    "    out = train_gnn_model(cfg, train_loader, val_loader, in_dim=D_NODE, device=DEVICE)\n",
    "\n",
    "    if best_overall is None or out[\"best_metric\"] > best_overall[\"best_metric\"]:\n",
    "        best_overall = out\n",
    "        best_cfg = cfg\n",
    "\n",
    "print(\"\\nBest config:\", best_cfg)\n",
    "print(\"Best validation\", best_overall[\"best_monitor\"], \"=\", best_overall[\"best_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107dc2c-c14c-46d9-a86d-dfdc6f7a80ba",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f1eea-8bf1-4d6b-8620-b3567760851d",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499402f4-f608-4b91-bdd8-a2a11fd4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: Inference + τ-Hysteresis + Confusion Matrices (Final Model) ====\n",
    "# This cell evaluates the full stack (TCN, SVD, Fusion, Fusion+Hysteresis)\n",
    "# and prints confusion matrices + basic metrics for each.\n",
    "#\n",
    "# Assumes you have:\n",
    "#   - DualHeadTCN, SVDGeometryHead, FusionHead classes defined\n",
    "#   - DEVICE, HYP, FUSION_ALPHA, TOP_K, K_PLUS_REST\n",
    "#   - a dataloader (e.g., val_loader) yielding (x_seq, x_aux, y, meta)\n",
    "#\n",
    "# Positive class is 'attack' (label == 2).\n",
    "#\n",
    "# How it works:\n",
    "#   1) Loads frozen TCN/SVD (best checkpoints if present) and FusionHead if saved.\n",
    "#   2) Streams the loader to compute p_TCN, p_SVD, p_FUSED per window.\n",
    "#   3) Applies τ-hysteresis to p_FUSED to get final ENFORCE decisions.\n",
    "#   4) Prints confusion matrices + precision/recall/F1 for:\n",
    "#        - TCN (p>=0.5)\n",
    "#        - SVD (p>=0.5)\n",
    "#        - Fusion (p>=0.5)\n",
    "#        - Fusion + Hysteresis (ENFORCE boolean)\n",
    "#\n",
    "# Optional: set INCLUDE_AE=True and provide ae_provider(x_seq, meta)->[B] to include AE in fusion.\n",
    "#\n",
    "from typing import Dict, Any, Tuple\n",
    "import os, math, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "INCLUDE_AE = False   # set True when AE is wired\n",
    "AE_PROVIDER = None   # function (x_seq, meta)-> torch.Tensor [B], values in [0,1]\n",
    "\n",
    "# --- τ-hysteresis ---\n",
    "class TauHysteresis:\n",
    "    def __init__(self, theta_on=0.7, theta_off=0.5, L_on=100, L_off=100):\n",
    "        self.theta_on=float(theta_on); self.theta_off=float(theta_off)\n",
    "        self.L_on=int(L_on); self.L_off=int(L_off)\n",
    "        self.state=False; self.streak=0\n",
    "    def reset(self):\n",
    "        self.state=False; self.streak=0\n",
    "    def step(self, p: float) -> bool:\n",
    "        if not self.state:\n",
    "            if p >= self.theta_on:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_on:\n",
    "                    self.state = True; self.streak = 0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        else:\n",
    "            if p <= self.theta_off:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_off:\n",
    "                    self.state = False; self.streak = 0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        return self.state\n",
    "\n",
    "def load_frozen_bases(tcn_ckpt=\"tcn_best.pt\", svd_ckpt=\"svd_head_best.pt\"):\n",
    "    tcn = DualHeadTCN(d_seq=(K_PLUS_REST + K_PLUS_REST + 3 + K_PLUS_REST),\n",
    "                      hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0)\n",
    "    svd = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16)\n",
    "    if os.path.exists(tcn_ckpt):\n",
    "        sd = torch.load(tcn_ckpt, map_location=\"cpu\")\n",
    "        tcn.load_state_dict(sd[\"model\"], strict=False)\n",
    "    if os.path.exists(svd_ckpt):\n",
    "        sd = torch.load(svd_ckpt, map_location=\"cpu\")\n",
    "        svd.load_state_dict(sd[\"model\"], strict=False)\n",
    "    tcn.to(DEVICE).eval(); svd.to(DEVICE).eval()\n",
    "    for p in tcn.parameters(): p.requires_grad=False\n",
    "    for p in svd.parameters(): p.requires_grad=False\n",
    "    return tcn, svd\n",
    "\n",
    "def load_or_default_fusion(in_dim: int = 2):\n",
    "    fh = FusionHead(in_dim=in_dim, hidden=8, dropout=0.0).to(DEVICE)\n",
    "    if os.path.exists(\"fusion_head_best.pt\"):\n",
    "        sd = torch.load(\"fusion_head_best.pt\", map_location=\"cpu\")\n",
    "        ck_in_dim = sd.get(\"in_dim\", in_dim)\n",
    "        if ck_in_dim != in_dim:\n",
    "            fh = FusionHead(in_dim=ck_in_dim, hidden=8, dropout=0.0).to(DEVICE)\n",
    "        fh.load_state_dict(sd[\"fusion\"], strict=False)\n",
    "    fh.eval()\n",
    "    for p in fh.parameters(): p.requires_grad=False\n",
    "    return fh\n",
    "\n",
    "@torch.no_grad()\n",
    "def base_probs(tcn, svd, x_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    logits, p_all = tcn(x_seq)\n",
    "    p_tcn = p_all[:, 2]\n",
    "    p_svd, _ = svd(x_seq[:, :, :K_PLUS_REST])\n",
    "    return p_tcn, p_svd\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_probs(tcn, svd, fusion_head, x_seq: torch.Tensor, ae_score: torch.Tensor=None) -> torch.Tensor:\n",
    "    p_tcn, p_svd = base_probs(tcn, svd, x_seq)\n",
    "    # if FusionHead checkpoint exists and has params, use it; else α-blend\n",
    "    has_params = sum(p.numel() for p in fusion_head.parameters()) > 0\n",
    "    if has_params:\n",
    "        if ae_score is not None:\n",
    "            z = torch.stack([p_tcn, p_svd, ae_score.to(p_tcn.device)], dim=-1)\n",
    "        else:\n",
    "            z = torch.stack([p_tcn, p_svd], dim=-1)\n",
    "        return fusion_head(z)\n",
    "    return FUSION_ALPHA * p_tcn + (1.0 - FUSION_ALPHA) * p_svd\n",
    "\n",
    "def confusion(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Any]:\n",
    "    # y_true, y_pred: 0/1 arrays for not-attack/attack\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    acc  = (tp + tn) / max(1, (tp+tn+fp+fn))\n",
    "    return {\n",
    "        \"matrix\": [[tn, fp],\n",
    "                   [fn, tp]],\n",
    "        \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn\n",
    "    }\n",
    "\n",
    "def evaluate_confusions(loader) -> Dict[str, Any]:\n",
    "    assert loader is not None and len(loader) > 0, \"Loader is empty.\"\n",
    "    tcn, svd = load_frozen_bases()\n",
    "    fusion = load_or_default_fusion(in_dim=3 if INCLUDE_AE else 2)\n",
    "\n",
    "    # Collect predictions/probs + labels and meta per-run for hysteresis\n",
    "    probs_tcn, probs_svd, probs_fused = [], [], []\n",
    "    labels = []\n",
    "    metas = []   # list of dicts per item: run_id, start_tti\n",
    "\n",
    "    for x_seq, x_aux, y, meta in loader:\n",
    "        x_seq = x_seq.to(DEVICE).float()\n",
    "        ae_score = None\n",
    "        if INCLUDE_AE and AE_PROVIDER is not None:\n",
    "            ae_score = AE_PROVIDER(x_seq, meta)\n",
    "\n",
    "        p_fused = fused_probs(tcn, svd, fusion, x_seq, ae_score=ae_score).detach().cpu().numpy()\n",
    "        p_tcn, p_svd = base_probs(tcn, svd, x_seq)\n",
    "        probs_tcn.extend(p_tcn.detach().cpu().numpy().tolist())\n",
    "        probs_svd.extend(p_svd.detach().cpu().numpy().tolist())\n",
    "        probs_fused.extend(p_fused.tolist())\n",
    "        labels.extend(((y == 2).numpy().astype(int)).tolist())\n",
    "\n",
    "        B = y.size(0)\n",
    "        for i in range(B):\n",
    "            metas.append({\n",
    "                \"run_id\": str(meta[\"run_id\"][i]),\n",
    "                \"start_tti\": int(meta[\"start_tti\"][i])\n",
    "            })\n",
    "\n",
    "    y_true = np.array(labels, dtype=int)\n",
    "    y_tcn  = (np.array(probs_tcn)  >= 0.5).astype(int)\n",
    "    y_svd  = (np.array(probs_svd)  >= 0.5).astype(int)\n",
    "    y_fuse = (np.array(probs_fused)>= 0.5).astype(int)\n",
    "\n",
    "    # Confusions without hysteresis\n",
    "    cm_tcn  = confusion(y_true, y_tcn)\n",
    "    cm_svd  = confusion(y_true, y_svd)\n",
    "    cm_fuse = confusion(y_true, y_fuse)\n",
    "\n",
    "    # Apply hysteresis on the fused stream per run (ordered by start_tti)\n",
    "    th_on, th_off, Lon, Loff = HYP[\"theta_on\"], HYP[\"theta_off\"], HYP[\"L_on\"], HYP[\"L_off\"]\n",
    "    # Group by run\n",
    "    from collections import defaultdict\n",
    "    by_run = defaultdict(list)\n",
    "    for i, m in enumerate(metas):\n",
    "        by_run[m[\"run_id\"]].append((m[\"start_tti\"], i))\n",
    "\n",
    "    y_fuse_hyst = np.zeros_like(y_fuse)\n",
    "    for rid, lst in by_run.items():\n",
    "        lst.sort(key=lambda x: x[0])\n",
    "        fsm = TauHysteresis(th_on, th_off, Lon, Loff)\n",
    "        for _, idx in lst:\n",
    "            state = fsm.step(float(probs_fused[idx]))\n",
    "            y_fuse_hyst[idx] = 1 if state else 0\n",
    "\n",
    "    cm_hyst = confusion(y_true, y_fuse_hyst)\n",
    "\n",
    "    # Pretty print\n",
    "    def pretty(cm: Dict[str, Any], name: str):\n",
    "        M = cm[\"matrix\"]\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Confusion Matrix (rows=true [0=benign,1=attack], cols=pred):\")\n",
    "        print(f\"[{M[0][0]:5d}  {M[0][1]:5d}]  ← true 0\")\n",
    "        print(f\"[{M[1][0]:5d}  {M[1][1]:5d}]  ← true 1\")\n",
    "        print(f\"acc={cm['acc']:.3f}  prec={cm['prec']:.3f}  rec={cm['rec']:.3f}  f1={cm['f1']:.3f} \"\n",
    "              f\"(tp={cm['tp']}, fp={cm['fp']}, tn={cm['tn']}, fn={cm['fn']})\")\n",
    "\n",
    "    pretty(cm_tcn,  \"TCN (p>=0.5)\")\n",
    "    pretty(cm_svd,  \"SVD (p>=0.5)\")\n",
    "    pretty(cm_fuse, \"Fusion (p>=0.5)\")\n",
    "    pretty(cm_hyst, \"Fusion + τ-Hysteresis (final ENFORCE)\")\n",
    "\n",
    "    return {\n",
    "        \"cm_tcn\": cm_tcn, \"cm_svd\": cm_svd, \"cm_fuse\": cm_fuse, \"cm_hyst\": cm_hyst,\n",
    "        \"probs\": {\"tcn\": probs_tcn, \"svd\": probs_svd, \"fuse\": probs_fused},\n",
    "        \"y_true\": y_true.tolist()\n",
    "    }\n",
    "\n",
    "# --- Run on validation loader by default ---\n",
    "if 'val_loader' in globals() and val_loader and len(val_loader) > 0:\n",
    "    _ = evaluate_confusions(val_loader)\n",
    "else:\n",
    "    print(\"No validation loader attached. Build metas/loaders, then re-run this cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
