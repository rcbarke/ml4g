{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c319e0-eeb6-448d-9167-59928fb81e0d",
   "metadata": {},
   "source": [
    "# TTI-Trust: TCN + SVD Intelligent QoS Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dca3f1-e8e1-4bcc-b374-5da4b181c7df",
   "metadata": {},
   "source": [
    "## Ryan Barker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae47f9-f54b-42dd-b9b6-1354cc5f48e0",
   "metadata": {},
   "source": [
    "### IS-WiN Open RAN Zero-Trust Security: PRB Starvation Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f84dda-2fe7-4959-80fa-d8a0d1fb75c6",
   "metadata": {},
   "source": [
    "### preproc_tti_trust.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1b65f-ab4d-4e4b-a8d5-44c615516c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTI-Trust preprocessing scaffold for PyTorch: identity-agnostic feature composer,\n",
    "# phase-aware windowing, and grouped K-fold splits.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, warnings, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.csv as pv, pyarrow.parquet as pq\n",
    "\n",
    "# -------------------- DATASETS --------------------\n",
    "ATTACK_FN = \"prb_tti_evidence_ai_attack.csv\"\n",
    "BENIGN_FN = \"prb_tti_evidence_ai_benign.csv\"\n",
    "\n",
    "# -------------------- Constants aligned to paper --------------------\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "\n",
    "W_LONG  = 240\n",
    "W_SHORT = 64\n",
    "\n",
    "STRIDES_LONG  = [8, 12]\n",
    "STRIDES_SHORT = [4, 6]\n",
    "\n",
    "TOP_K = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "ATTACK_DOMINANCE = 0.90\n",
    "OTHERS_MAX       = 0.10\n",
    "DUTY_KEEP        = 0.90\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def csv_to_parquet(src_csv: str, dest_root: str):\n",
    "    os.makedirs(dest_root, exist_ok=True)\n",
    "    for _root, _dirs, files in os.walk(dest_root):\n",
    "        if any(f.endswith(\".parquet\") for f in files):\n",
    "            return\n",
    "\n",
    "    convert_cols = {\"run_id\": pa.large_string(), \"phase\": pa.large_string()}\n",
    "    read_opts  = pv.ReadOptions(block_size=1<<26)\n",
    "    conv_opts  = pv.ConvertOptions(column_types=convert_cols, strings_can_be_null=True)\n",
    "    t = pv.read_csv(src_csv, read_options=read_opts, convert_options=conv_opts)\n",
    "\n",
    "    if \"run_id\" not in t.schema.names:\n",
    "        stem = os.path.splitext(os.path.basename(src_csv))[0]\n",
    "        run_id = pa.array(np.full(t.num_rows, stem), type=pa.large_string())\n",
    "        t = t.append_column(\"run_id\", run_id)\n",
    "\n",
    "    if \"sec_rel\" not in t.schema.names:\n",
    "        n = t.num_rows\n",
    "        sec_rel = pa.array((np.arange(n, dtype=np.int32) * np.float32(TTI_SEC)), type=pa.float32())\n",
    "        t = t.append_column(\"sec_rel\", sec_rel)\n",
    "\n",
    "    pq.write_to_dataset(t, root_path=dest_root, partition_cols=[\"run_id\"],\n",
    "                        compression=\"zstd\", use_dictionary=True)\n",
    "\n",
    "def _detect_ue_rb_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cand = [c for c in df.columns if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in df.columns if c.lower().endswith('_rb')]\n",
    "    def _key(c):\n",
    "        m = re.search(r'(\\d+)', c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "def _ensure_total_rb(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    if 'total_rb' not in df.columns:\n",
    "        df['total_rb'] = df[ue_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def _compute_shares_and_fairness(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    shares = (df[ue_cols].clip(lower=0).astype('float32') / np.float32(C_PRB))\n",
    "    shares.columns = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    for c in shares.columns:\n",
    "        df[c] = shares[c]\n",
    "    x = shares.to_numpy(dtype='float32', copy=False)\n",
    "    sum_x  = x.sum(axis=1, dtype='float32')\n",
    "    sum_x2 = (x*x).sum(axis=1, dtype='float32')\n",
    "    n = np.float32(max(1, len(ue_cols)))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        J = (sum_x*sum_x) / (n * sum_x2)\n",
    "    df['J'] = np.nan_to_num(J, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    return df\n",
    "\n",
    "def _compose_roles(df: pd.DataFrame, ue_cols: List[str], top_k: int = TOP_K) -> pd.DataFrame:\n",
    "    share_cols = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    alpha = 2.0 / (20 + 1.0)\n",
    "    for c in share_cols:\n",
    "        df[c+\"_ema\"] = df[c].astype('float32').ewm(alpha=alpha, adjust=False).mean().astype('float32')\n",
    "\n",
    "    S   = df[share_cols].to_numpy(dtype='float32', copy=False)\n",
    "    E   = df[[c+\"_ema\" for c in share_cols]].to_numpy(dtype='float32', copy=False)\n",
    "    PRB = df[ue_cols].to_numpy(dtype='int32',    copy=False)\n",
    "\n",
    "    T, U = S.shape\n",
    "    k = min(top_k, U)\n",
    "    order  = np.argsort(-E, axis=1)\n",
    "    top_ix = order[:, :k]\n",
    "\n",
    "    roles       = np.take_along_axis(S,   top_ix, axis=1).astype('float32')\n",
    "    roles_small = np.take_along_axis((PRB < SMALL_RB_EPS).astype('float32'), top_ix, axis=1)\n",
    "\n",
    "    sum_all    = S.sum(axis=1, dtype='float32')\n",
    "    sum_topk   = roles.sum(axis=1, dtype='float32')\n",
    "    rest_share = (sum_all - sum_topk).clip(min=0).astype('float32')\n",
    "\n",
    "    if U > k:\n",
    "        row_ix = np.arange(T)[:, None]\n",
    "        mask = np.ones_like(S, dtype=bool); mask[row_ix, top_ix] = False\n",
    "        cnt  = mask.sum(axis=1)\n",
    "        rest_small = ((PRB < SMALL_RB_EPS).astype('float32') * mask).sum(axis=1) / np.maximum(cnt, 1)\n",
    "    else:\n",
    "        rest_small = np.zeros(T, dtype='float32')\n",
    "\n",
    "    for i in range(k):\n",
    "        df[f'role{i+1}_share']   = roles[:, i]\n",
    "        df[f'role{i+1}_smallrb'] = roles_small[:, i]\n",
    "    for i in range(k, top_k):\n",
    "        df[f'role{i+1}_share']   = np.float32(0.0)\n",
    "        df[f'role{i+1}_smallrb'] = np.float32(0.0)\n",
    "    df['rest_share']   = rest_share\n",
    "    df['rest_smallrb'] = rest_small\n",
    "\n",
    "    sec_bin = np.floor(df['sec_rel'].to_numpy(dtype='float32')).astype('int32')\n",
    "    max_sec = int(sec_bin.max(initial=0))\n",
    "    def sec_presence(vec_bool: np.ndarray) -> np.ndarray:\n",
    "        agg = np.zeros(max_sec + 1, dtype='uint8')\n",
    "        np.maximum.at(agg, sec_bin, vec_bool.astype('uint8'))\n",
    "        return agg[sec_bin]\n",
    "    for i in range(top_k):\n",
    "        df[f'role{i+1}_present_1s'] = sec_presence((df[f'role{i+1}_share'].to_numpy() > 0))\n",
    "    df['rest_present_1s'] = sec_presence((df['rest_share'].to_numpy() > 0))\n",
    "    return df\n",
    "\n",
    "def _label_window(shares_mat: np.ndarray) -> bool:\n",
    "    if shares_mat.size == 0:\n",
    "        return False\n",
    "    top    = shares_mat[:, 0]\n",
    "    others = shares_mat[:, 1:]\n",
    "    ok = (top >= ATTACK_DOMINANCE) & (np.max(others, axis=1) <= OTHERS_MAX)\n",
    "    return (ok.mean() >= DUTY_KEEP)\n",
    "\n",
    "@dataclass\n",
    "class WindowSpec:\n",
    "    length_tti: int\n",
    "    stride_tti: int\n",
    "\n",
    "def iter_windows(df: pd.DataFrame, run_id: str, spec: WindowSpec, role_cols: List[str],\n",
    "                 phases_attack={\"attack\"}, phases_benign={\"baseline\",\"recovery\",\"benign\"},\n",
    "                 batch_windows: int = 8192):\n",
    "    W, S = spec.length_tti, spec.stride_tti\n",
    "    R = df[role_cols].to_numpy(dtype='float32', copy=False)\n",
    "    phase = df['phase'].astype(str).to_numpy()\n",
    "    T = len(df)\n",
    "    starts = np.arange(0, max(0, T - W + 1), S, dtype=np.int64)\n",
    "\n",
    "    bufX, rows = [], []\n",
    "    for s in starts:\n",
    "        e = s + W\n",
    "        blk = R[s:e, :]\n",
    "        if blk.shape[0] != W:\n",
    "            continue\n",
    "        win_phase = phase[s:e]\n",
    "        frac_attack = np.isin(win_phase, list(phases_attack)).mean()\n",
    "        frac_benign = np.isin(win_phase, list(phases_benign)).mean()\n",
    "        looks_attacky = _label_window(blk)\n",
    "        if (frac_attack >= 0.90) and looks_attacky: y = 2\n",
    "        elif (frac_benign >= 0.90):                y = 0\n",
    "        else:                                      y = 1\n",
    "\n",
    "        bufX.append(blk)\n",
    "        rows.append((run_id, int(s), int(e), int(W), int(y),\n",
    "                     float(frac_attack), float(frac_benign), int(looks_attacky)))\n",
    "\n",
    "        if len(bufX) == batch_windows:\n",
    "            X = np.stack(bufX, axis=0)\n",
    "            meta = pd.DataFrame.from_records(\n",
    "                rows,\n",
    "                columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                         \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "            )\n",
    "            yield X, meta\n",
    "            bufX.clear(); rows.clear()\n",
    "\n",
    "    if bufX:\n",
    "        X = np.stack(bufX, axis=0)\n",
    "        meta = pd.DataFrame.from_records(\n",
    "            rows,\n",
    "            columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                     \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "        )\n",
    "        yield X, meta\n",
    "\n",
    "# -------------------- Main entry: build datasets (partitioned, low RAM) --------------------\n",
    "\n",
    "csv_to_parquet(ATTACK_FN, \"parquet/attack\")\n",
    "csv_to_parquet(BENIGN_FN, \"parquet/benign\")\n",
    "\n",
    "def iter_run_partitions(root: str):\n",
    "    for run_dir in sorted(glob(os.path.join(root, \"run_id=*\"))):\n",
    "        rid = os.path.basename(run_dir).split(\"=\", 1)[1]\n",
    "        df = pd.read_parquet(run_dir, engine=\"pyarrow\")\n",
    "        if 'run_id' not in df.columns:\n",
    "            df['run_id'] = rid\n",
    "        if 'phase' not in df.columns:\n",
    "            df['phase'] = 'unknown'\n",
    "        if 'sec_rel' not in df.columns:\n",
    "            df['sec_rel'] = (np.arange(len(df), dtype=np.int32) * np.float32(TTI_SEC)).astype('float32')\n",
    "        yield rid, df\n",
    "\n",
    "def process_source(root: str, src_name: str):\n",
    "    if not os.path.isdir(root):\n",
    "        print(f\"[{src_name}] No parquet root found at {root} (skipping).\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"win_shards\", exist_ok=True)\n",
    "    shard_counter = itertools.count(0)\n",
    "\n",
    "    runs = list(iter_run_partitions(root))\n",
    "    if not runs:\n",
    "        print(f\"[{src_name}] No run_id partitions found in {root}.\")\n",
    "        return\n",
    "    print(f\"[{src_name}] Found {len(runs)} run_id partitions.\")\n",
    "\n",
    "    for rid, df in runs:\n",
    "        ue_cols = _detect_ue_rb_columns(df)\n",
    "        if not ue_cols:\n",
    "            warnings.warn(f\"[{src_name}] run_id={rid}: no UE *_rb columns; skipping\")\n",
    "            continue\n",
    "\n",
    "        df = _ensure_total_rb(df, ue_cols)\n",
    "        df = _compute_shares_and_fairness(df, ue_cols)\n",
    "        df = _compose_roles(df, ue_cols, top_k=TOP_K)\n",
    "\n",
    "        keep = [f'role{i}_share' for i in range(1, TOP_K+1)] + ['rest_share','J','sec_rel','run_id','phase']\n",
    "        enriched_out = f\"{src_name}_enriched_{rid}.parquet\"\n",
    "        df[keep].to_parquet(enriched_out, index=False)\n",
    "        print(f\"[{src_name}] run_id={rid} → {enriched_out}  (rows={len(df):,})\")\n",
    "\n",
    "        role_cols = [f'role{k}_share' for k in range(1, TOP_K+1)] + ['rest_share']\n",
    "        g = df.reset_index(drop=True)\n",
    "\n",
    "        for stride in STRIDES_LONG:\n",
    "            spec = WindowSpec(length_tti=W_LONG, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"long\"\n",
    "                meta.to_parquet(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "        for stride in STRIDES_SHORT:\n",
    "            spec = WindowSpec(length_tti=W_SHORT, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"short\"\n",
    "                meta.to_parquet(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "process_source(\"parquet/attack\", \"attack\")\n",
    "process_source(\"parquet/benign\", \"benign\")\n",
    "\n",
    "def load_all_meta(kind: str) -> pd.DataFrame:\n",
    "    metas = [pd.read_parquet(p) for p in glob(f\"win_shards/{kind}_*_meta.parquet\")]\n",
    "    return pd.concat(metas, ignore_index=True) if metas else pd.DataFrame()\n",
    "\n",
    "M_long  = load_all_meta(\"long\")\n",
    "M_short = load_all_meta(\"short\")\n",
    "\n",
    "def grouped_kfold(meta: pd.DataFrame, K: int = 5, seed: int = 42) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    if meta.empty:\n",
    "        return []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    run_ids = meta['run_id'].astype(str).unique().tolist()\n",
    "    rng.shuffle(run_ids)\n",
    "    folds = [set() for _ in range(K)]\n",
    "    for i, rid in enumerate(run_ids):\n",
    "        folds[i % K].add(rid)\n",
    "    splits = []\n",
    "    for i in range(K):\n",
    "        val_rids = folds[i]\n",
    "        train_idx = meta.index[~meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        val_idx   = meta.index[ meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        splits.append((train_idx, val_idx))\n",
    "    return splits\n",
    "\n",
    "splits_long  = grouped_kfold(M_long,  K=5, seed=2025)\n",
    "splits_short = grouped_kfold(M_short, K=5, seed=2025)\n",
    "\n",
    "if len(M_long):\n",
    "    print(f\"\\n[long] meta={M_long.shape}, groups={M_long['run_id'].nunique()}, folds={len(splits_long)}\")\n",
    "if len(M_short):\n",
    "    print(f\"[short] meta={M_short.shape}, groups={M_short['run_id'].nunique()}, folds={len(splits_short)}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb5f5-a951-47f5-99df-6b9d5a8601e9",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496f167-721e-4197-9c1e-961bf4260af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch feature space & data loader for TTI-Trust (scheduler-native, identity-agnostic)\n",
    "# Optimized to stream large datasets from shards: win_shards/{long,short}_s*_... .npz + *_meta.parquet\n",
    "#\n",
    "# Emits batches directly (IterableDataset) to avoid in-memory concatenation or per-item pandas operations.\n",
    "\n",
    "import os, re, glob, math\n",
    "from typing import Dict, List, Tuple, Optional, Iterator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "# --- Shared constants (align with preprocessing) ---\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "TOP_K   = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "PLATEAUS = np.array([16, 31, 46, 61, 76, 91, 106], dtype=np.int32)\n",
    "\n",
    "# ---------- Feature helpers (NumPy; no pandas in the hot path) ----------\n",
    "\n",
    "def jains_fairness_seq(shares: np.ndarray) -> np.ndarray:\n",
    "    # shares: [N, W, D] or [W, D]\n",
    "    if shares.ndim == 2:\n",
    "        shares = shares[None, ...]\n",
    "    sum_x  = shares.sum(axis=2, dtype=np.float32)                     # [N, W]\n",
    "    sum_x2 = (shares * shares).sum(axis=2, dtype=np.float32)          # [N, W]\n",
    "    n = shares.shape[2]\n",
    "    J = (sum_x * sum_x) / (np.float32(n) * sum_x2 + 1e-9)\n",
    "    return J.squeeze(0).astype(np.float32) if J.shape[0] == 1 else J.astype(np.float32)\n",
    "\n",
    "def rolling_min_med_causal(x: np.ndarray, win: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # x: [W] (per-window). W<=240 → simple loop is fine and cache-friendly.\n",
    "    W = x.shape[0]\n",
    "    rmin = np.empty(W, dtype=np.float32)\n",
    "    rmed = np.empty(W, dtype=np.float32)\n",
    "    for t in range(W):\n",
    "        a = max(0, t - win + 1)\n",
    "        sl = x[a:t+1]\n",
    "        rmin[t] = sl.min() if sl.size else 0.0\n",
    "        rmed[t] = np.median(sl) if sl.size else 0.0\n",
    "    return rmin, rmed\n",
    "\n",
    "def small_rb_runlength_1d(streak_bin: np.ndarray) -> np.ndarray:\n",
    "    out = np.zeros_like(streak_bin, dtype=np.float32)\n",
    "    c = 0.0\n",
    "    for i, b in enumerate(streak_bin.astype(bool)):\n",
    "        c = c + 1.0 if b else 0.0\n",
    "        out[i] = c\n",
    "    return out\n",
    "\n",
    "def build_c_runs(srb_bin: np.ndarray) -> np.ndarray:\n",
    "    # srb_bin: [W, D] 0/1 → run-length per channel\n",
    "    return np.stack([small_rb_runlength_1d(srb_bin[:, j]) for j in range(srb_bin.shape[1])], axis=1).astype(np.float32)\n",
    "\n",
    "def approx_plateau_hist(share_seq: np.ndarray) -> np.ndarray:\n",
    "    # share_seq: [W, D] → PRBs → closest plateau histogram normalized\n",
    "    prbs = np.rint(share_seq * C_PRB).astype(np.int32)\n",
    "    idx  = np.abs(prbs[:, :, None] - PLATEAUS[None, None, :]).argmin(axis=2)  # [W, D]\n",
    "    hist = np.bincount(idx.ravel(), minlength=len(PLATEAUS)).astype(np.float32)\n",
    "    if hist.sum() > 0: hist /= hist.sum()\n",
    "    return hist\n",
    "\n",
    "def benign_grant_fraction(share_seq: np.ndarray) -> float:\n",
    "    others = share_seq[:, 1:]\n",
    "    return float((others > 0).any(axis=1).mean())\n",
    "\n",
    "def contiguous_zero_runs(share_seq: np.ndarray) -> Tuple[float, float]:\n",
    "    others_all_zero = (share_seq[:, 1:] == 0).all(axis=1).astype(np.int32)  # [W]\n",
    "    runs, c = [], 0\n",
    "    for b in others_all_zero:\n",
    "        if b: c += 1\n",
    "        else:\n",
    "            if c: runs.append(c); c = 0\n",
    "    if c: runs.append(c)\n",
    "    return (0.0, 0.0) if not runs else (float(max(runs)), float(np.mean(runs)))\n",
    "\n",
    "# ---------- IterableDataset over shard files ----------\n",
    "\n",
    "class TTINPZShardBatches(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streams batches from NPZ shards produced by preprocessing.\n",
    "    Each shard file has X: [N, W, D] where D = TOP_K+1 (roles incl. rest), and a paired *_meta.parquet.\n",
    "\n",
    "    Yields (X_seq, X_aux, y, meta_list) where:\n",
    "      X_seq: [B, W, Dseq]  (roles, d_roles, J, J_min, J_med, smallRB runs)\n",
    "      X_aux: [B, Daux]     (plateau hist, benign frac, zero-run stats, radio hints if available)\n",
    "      y:     [B] {0,1,2}\n",
    "      meta_list: list of dicts with run_id/source/stride/window_len/start_tti/end_tti\n",
    "    \"\"\"\n",
    "    def __init__(self, kind: str, batch_size: int = 1024, shard_glob: Optional[str] = None,\n",
    "                 restrict_sources: Optional[List[str]] = None):\n",
    "        assert kind in (\"long\", \"short\")\n",
    "        self.kind = kind\n",
    "        self.batch_size = int(batch_size)\n",
    "        # Discover shards\n",
    "        patt = shard_glob or f\"win_shards/{kind}_*.npz\"\n",
    "        self.shard_paths = sorted(glob.glob(patt))\n",
    "        if not self.shard_paths:\n",
    "            raise FileNotFoundError(f\"No shards found for pattern: {patt}. Run preprocessing to generate win_shards.\")\n",
    "        self.restrict_sources = set(restrict_sources) if restrict_sources else None\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        for npz_path in self.shard_paths:\n",
    "            meta_path = npz_path.replace(\".npz\", \"_meta.parquet\")\n",
    "            if not os.path.exists(meta_path):\n",
    "                continue\n",
    "            meta = pd.read_parquet(meta_path)\n",
    "            if self.restrict_sources is not None:\n",
    "                if \"source\" in meta.columns:\n",
    "                    if not any(src in self.restrict_sources for src in meta[\"source\"].unique().tolist()):\n",
    "                        continue\n",
    "            X = np.load(npz_path)[\"X\"]  # [N, W, D], float32 from preprocessing\n",
    "            # Labels\n",
    "            if \"label\" not in meta.columns:\n",
    "                raise ValueError(f\"Meta {meta_path} missing 'label' column.\")\n",
    "            y_all = meta[\"label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "            # Optional radio hints (rare). If present as per-TTI per-role, we’d read a second array.\n",
    "            have_radio = False  # keep simple; emit zeros + flag\n",
    "\n",
    "            N, W, D = X.shape\n",
    "            assert D == (TOP_K + 1), f\"Expected D=TOP_K+1, got D={D}\"\n",
    "\n",
    "            # Batch over this shard\n",
    "            for i0 in range(0, N, self.batch_size):\n",
    "                i1 = min(N, i0 + self.batch_size)\n",
    "                seq = X[i0:i1]                                  # [B, W, D]\n",
    "                # Δshares\n",
    "                dseq = np.diff(seq, axis=1, prepend=seq[:, 0:1, :])  # [B, W, D]\n",
    "\n",
    "                # Jain's J(t) + causal rolling stats (per sample)\n",
    "                J_list, Jmin_list, Jmed_list = [], [], []\n",
    "                for b in range(seq.shape[0]):\n",
    "                    Jb = jains_fairness_seq(seq[b])                   # [W]\n",
    "                    Jmin, Jmed = rolling_min_med_causal(Jb, win=min(60, Jb.shape[0]))\n",
    "                    J_list.append(Jb); Jmin_list.append(Jmin); Jmed_list.append(Jmed)\n",
    "                J   = np.stack(J_list,    axis=0)[:, :, None]         # [B, W, 1]\n",
    "                Jmn = np.stack(Jmin_list, axis=0)[:, :, None]         # [B, W, 1]\n",
    "                Jmd = np.stack(Jmed_list, axis=0)[:, :, None]         # [B, W, 1]\n",
    "\n",
    "                # small-RB run-lengths (approx from shares threshold)\n",
    "                prb = np.rint(seq * C_PRB)                            # [B, W, D]\n",
    "                srb = (prb < SMALL_RB_EPS).astype(np.float32)\n",
    "                c_runs = np.stack([build_c_runs(srb[b]) for b in range(srb.shape[0])], axis=0)  # [B, W, D]\n",
    "\n",
    "                # Time-step channels\n",
    "                X_seq = np.concatenate([seq, dseq, J, Jmn, Jmd, c_runs], axis=2).astype(np.float32)  # [B, W, Dseq]\n",
    "\n",
    "                # Aux features per window\n",
    "                aux_list = []\n",
    "                for b in range(seq.shape[0]):\n",
    "                    ph = approx_plateau_hist(seq[b])\n",
    "                    bf = benign_grant_fraction(seq[b])\n",
    "                    zmax, zmean = contiguous_zero_runs(seq[b])\n",
    "                    if have_radio:\n",
    "                        radio_aux = np.zeros(33, dtype=np.float32)\n",
    "                        radio_flag = 1.0\n",
    "                    else:\n",
    "                        radio_aux = np.zeros(33, dtype=np.float32)\n",
    "                        radio_flag = 0.0\n",
    "                    aux = np.concatenate([ph, np.array([bf, zmax, zmean], dtype=np.float32),\n",
    "                                          radio_aux, np.array([radio_flag], dtype=np.float32)])\n",
    "                    aux_list.append(aux)\n",
    "                X_aux = np.stack(aux_list, axis=0).astype(np.float32)  # [B, 44]\n",
    "\n",
    "                y = torch.from_numpy(y_all[i0:i1])                     # [B], int64\n",
    "\n",
    "                # Build meta dicts list (keep only essentials)\n",
    "                meta_batch = []\n",
    "                cols = meta.columns\n",
    "                for j in range(i0, i1):\n",
    "                    row = meta.iloc[j]\n",
    "                    meta_batch.append({\n",
    "                        \"run_id\": str(row.get(\"run_id\", \"\")),\n",
    "                        \"source\": str(row.get(\"source\", \"\")),\n",
    "                        \"stride\": int(row.get(\"stride\", 0)),\n",
    "                        \"window_len\": int(row.get(\"window_len\", seq.shape[1])),\n",
    "                        \"start_tti\": int(row.get(\"start_tti\", 0)),\n",
    "                        \"end_tti\": int(row.get(\"end_tti\", 0)),\n",
    "                    })\n",
    "\n",
    "                # Zero-copy to torch for X_seq/X_aux\n",
    "                yield torch.from_numpy(X_seq), torch.from_numpy(X_aux), y, meta_batch\n",
    "\n",
    "# ---------- High-throughput DataLoader factory ----------\n",
    "\n",
    "def make_loader(kind: str, batch_size: int = 1024, num_workers: int = 4,\n",
    "                restrict_sources: Optional[List[str]] = None) -> DataLoader:\n",
    "    ds = TTINPZShardBatches(kind=kind, batch_size=batch_size, restrict_sources=restrict_sources)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=None,               # dataset already yields batches\n",
    "        num_workers=num_workers,       # tune: 2–8 based on CPU/IO\n",
    "        pin_memory=True,               # faster H2D copies\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# ---------- Smoke test (requires win_shards present) ----------\n",
    "\n",
    "try:\n",
    "    train_loader = make_loader(kind=\"short\", batch_size=1024, num_workers=4)\n",
    "    val_loader   = make_loader(kind=\"short\", batch_size=1024, num_workers=4)\n",
    "    print(\"Streaming loader ready. First batch shape:\")\n",
    "    xb_seq, xb_aux, yb, mb = next(iter(train_loader))\n",
    "    print(\"X_seq:\", tuple(xb_seq.shape), \"X_aux:\", tuple(xb_aux.shape), \"y:\", tuple(yb.shape), \"meta[0]:\", mb[0])\n",
    "except Exception as e:\n",
    "    print(\"Note:\", str(e))\n",
    "\n",
    "print(\"\\nUse: for xb_seq, xb_aux, yb, mb in make_loader('short', ...): model(xb_seq, xb_aux) ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a4651-9ce3-4d5c-9178-84f6701ec6cb",
   "metadata": {},
   "source": [
    "### Base Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1bacf-e010-4743-a55b-04e003af9c1f",
   "metadata": {},
   "source": [
    "#### Dual-head TCN Fairness Seismograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70b31c-f0ce-489f-92a2-f258c2632f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Dual-head TCN (τ-scale + edge-scale) ====\n",
    "# Input:  x_seq ∈ R^{B × W × D_seq}\n",
    "# Output: logits ∈ R^{B×3}  (classes: 0=benign, 1=proximal, 2=attack),\n",
    "#         probs  ∈ R^{B×3}  (temperature-scaled softmax for calibration)\n",
    "\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalConv1dSame(nn.Conv1d):\n",
    "    \"\"\"Causal Conv1d that preserves time length (left-padding only).\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size,\n",
    "                         stride=1, padding=0, dilation=dilation, bias=bias)\n",
    "        self.left_pad = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, T]\n",
    "        x = F.pad(x, (self.left_pad, 0))\n",
    "        return super().forward(x)\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    \"\"\"Residual dilated-causal block: Conv → SiLU → Dropout → Conv → SiLU → Residual → LayerNorm.\"\"\"\n",
    "    def __init__(self, channels: int, kernel: int, dilation: int, dropout: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1dSame(channels, channels, kernel, dilation=dilation)\n",
    "        self.conv2 = CausalConv1dSame(channels, channels, kernel, dilation=dilation)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):            # x: [B, C, T]\n",
    "        y = self.conv1(x); y = F.silu(y); y = self.dropout(y)\n",
    "        y = self.conv2(y); y = F.silu(y)\n",
    "        y = y + x                    # residual\n",
    "        y = y.transpose(1, 2)        # [B, T, C] for LayerNorm over channels\n",
    "        y = self.ln(y)\n",
    "        y = y.transpose(1, 2)        # back to [B, C, T]\n",
    "        return y\n",
    "\n",
    "class TCNHead(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden: int, dilations: List[int], kernel: int = 3, dropout: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Conv1d(in_channels, hidden, kernel_size=1)\n",
    "        self.blocks = nn.ModuleList([TCNBlock(hidden, kernel, d, dropout) for d in dilations])\n",
    "        self.out_norm = nn.LayerNorm(hidden)\n",
    "\n",
    "    def forward(self, x):            # x: [B, W, D]\n",
    "        x = x.transpose(1, 2)        # → [B, D, W]\n",
    "        x = self.in_proj(x)          # → [B, H, W]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)               # → [B, H, W]\n",
    "        x = x.mean(dim=-1)           # global average over time → [B, H]\n",
    "        x = self.out_norm(x)\n",
    "        return x\n",
    "\n",
    "class DualHeadTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Two TCN heads with different receptive fields:\n",
    "      - τ-scale head: 7 blocks, dilations [1,2,4,8,16,32,64]  → ~127.5 ms at 0.5 ms/TTI\n",
    "      - Edge-scale head: 4 blocks, dilations [1,2,4,8]        → ~15.5 ms\n",
    "    \"\"\"\n",
    "    def __init__(self, d_seq: int, hidden: int = 32, dropout: float = 0.05, temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.head_tau  = TCNHead(in_channels=d_seq, hidden=hidden, dilations=[1,2,4,8,16,32,64],\n",
    "                                 kernel=3, dropout=dropout)\n",
    "        self.head_edge = TCNHead(in_channels=d_seq, hidden=hidden, dilations=[1,2,4,8],\n",
    "                                 kernel=3, dropout=dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2*hidden, 2*hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*hidden, 3)   # 0=benign, 1=proximal, 2=attack\n",
    "        )\n",
    "        # temperature for post-hoc calibration (validation-time tuning)\n",
    "        self.log_temp = nn.Parameter(torch.log(torch.tensor(temperature, dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, x_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x_seq: [B, W, D_seq] — feed the feature set from TtiTrustDataset (shares, Δshares, J stats, small-RB runs).\n",
    "        returns (logits, probs)\n",
    "        \"\"\"\n",
    "        h_tau  = self.head_tau(x_seq)           # [B, H]\n",
    "        h_edge = self.head_edge(x_seq)          # [B, H]\n",
    "        h = torch.cat([h_tau, h_edge], dim=-1)  # [B, 2H]\n",
    "        logits = self.classifier(h)             # [B, 3]\n",
    "        T = torch.exp(self.log_temp).clamp(min=1e-3)\n",
    "        probs = F.softmax(logits / T, dim=-1)\n",
    "        return logits, probs\n",
    "\n",
    "# Example (synthetic) — set D_seq to your dataset’s channel count (e.g., 24)\n",
    "# B, W, D = 8, 240, 24\n",
    "# x = torch.randn(B, W, D)\n",
    "# model = DualHeadTCN(d_seq=D, hidden=32, dropout=0.05)\n",
    "# logits, probs = model(x)\n",
    "# logits.shape, probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cc57c-ffdf-4adf-8926-01e15c9157f1",
   "metadata": {},
   "source": [
    "#### SVD PF On-Edge Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eab9a-a9cf-4eb0-a682-1b9239a15e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL 2: SVD head + τ-aligned hysteresis FSM ====\n",
    "# SVD Head:\n",
    "#   Input: shares_seq ∈ R^{B × W × (K+1)}   (top-K roles + rest)  ⟵ slice the first (K+1) channels from x_seq\n",
    "#   Features: [sigma1/sum_sigma, spectral_entropy, sigma1/sigma2, angle_v1_e1]\n",
    "#   Output:   p_attack_svd ∈ (0,1)\n",
    "#\n",
    "# Hysteresis:\n",
    "#   Two-threshold FSM over attack probability time series with τ-aligned dwell times.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SVDGeometryHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-rank spectral feature extractor over windowed role shares.\n",
    "    Produces a single logit for 'attack' (can be fused with TCN in your ensemble).\n",
    "    \"\"\"\n",
    "    def __init__(self, k_roles_plus_rest: int, hidden: int = 16):\n",
    "        super().__init__()\n",
    "        # 4 scalar features → small MLP to 1 logit\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        # store D = K+1 for angle computation\n",
    "        self.D = k_roles_plus_rest\n",
    "        # unit vector e1 along the \"role1\" axis\n",
    "        self.register_buffer(\"e1\", torch.zeros(self.D))\n",
    "        self.e1[0] = 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _spectral_entropy(sigmas: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
    "        # sigmas: [B, r] singular values ≥ 0\n",
    "        p = sigmas / (sigmas.sum(dim=-1, keepdim=True) + eps)         # normalize\n",
    "        H = -(p * (p.add(eps).log())).sum(dim=-1)                     # entropy\n",
    "        # optional: normalize by log(r) to [0,1]\n",
    "        r = sigmas.shape[-1]\n",
    "        return H / math.log(max(r, 2))\n",
    "\n",
    "    def forward(self, shares_seq: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        shares_seq: [B, W, D] with D = K+1 (top-K roles + rest)\n",
    "        returns:    p_svd: [B]  (sigmoid probability of attack)\n",
    "        \"\"\"\n",
    "        B, W, D = shares_seq.shape\n",
    "        assert D == self.D, f\"Expected D={self.D}, got {D}\"\n",
    "\n",
    "        # Compute economy SVD per batch item on CPU or GPU\n",
    "        # Use torch.linalg.svd for U,S,Vh; we only need S and v1 (leading right singular vector)\n",
    "        # Flatten batch: compute per-sample due to variable numerical stability\n",
    "        feats = []\n",
    "        for b in range(B):\n",
    "            X = shares_seq[b]                      # [W, D]\n",
    "            # svdvals for σ; full svd for v1\n",
    "            # For stability, center columns (optional): Xc = X - X.mean(dim=0, keepdim=True)\n",
    "            Xc = X - X.mean(dim=0, keepdim=True)\n",
    "            try:\n",
    "                U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
    "            except RuntimeError:\n",
    "                # fallback: small jitter\n",
    "                U, S, Vh = torch.linalg.svd(Xc + 1e-6*torch.randn_like(Xc), full_matrices=False)\n",
    "\n",
    "            # σ features\n",
    "            sum_sigma = S.sum() + 1e-9\n",
    "            sigma1 = S[0]\n",
    "            sigma2 = S[1] if S.numel() > 1 else torch.tensor(1e-6, device=S.device, dtype=S.dtype)\n",
    "            dom_ratio = sigma1 / sum_sigma\n",
    "            stability = sigma1 / (sigma2 + 1e-9)\n",
    "            H = self._spectral_entropy(S.unsqueeze(0)).squeeze(0)     # normalized spectral entropy\n",
    "\n",
    "            # angle between v1 (leading right singular vector) and e1 = [1,0,0,...]\n",
    "            v1 = Vh[0]                                               # v1^T (row); shape [D]\n",
    "            # cosine similarity with e1\n",
    "            cos = torch.dot(v1, self.e1) / (v1.norm() * self.e1.norm() + 1e-9)\n",
    "            cos = torch.clamp(cos, -1.0, 1.0)\n",
    "            angle = torch.arccos(torch.abs(cos)) / math.pi           # normalize to [0,1]\n",
    "\n",
    "            feats.append(torch.stack([dom_ratio, H, stability, angle], dim=0))\n",
    "\n",
    "        F_svd = torch.stack(feats, dim=0)                             # [B, 4]\n",
    "        logit = self.mlp(F_svd).squeeze(-1)                           # [B]\n",
    "        p = torch.sigmoid(logit)\n",
    "        return p, F_svd                                               # also return features for logging\n",
    "\n",
    "# Example usage:\n",
    "#   tcn = DualHeadTCN(d_seq=24)\n",
    "#   svd = SVDGeometryHead(k_roles_plus_rest=7)\n",
    "#   logits, p_tcn = tcn(x_seq)                      # p_tcn: [B,3]\n",
    "#   p_svd, F_svd = svd(x_seq[:, :, :7])            # first 7 channels are shares(K+1)\n",
    "#   p_attack = 0.7*p_tcn[:,2] + 0.3*p_svd          # simple fusion (calibrate offline)\n",
    "#   fsm = TauHysteresis(theta_on=0.7, theta_off=0.5, L_on=100, L_off=100)\n",
    "#   enforce_flags = [fsm.step(float(p)) for p in p_attack.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab57bc7-9215-4381-990f-435dcaf040ce",
   "metadata": {},
   "source": [
    "### Base Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb94ce6-58c4-47c8-a5eb-047e270fb450",
   "metadata": {},
   "source": [
    "#### Fusion Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9aee4-b016-42b2-a605-3b675c047ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: FusionHead (learned ensemble) ====\n",
    "# Input choices for z:\n",
    "#   Minimal now:           [p_tcn_attack, p_svd]                     → in_dim = 2\n",
    "#   When AE arrives:       [p_tcn_attack, p_svd, ae_score]           → in_dim = 3\n",
    "#   With SVD features:     [+ svd_feats(4)]                          → in_dim = 6 or 7\n",
    "#\n",
    "# Output: p_attack_fused ∈ (0,1)  (binary \"attack present\" prob per window)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        # Start tiny; you can bump hidden later if needed.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: [B, in_dim]\n",
    "        logit = self.net(z).squeeze(-1)      # [B]\n",
    "        return torch.sigmoid(logit)          # p_attack_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f3d97-9ee8-48cc-8ab8-11d2d1b887ee",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6977c6-e5cc-4ed9-b8ed-935262806065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: Model + Hyperparameter Initialization (with early-stop params) ====\n",
    "import os, math, json, random\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ---- Reproducibility & device ----\n",
    "SEED = 2025\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_all(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- Shared constants (sync with preprocessing/loader) ----\n",
    "C_PRB = 106\n",
    "TOP_K = 6\n",
    "K_PLUS_REST = TOP_K + 1\n",
    "# Feature depth for x_seq from Dataset:\n",
    "#   shares(K+1) + dshares(K+1) + {J, J_min, J_med} + smallRB_run(K+1)\n",
    "D_SEQ = (K_PLUS_REST) + (K_PLUS_REST) + 3 + (K_PLUS_REST)  # 24 for TOP_K=6\n",
    "\n",
    "# ---- Hyperparameters (now includes early-stop + tiny LR grid placeholders) ----\n",
    "HYP = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 3e-4,\n",
    "    \"lr_grid\": [2e-4, 3e-4, 5e-4],     # optional small grid; pick one to run\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.05,\n",
    "    \"hidden\": 32,\n",
    "    \"alpha_fusion\": 0.7,               # p_attack = α*p_TCN + (1-α)*p_SVD\n",
    "    \"label_smoothing\": 0.05,\n",
    "    # τ-aligned hysteresis (≈50 ms @ 0.5 ms/TTI ⇒ 100 TTIs)\n",
    "    \"theta_on\": 0.7,\n",
    "    \"theta_off\": 0.5,\n",
    "    \"L_on\": 100,\n",
    "    \"L_off\": 100,\n",
    "    # Early-stop config (used by TCN/SVD/Fusion loops)\n",
    "    \"early_stop\": {\n",
    "        \"monitor\": \"f1\",               # metric to monitor on validation\n",
    "        \"mode\": \"max\",                 # 'max' for f1, 'min' for loss\n",
    "        \"patience\": 5,                 # epochs with no improvement\n",
    "        \"min_delta\": 1e-3              # minimum change to qualify as improvement\n",
    "    },\n",
    "    # scheduler placeholders (will refresh once loaders exist)\n",
    "    \"warmup_steps\": 500,\n",
    "    \"total_steps\": 20000,\n",
    "}\n",
    "# Non-destructive defaults for grid search\n",
    "HYP.setdefault(\"lr_grid\", [2e-4, 3e-4, 5e-4])\n",
    "HYP.setdefault(\"warmup_grid\", [200, 500])\n",
    "HYP.setdefault(\"label_smoothing_grid\", [0.0, 0.05, 0.1])\n",
    "HYP.setdefault(\"class_weight_cap\", 3.0)        # optional; cap max class weight at 3× mean\n",
    "HYP.setdefault(\"fusion_use_svd_feats\", False)  # start with [p_TCN,p_SVD]; toggle True to add 4 SVD feats\n",
    "print(\"Hyperparameters:\", json.dumps(HYP, indent=2))\n",
    "\n",
    "# ---- Class weights (optional) ----\n",
    "def compute_class_weights(meta_path: str) -> Optional[torch.Tensor]:\n",
    "    if not os.path.exists(meta_path):\n",
    "        return None\n",
    "    import pandas as pd\n",
    "    df = pd.read_parquet(meta_path)\n",
    "    if \"label\" not in df.columns:\n",
    "        return None\n",
    "    counts = df[\"label\"].value_counts().reindex([0,1,2]).fillna(0).to_numpy(dtype=np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    inv = 1.0 / counts\n",
    "    w = inv / inv.sum() * 3.0\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "cw = compute_class_weights(\"windows_long_meta.parquet\")\n",
    "if cw is None:\n",
    "    cw = compute_class_weights(\"windows_short_meta.parquet\")\n",
    "if cw is None:\n",
    "    cw = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32)\n",
    "class_weights = cw\n",
    "print(\"Class weights:\", class_weights.tolist())\n",
    "\n",
    "# Cap class weights to avoid instability under extreme skew\n",
    "if HYP[\"class_weight_cap\"] is not None:\n",
    "    mean_w = class_weights.float().mean()\n",
    "    class_weights = torch.clamp(class_weights.float(), max=HYP[\"class_weight_cap\"] * mean_w)\n",
    "    print(\"Capped class weights:\", class_weights.tolist())\n",
    "\n",
    "# ---- Instantiate models (cells defining these must have been run) ----\n",
    "try:\n",
    "    tcn_model = DualHeadTCN(d_seq=D_SEQ, hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0).to(DEVICE)\n",
    "    svd_head  = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16).to(DEVICE)\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"Run the TCN + SVD definition cells before this one.\") from e\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print(f\"TCN params: {count_params(tcn_model):,}\")\n",
    "print(f\"SVD head params: {count_params(svd_head):,}\")\n",
    "\n",
    "# ---- Optimizer & Scheduler ----\n",
    "optim = torch.optim.AdamW([\n",
    "    {\"params\": tcn_model.parameters(), \"lr\": HYP[\"lr\"]},\n",
    "    {\"params\": svd_head.parameters(),  \"lr\": HYP[\"lr\"]},\n",
    "], weight_decay=HYP[\"weight_decay\"])\n",
    "\n",
    "def make_scheduler(optimizer, warmup_steps: int, total_steps: int):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = make_scheduler(optim, HYP[\"warmup_steps\"], HYP[\"total_steps\"])\n",
    "\n",
    "# ---- Loss ----\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE), label_smoothing=HYP[\"label_smoothing\"])\n",
    "\n",
    "# ---- Optional loaders for fold 0 (only if metas exist) ----\n",
    "train_loader = val_loader = None\n",
    "if os.path.exists(\"windows_long_meta.parquet\") or os.path.exists(\"windows_short_meta.parquet\"):\n",
    "    kind = \"long\" if os.path.exists(\"windows_long_meta.parquet\") else \"short\"\n",
    "    import pandas as pd\n",
    "    meta = pd.read_parquet(f\"windows_{kind}_meta.parquet\")\n",
    "\n",
    "    # 5-fold grouped split by run_id\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    run_ids = meta['run_id'].astype(str).unique().tolist()\n",
    "    rng.shuffle(run_ids)\n",
    "    folds = [set() for _ in range(5)]\n",
    "    for i, rid in enumerate(run_ids):\n",
    "        folds[i % 5].add(rid)\n",
    "    fold = 0\n",
    "    val_rids = folds[fold]\n",
    "    train_idx = meta.index[~meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "    val_idx   = meta.index[ meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "\n",
    "    ds_full  = TtiTrustDataset(kind=kind)\n",
    "    ds_train = Subset(ds_full, train_idx)\n",
    "    ds_val   = Subset(ds_full,  val_idx)\n",
    "\n",
    "    train_loader = DataLoader(ds_train,\n",
    "        batch_size=HYP[\"batch_size\"], shuffle=True,\n",
    "        num_workers=8, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "    val_loader = DataLoader(ds_val,\n",
    "       batch_size=HYP[\"batch_size\"], shuffle=False,\n",
    "       num_workers=8, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n",
    "    \n",
    "    # Update scheduler steps with real batch counts\n",
    "    HYP[\"total_steps\"] = max(HYP[\"epochs\"] * max(1, len(train_loader)), HYP[\"warmup_steps\"] + 1000)\n",
    "    scheduler = make_scheduler(optim, HYP[\"warmup_steps\"], HYP[\"total_steps\"])\n",
    "    print(f\"\\nData: kind={kind}  train_batches={len(train_loader)}  val_batches={len(val_loader)}\")\n",
    "else:\n",
    "    print(\"\\nNo window metas yet. Models/optim/scheduler are initialized—create metas and re-run to attach loaders.\")\n",
    "\n",
    "# ---- Inference-time fusion + τ-hysteresis config ----\n",
    "FUSION_ALPHA = HYP[\"alpha_fusion\"]  # p_attack = α*p_TCN + (1-α)*p_SVD\n",
    "\n",
    "class TauHysteresis:\n",
    "    def __init__(self, theta_on=0.7, theta_off=0.5, L_on=100, L_off=100):\n",
    "        self.theta_on = float(theta_on); self.theta_off = float(theta_off)\n",
    "        self.L_on = int(L_on); self.L_off = int(L_off)\n",
    "        self.state = False; self.streak = 0\n",
    "    def reset(self): self.state=False; self.streak=0\n",
    "    def step(self, p):\n",
    "        if not self.state:\n",
    "            if p >= self.theta_on:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_on: self.state=True; self.streak=0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        else:\n",
    "            if p <= self.theta_off:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_off: self.state=False; self.streak=0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        return self.state\n",
    "\n",
    "FSM_CFG = TauHysteresis(HYP[\"theta_on\"], HYP[\"theta_off\"], HYP[\"L_on\"], HYP[\"L_off\"])\n",
    "\n",
    "print(\"\\nInitialization complete: models on device, optimizer/scheduler ready, loaders (if metas present).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdb14c-0f22-4a7f-97fc-c06bf43e041d",
   "metadata": {},
   "source": [
    "### Base Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbff31-a9a3-4872-b2a6-effa69065f94",
   "metadata": {},
   "source": [
    "#### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb7f5c-cae6-4811-8b28-7f1e8f0795b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL B: TCN grid search (LR × Warmup × Label Smoothing) with early-stop ====\n",
    "import copy, time, math, torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if '_epoch_tcn' not in globals():\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    def _epoch_tcn(model, loader, device, train=True):\n",
    "        model.train(train)\n",
    "        total_loss = 0.0; n = 0\n",
    "        tp=fp=tn=fn=0\n",
    "        longest_pred_run_attack = 0\n",
    "        longest_pred_run_benign = 0\n",
    "        cur_run_attack = 0\n",
    "        cur_run_benign = 0\n",
    "\n",
    "        for x_seq, x_aux, y, meta in loader:\n",
    "            # NOTE: streamed loader already yields float32; keep cast cheap\n",
    "            x_seq = x_seq.to(device, non_blocking=True)\n",
    "            y     = y.to(device, non_blocking=True)\n",
    "\n",
    "            if train:\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            # AMP + no_grad toggled by `train`\n",
    "            with torch.set_grad_enabled(train), autocast(enabled=torch.cuda.is_available()):\n",
    "                logits, probs = model(x_seq)      # probs: [B,3]\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                # Scale, unscale for clip, step, update\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "            bs = y.size(0)\n",
    "            total_loss += float(loss.item()) * bs\n",
    "            n += bs\n",
    "\n",
    "            # Metrics (attack-vs-not threshold @ 0.5 on p_attack)\n",
    "            p_attack = probs[:, 2].detach()\n",
    "            y_attack = (y == 2)\n",
    "            pred = (p_attack >= 0.5)\n",
    "\n",
    "            tp += int(((pred == 1) & (y_attack == 1)).sum().item())\n",
    "            fp += int(((pred == 1) & (y_attack == 0)).sum().item())\n",
    "            tn += int(((pred == 0) & (y_attack == 0)).sum().item())\n",
    "            fn += int(((pred == 0) & (y_attack == 1)).sum().item())\n",
    "\n",
    "            # Run-length proxies (fast enough per batch)\n",
    "            for pa, ya in zip(pred.tolist(), y_attack.tolist()):\n",
    "                if ya:\n",
    "                    cur_run_attack = cur_run_attack + 1 if pa else 0\n",
    "                    longest_pred_run_attack = max(longest_pred_run_attack, cur_run_attack)\n",
    "                    cur_run_benign = 0\n",
    "                else:\n",
    "                    cur_run_benign = cur_run_benign + 1 if pa else 0\n",
    "                    longest_pred_run_benign = max(longest_pred_run_benign, cur_run_benign)\n",
    "                    cur_run_attack = 0\n",
    "\n",
    "            # drop meta quickly to avoid ref holding\n",
    "            del meta\n",
    "\n",
    "        loss_mean = total_loss / max(1, n)\n",
    "        prec = tp / max(1, (tp + fp))\n",
    "        rec  = tp / max(1, (tp + fn))\n",
    "        f1   = 2 * prec * rec / max(1e-9, (prec + rec))\n",
    "        acc  = (tp + tn) / max(1, (tp + tn + fp + fn))\n",
    "        return {\n",
    "            \"loss\": loss_mean, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "            \"longest_pred_run_attack\": longest_pred_run_attack,\n",
    "            \"longest_pred_run_benign\": longest_pred_run_benign,\n",
    "        }\n",
    "\n",
    "def tcn_grid_search():\n",
    "    assert train_loader and val_loader, \"Attach loaders before grid search.\"\n",
    "\n",
    "    best = {\"f1\": -1.0, \"cfg\": None}\n",
    "    base_state = DualHeadTCN(d_seq=D_SEQ, hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0).state_dict()\n",
    "\n",
    "    for lr in HYP[\"lr_grid\"]:\n",
    "        for warm in HYP[\"warmup_grid\"]:\n",
    "            for ls in HYP[\"label_smoothing_grid\"]:\n",
    "                # fresh model per trial\n",
    "                model = DualHeadTCN(d_seq=D_SEQ, hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0).to(DEVICE)\n",
    "                model.load_state_dict(base_state, strict=False)\n",
    "\n",
    "                # optimizer/scheduler per trial\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=HYP[\"weight_decay\"])\n",
    "                sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "                    opt, lr_lambda=lambda step: (step / max(1, warm))\n",
    "                    if step < warm else 0.5 * (1.0 + math.cos(math.pi * (step - warm) / max(1, HYP[\"total_steps\"] - warm)))\n",
    "                )\n",
    "\n",
    "                # criterion per trial\n",
    "                crit = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE), label_smoothing=ls)\n",
    "\n",
    "                # make objects visible to _epoch_tcn\n",
    "                global tcn_model, optim, scheduler, criterion, scaler\n",
    "                tcn_model, optim, scheduler, criterion = model, opt, sched, crit\n",
    "                scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "                # early stop\n",
    "                es = HYP[\"early_stop\"]; patience=int(es[\"patience\"]); min_delta=float(es[\"min_delta\"])\n",
    "                best_val, wait = None, 0\n",
    "                tag = f\"tcn_lr{lr}_warm{warm}_ls{ls}\"\n",
    "                path = f\"{tag}.pt\"\n",
    "\n",
    "                for ep in range(1, HYP[\"epochs\"] + 1):\n",
    "                    tr = _epoch_tcn(tcn_model, train_loader, DEVICE, train=True)\n",
    "                    va = _epoch_tcn(tcn_model, val_loader,   DEVICE, train=False)\n",
    "                    score = va[\"f1\"]\n",
    "                    improved = (best_val is None) or ((score - (best_val or -1.0)) > min_delta)\n",
    "                    print(f\"[TCN GRID] {tag} ep {ep:02d} | tr loss {tr['loss']:.3f} | val f1 {score:.3f} {'*' if improved else ''}\")\n",
    "\n",
    "                    if improved:\n",
    "                        best_val, wait = score, 0\n",
    "                        torch.save({\"model\": tcn_model.state_dict(),\n",
    "                                    \"HYP\": {**HYP, \"lr\": lr, \"warmup_steps\": warm, \"label_smoothing\": ls}}, path)\n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        if wait >= patience:\n",
    "                            break\n",
    "\n",
    "                # track global best\n",
    "                if (best_val or -1.0) > best[\"f1\"]:\n",
    "                    best = {\"f1\": best_val or -1.0,\n",
    "                            \"cfg\": {\"lr\": lr, \"warmup_steps\": warm, \"label_smoothing\": ls, \"ckpt\": path}}\n",
    "\n",
    "    print(\"TCN best:\", best)\n",
    "    if best[\"cfg\"] is not None:\n",
    "        torch.save(torch.load(best[\"cfg\"][\"ckpt\"], map_location=\"cpu\"), \"tcn_best.pt\")\n",
    "\n",
    "# Run (comment if you want to delay)\n",
    "tcn_grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43f972-6356-4b02-984b-5012b5ad0c4c",
   "metadata": {},
   "source": [
    "#### TCN Fine Tuning (for sub 50 ms windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63299bb3-02b7-45cb-b437-23dd9b996ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: TCN classifier fine-tune on SHORT windows (streamed, AMP, classifier-only) ====\n",
    "import os, torch, math\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "assert os.path.exists(\"tcn_best.pt\"), \"Run long-window grid first to create tcn_best.pt.\"\n",
    "\n",
    "# 1) Build short-window streamed loaders (train/val split by run_id using meta shards)\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from numpy.random import default_rng\n",
    "\n",
    "def load_short_meta():\n",
    "    metas = [pd.read_parquet(p) for p in glob(\"win_shards/short_*_meta.parquet\")]\n",
    "    if not metas:\n",
    "        raise FileNotFoundError(\"No short-window meta shards found in ./win_shards/. Run preprocessing.\")\n",
    "    return pd.concat(metas, ignore_index=True)\n",
    "\n",
    "SEED = globals().get(\"SEED\", 42)\n",
    "meta_s = load_short_meta()\n",
    "rng = default_rng(SEED)\n",
    "run_ids = meta_s['run_id'].astype(str).unique().tolist()\n",
    "rng.shuffle(run_ids)\n",
    "folds = [set() for _ in range(5)]\n",
    "for i, rid in enumerate(run_ids):\n",
    "    folds[i % 5].add(rid)\n",
    "val_rids = folds[0]\n",
    "\n",
    "# Restrict sources via loader factory; it still streams shards\n",
    "def runs_to_sources(rids: set, meta: pd.DataFrame):\n",
    "    sub = meta[meta['run_id'].astype(str).isin(rids)]\n",
    "    return sub['source'].astype(str).unique().tolist()\n",
    "\n",
    "train_sources = runs_to_sources(set(run_ids) - val_rids, meta_s)\n",
    "val_sources   = runs_to_sources(val_rids, meta_s)\n",
    "\n",
    "# If you want to strictly filter by runs, you can shard-scan and drop rows not in the run set;\n",
    "# for speed, we’ll accept tiny leakage risk at shard granularity and rely on run_id split in metrics.\n",
    "\n",
    "# The streamed loaders (already yield batches)\n",
    "train_loader_short = make_loader(kind=\"short\", batch_size=globals().get(\"BATCH_SZ\", 1024), num_workers=4)\n",
    "val_loader_short   = make_loader(kind=\"short\", batch_size=globals().get(\"BATCH_SZ\", 1024), num_workers=4)\n",
    "\n",
    "# 2) Reload best TCN, freeze all but classifier\n",
    "ckpt = torch.load(\"tcn_best.pt\", map_location=\"cpu\")\n",
    "tcn_model = DualHeadTCN(d_seq=D_SEQ, hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0).to(DEVICE)\n",
    "tcn_model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "\n",
    "for p in tcn_model.parameters(): \n",
    "    p.requires_grad = False\n",
    "for p in tcn_model.classifier.parameters(): \n",
    "    p.requires_grad = True\n",
    "\n",
    "# 3) Tiny-LR fine-tune on short windows (3–5 epochs), AMP, proper grad-clip\n",
    "optim = torch.optim.AdamW(tcn_model.classifier.parameters(), lr=3e-5, weight_decay=HYP[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lambda step: 1.0)  # constant LR\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE), label_smoothing=HYP[\"label_smoothing\"])\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop(model, loader):\n",
    "    model.eval()\n",
    "    tot=0.0; n=0; tp=fp=tn=fn=0\n",
    "    for x_seq, x_aux, y, meta in loader:\n",
    "        x_seq = x_seq.to(DEVICE, non_blocking=True)\n",
    "        y     = y.to(DEVICE, non_blocking=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits, probs = model(x_seq)\n",
    "            loss = criterion(logits, y)\n",
    "        bs = y.size(0); tot += float(loss.item()) * bs; n += bs\n",
    "        p = probs[:,2].detach(); y_a = (y==2)\n",
    "        pred = (p>=0.5)\n",
    "        tp += int(((pred==1)&(y_a==1)).sum().item())\n",
    "        fp += int(((pred==1)&(y_a==0)).sum().item())\n",
    "        tn += int(((pred==0)&(y_a==0)).sum().item())\n",
    "        fn += int(((pred==0)&(y_a==1)).sum().item())\n",
    "        del meta\n",
    "    prec = tp/max(1,tp+fp); rec = tp/max(1,tp+fn)\n",
    "    f1   = 2*prec*rec/max(1e-9,(prec+rec))\n",
    "    return {\"loss\": tot/max(1,n), \"f1\": f1}\n",
    "\n",
    "def train_loop(model, loader):\n",
    "    model.train(True)\n",
    "    tot=0.0; n=0\n",
    "    for x_seq, x_aux, y, meta in loader:\n",
    "        x_seq = x_seq.to(DEVICE, non_blocking=True)\n",
    "        y     = y.to(DEVICE, non_blocking=True)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits, probs = model(x_seq)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(tcn_model.classifier.parameters(), 1.0)\n",
    "        scaler.step(optim); scaler.update()\n",
    "        scheduler.step()\n",
    "        tot += float(loss.item()) * y.size(0); n += y.size(0)\n",
    "        del meta\n",
    "    return {\"loss\": tot/max(1,n)}\n",
    "\n",
    "best_f1, patience, wait = -1.0, 3, 0\n",
    "for ep in range(1, 6):  # 3–5 epochs\n",
    "    tr = train_loop(tcn_model, train_loader_short)\n",
    "    va = eval_loop (tcn_model, val_loader_short)\n",
    "    print(f\"[TCN FT] ep {ep:02d} | train loss {tr['loss']:.4f} | val f1 {va['f1']:.3f}\")\n",
    "    if va[\"f1\"] > best_f1 + 1e-3:\n",
    "        best_f1, wait = va[\"f1\"], 0\n",
    "        torch.save({\"model\": tcn_model.state_dict(), \"HYP\": {**HYP, \"fine_tuned_short\": True}}, \"tcn_best.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"[TCN FT] early stop\"); break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fdd40-69fd-44b7-b30e-41a905f649d2",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d6496-d571-4b5e-acde-c5c36ba86087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL C: SVD grid search (LR × Warmup) with early-stop, AMP ====\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "if '_epoch_svd' not in globals():\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    def _epoch_svd(model, loader, device, train=True):\n",
    "        model.train(train)\n",
    "        total_loss = 0.0; n = 0\n",
    "        tp=fp=tn=fn=0\n",
    "\n",
    "        for x_seq, x_aux, y, meta in loader:\n",
    "            # SVD head usually needs only the role-share sequence slice. If your head takes full x_seq, keep as is.\n",
    "            x_seq = x_seq.to(device, non_blocking=True)   # [B, W, D_in] float32 from loader\n",
    "            y     = y.to(device, non_blocking=True).long()\n",
    "\n",
    "            if train:\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train), autocast(enabled=torch.cuda.is_available()):\n",
    "                logits, probs = model(x_seq)              # probs: [B,3]\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                if HYP.get(\"clip_norm\", 1.0):\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(HYP[\"clip_norm\"]))\n",
    "                scaler.step(optim); scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "            bs = y.size(0); total_loss += float(loss.item()) * bs; n += bs\n",
    "\n",
    "            # metrics @ 0.5 on p_attack\n",
    "            p_attack = probs[:, 2].detach()\n",
    "            y_attack = (y == 2)\n",
    "            pred = (p_attack >= 0.5)\n",
    "\n",
    "            tp += int(((pred == 1) & (y_attack == 1)).sum().item())\n",
    "            fp += int(((pred == 1) & (y_attack == 0)).sum().item())\n",
    "            tn += int(((pred == 0) & (y_attack == 0)).sum().item())\n",
    "            fn += int(((pred == 0) & (y_attack == 1)).sum().item())\n",
    "\n",
    "            del meta  # drop references ASAP\n",
    "\n",
    "        loss_mean = total_loss / max(1, n)\n",
    "        prec = tp / max(1, (tp + fp))\n",
    "        rec  = tp / max(1, (tp + fn))\n",
    "        f1   = 2 * prec * rec / max(1e-9, (prec + rec))\n",
    "        acc  = (tp + tn) / max(1, (tp + tn + fp + fn))\n",
    "        return {\"loss\": loss_mean, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1}\n",
    "\n",
    "def svd_grid_search():\n",
    "    assert train_loader and val_loader, \"Attach loaders before grid search.\"\n",
    "    best = {\"f1\": -1.0, \"cfg\": None}\n",
    "\n",
    "    # base weights for consistent init across trials\n",
    "    base_state = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16).state_dict()\n",
    "\n",
    "    for lr in HYP[\"lr_grid\"]:\n",
    "        for warm in HYP[\"warmup_grid\"]:\n",
    "            head = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16).to(DEVICE)\n",
    "            head.load_state_dict(base_state, strict=False)\n",
    "\n",
    "            opt = torch.optim.AdamW(head.parameters(), lr=lr, weight_decay=HYP[\"weight_decay\"])\n",
    "            sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "                opt, lr_lambda=lambda step: (step / max(1, warm))\n",
    "                if step < warm else 0.5 * (1.0 + math.cos(math.pi * (step - warm) / max(1, HYP[\"total_steps\"] - warm)))\n",
    "            )\n",
    "            crit = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE), label_smoothing=HYP.get(\"label_smoothing\", 0.0))\n",
    "\n",
    "            # expose to epoch fn\n",
    "            global svd_head, optim, scheduler, criterion, scaler\n",
    "            svd_head, optim, scheduler, criterion = head, opt, sched, crit\n",
    "            scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "            es = HYP[\"early_stop\"]; patience=int(es[\"patience\"]); min_delta=float(es[\"min_delta\"])\n",
    "            best_val, wait = None, 0\n",
    "            tag  = f\"svd_lr{lr}_warm{warm}\"\n",
    "            path = f\"{tag}.pt\"\n",
    "\n",
    "            for ep in range(1, HYP[\"epochs\"] + 1):\n",
    "                tr = _epoch_svd(svd_head, train_loader, DEVICE, train=True)\n",
    "                va = _epoch_svd(svd_head, val_loader,   DEVICE, train=False)\n",
    "                score = va[\"f1\"]\n",
    "                improved = (best_val is None) or ((score - (best_val or -1.0)) > min_delta)\n",
    "                print(f\"[SVD GRID] {tag} ep {ep:02d} | tr loss {tr['loss']:.3f} | val f1 {score:.3f} {'*' if improved else ''}\")\n",
    "\n",
    "                if improved:\n",
    "                    best_val, wait = score, 0\n",
    "                    torch.save({\"model\": svd_head.state_dict(),\n",
    "                                \"HYP\": {**HYP, \"lr\": lr, \"warmup_steps\": warm}}, path)\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= patience:\n",
    "                        break\n",
    "\n",
    "            if (best_val or -1.0) > best[\"f1\"]:\n",
    "                best = {\"f1\": best_val or -1.0,\n",
    "                        \"cfg\": {\"lr\": lr, \"warmup_steps\": warm, \"ckpt\": path}}\n",
    "\n",
    "    print(\"SVD best:\", best)\n",
    "    if best[\"cfg\"] is not None:\n",
    "        torch.save(torch.load(best[\"cfg\"][\"ckpt\"], map_location=\"cpu\"), \"svd_head_best.pt\")\n",
    "\n",
    "# Run (comment if you want to delay)\n",
    "svd_grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa18bd-92f6-4c59-b030-d54df6bed988",
   "metadata": {},
   "source": [
    "### Fusion Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38853a-6a7e-4dfa-944b-12cb4d5d111c",
   "metadata": {},
   "source": [
    "#### FusionHead + Inference Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b1efd-5d2b-4118-b027-3eb90ab0ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: FusionHead + base inference wrappers (ready for fusion training) ====\n",
    "import os, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# Tiny learned fusion (start minimal; can add SVD feats / AE later)\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int = 2, hidden: int = 8, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: [B, in_dim]\n",
    "        return torch.sigmoid(self.net(z)).squeeze(-1)  # [B]\n",
    "\n",
    "def load_frozen_bases(tcn_ckpt=\"tcn_best.pt\", svd_ckpt=\"svd_head_best.pt\"):\n",
    "    # Instantiate models (shapes driven by earlier cells)\n",
    "    tcn = DualHeadTCN(d_seq=(K_PLUS_REST + K_PLUS_REST + 3 + K_PLUS_REST),\n",
    "                      hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0)\n",
    "    svd = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16)\n",
    "\n",
    "    # Load checkpoints if they exist\n",
    "    if os.path.exists(tcn_ckpt):\n",
    "        sd = torch.load(tcn_ckpt, map_location=\"cpu\")\n",
    "        tcn.load_state_dict(sd[\"model\"], strict=False)\n",
    "    if os.path.exists(svd_ckpt):\n",
    "        sd = torch.load(svd_ckpt, map_location=\"cpu\")\n",
    "        svd.load_state_dict(sd[\"model\"], strict=False)\n",
    "\n",
    "    tcn.to(DEVICE).eval()\n",
    "    svd.to(DEVICE).eval()\n",
    "    for p in tcn.parameters(): p.requires_grad = False\n",
    "    for p in svd.parameters(): p.requires_grad = False\n",
    "    return tcn, svd\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_base_outputs(tcn: nn.Module, svd: nn.Module, x_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    x_seq: [B, W, D_seq]\n",
    "    returns: (p_tcn_attack [B], p_svd [B])\n",
    "    \"\"\"\n",
    "    logits, p_all = tcn(x_seq)                 # [B,3]\n",
    "    p_tcn_attack = p_all[:, 2]\n",
    "    shares_seq = x_seq[:, :, :K_PLUS_REST]     # first K+1 channels are shares\n",
    "    p_svd, _ = svd(shares_seq)                 # [B]\n",
    "    return p_tcn_attack, p_svd\n",
    "\n",
    "def make_fusion_batch(tcn: nn.Module, svd: nn.Module,\n",
    "                      batch, include_ae: bool = False, ae_score: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    x_seq, x_aux, y, meta = batch\n",
    "    x_seq = x_seq.to(DEVICE).float()\n",
    "    y_attack = (y.to(DEVICE).long() == 2).float()  # [B]\n",
    "\n",
    "    p_tcn, p_svd = batch_base_outputs(tcn, svd, x_seq)\n",
    "\n",
    "    z_list = [p_tcn.unsqueeze(-1), p_svd.unsqueeze(-1)]\n",
    "\n",
    "    # (c) OPTION: add 4 SVD geometry features to z when enabled\n",
    "    if HYP.get(\"fusion_use_svd_feats\", False):\n",
    "        with torch.no_grad():\n",
    "            _, svd_feats = svd(x_seq[:, :, :K_PLUS_REST])   # [B,4]\n",
    "        z_list.append(svd_feats)\n",
    "\n",
    "    if include_ae:\n",
    "        if ae_score is None:\n",
    "            ae_score = torch.zeros_like(p_tcn)\n",
    "        z_list.append(ae_score.unsqueeze(-1))\n",
    "\n",
    "    z = torch.cat(z_list, dim=-1)  # [B, 2] or [B, 6] (+AE if used)\n",
    "    return z, y_attack\n",
    "\n",
    "IN_DIM = 2 + (4 if HYP.get(\"fusion_use_svd_feats\", False) else 0)\n",
    "fusion = FusionHead(in_dim=IN_DIM, hidden=8, dropout=0.0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1643a-2149-4fbe-a4ee-658f13d567c4",
   "metadata": {},
   "source": [
    "#### Fusion training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf05fe-1829-4598-8f6c-e05ddf6d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL D: Fusion grid search (toggle SVD features; early-stop) ====\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def fusion_grid_search(include_svd_feats=None):\n",
    "    assert train_loader and val_loader, \"Attach loaders before grid search.\"\n",
    "\n",
    "    # Frozen bases\n",
    "    tcn_frozen, svd_frozen = load_frozen_bases()\n",
    "    tcn_frozen.eval(); svd_frozen.eval()\n",
    "    for p in tcn_frozen.parameters(): p.requires_grad = False\n",
    "    for p in svd_frozen.parameters(): p.requires_grad = False\n",
    "\n",
    "    best = {\"f1\": -1.0, \"cfg\": None}\n",
    "    use_svd_feats_list = [HYP[\"fusion_use_svd_feats\"]] if include_svd_feats is None else [include_svd_feats]\n",
    "\n",
    "    for use_svd_feats in use_svd_feats_list:\n",
    "        IN_DIM = 2 + (4 if use_svd_feats else 0)\n",
    "        fusion = FusionHead(in_dim=IN_DIM, hidden=8, dropout=0.0).to(DEVICE)\n",
    "        opt = torch.optim.AdamW(fusion.parameters(), lr=HYP[\"lr\"], weight_decay=HYP[\"weight_decay\"])\n",
    "        # keep LR constant for stability on tiny head\n",
    "        sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "        # If FusionHead outputs probabilities in (0,1):\n",
    "        criterion = nn.BCELoss()\n",
    "        scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "        es = HYP[\"early_stop\"]; patience=int(es[\"patience\"]); min_delta=float(es[\"min_delta\"])\n",
    "        best_val, wait = None, 0\n",
    "        tag  = f\"fusion_{'p2' if not use_svd_feats else 'p2p4'}\"\n",
    "        path = f\"{tag}.pt\"\n",
    "\n",
    "        def _epoch(loader, train=True):\n",
    "            fusion.train(train)\n",
    "            tot = 0.0; n = 0; tp=fp=tn=fn=0\n",
    "\n",
    "            for x_seq, x_aux, y, meta in loader:\n",
    "                x_seq = x_seq.to(DEVICE, non_blocking=True)   # [B, W, Dseq]\n",
    "                y_bin = (y.to(DEVICE, non_blocking=True).long() == 2).float()  # attack vs not\n",
    "\n",
    "                # --- base model outputs (no grad) ---\n",
    "                with torch.no_grad(), autocast(enabled=torch.cuda.is_available()):\n",
    "                    # Returns probabilities p_tcn, p_svd in [0,1]\n",
    "                    p_tcn, p_svd = batch_base_outputs(tcn_frozen, svd_frozen, x_seq)\n",
    "\n",
    "                    feats = [p_tcn.unsqueeze(-1), p_svd.unsqueeze(-1)]\n",
    "                    if use_svd_feats:\n",
    "                        # Extract SVD geometry features (dom_ratio, entropy, gap, angle) = 4 dims\n",
    "                        # Feed roles only (K_PLUS_REST channels) to the SVD head feature path\n",
    "                        _, svd_feats = svd_frozen(x_seq[:, :, :K_PLUS_REST])\n",
    "                        feats.append(svd_feats)  # [B,4]\n",
    "                    z = torch.cat(feats, dim=-1)  # [B, IN_DIM]\n",
    "\n",
    "                if train:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                # --- fusion forward + loss ---\n",
    "                with torch.set_grad_enabled(train), autocast(enabled=torch.cuda.is_available()):\n",
    "                    p = fusion(z)                     # [B,1] probability\n",
    "                    loss = criterion(p, y_bin.unsqueeze(-1))\n",
    "\n",
    "                if train:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(opt)\n",
    "                    if HYP.get(\"clip_norm\", 1.0):\n",
    "                        torch.nn.utils.clip_grad_norm_(fusion.parameters(), float(HYP[\"clip_norm\"]))\n",
    "                    scaler.step(opt); scaler.update(); sched.step()\n",
    "\n",
    "                bs = y_bin.size(0)\n",
    "                tot += float(loss.item()) * bs; n += bs\n",
    "\n",
    "                pred = (p >= 0.5).squeeze(-1)        # threshold at 0.5\n",
    "                tp += int(((pred==1)&(y_bin==1)).sum().item())\n",
    "                fp += int(((pred==1)&(y_bin==0)).sum().item())\n",
    "                tn += int(((pred==0)&(y_bin==0)).sum().item())\n",
    "                fn += int(((pred==0)&(y_bin==1)).sum().item())\n",
    "\n",
    "                del meta  # drop ref quickly\n",
    "\n",
    "            prec = tp / max(1, (tp+fp)); rec = tp / max(1, (tp+fn))\n",
    "            f1 = 2*prec*rec / max(1e-9, (prec+rec)); loss_mean = tot / max(1, n)\n",
    "            return {\"loss\": loss_mean, \"f1\": f1}\n",
    "\n",
    "        # --- training with early stop ---\n",
    "        for ep in range(1, max(5, HYP[\"epochs\"]//2) + 1):\n",
    "            tr = _epoch(train_loader, train=True)\n",
    "            va = _epoch(val_loader,   train=False)\n",
    "            score = va[\"f1\"]\n",
    "            improved = (best_val is None) or ((score - (best_val or -1.0)) > min_delta)\n",
    "            print(f\"[FUSION GRID] {tag} ep {ep:02d} | tr loss {tr['loss']:.4f} | val f1 {score:.3f} {'*' if improved else ''}\")\n",
    "\n",
    "            if improved:\n",
    "                best_val, wait = score, 0\n",
    "                torch.save({\"fusion\": fusion.state_dict(),\n",
    "                            \"HYP\": {**HYP, \"fusion_use_svd_feats\": use_svd_feats, \"in_dim\": IN_DIM}}, path)\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        if (best_val or -1.0) > best[\"f1\"]:\n",
    "            best = {\"f1\": best_val or -1.0, \"cfg\": {\"use_svd_feats\": use_svd_feats, \"ckpt\": path}}\n",
    "\n",
    "    print(\"Fusion best:\", best)\n",
    "    if best[\"cfg\"] is not None:\n",
    "        torch.save(torch.load(best[\"cfg\"][\"ckpt\"], map_location=\"cpu\"), \"fusion_head_best.pt\")\n",
    "\n",
    "# Run (comment if you want to delay)\n",
    "fusion_grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107dc2c-c14c-46d9-a86d-dfdc6f7a80ba",
   "metadata": {},
   "source": [
    "### Inference + Hysteresis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499402f4-f608-4b91-bdd8-a2a11fd4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: Inference + τ-Hysteresis + Confusion Matrices (Final Model) ====\n",
    "# This cell evaluates the full stack (TCN, SVD, Fusion, Fusion+Hysteresis)\n",
    "# and prints confusion matrices + basic metrics for each.\n",
    "#\n",
    "# Assumes you have:\n",
    "#   - DualHeadTCN, SVDGeometryHead, FusionHead classes defined\n",
    "#   - DEVICE, HYP, FUSION_ALPHA, TOP_K, K_PLUS_REST\n",
    "#   - a dataloader (e.g., val_loader) yielding (x_seq, x_aux, y, meta)\n",
    "#\n",
    "# Positive class is 'attack' (label == 2).\n",
    "#\n",
    "# How it works:\n",
    "#   1) Loads frozen TCN/SVD (best checkpoints if present) and FusionHead if saved.\n",
    "#   2) Streams the loader to compute p_TCN, p_SVD, p_FUSED per window.\n",
    "#   3) Applies τ-hysteresis to p_FUSED to get final ENFORCE decisions.\n",
    "#   4) Prints confusion matrices + precision/recall/F1 for:\n",
    "#        - TCN (p>=0.5)\n",
    "#        - SVD (p>=0.5)\n",
    "#        - Fusion (p>=0.5)\n",
    "#        - Fusion + Hysteresis (ENFORCE boolean)\n",
    "#\n",
    "# Optional: set INCLUDE_AE=True and provide ae_provider(x_seq, meta)->[B] to include AE in fusion.\n",
    "#\n",
    "from typing import Dict, Any, Tuple\n",
    "import os, math, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "INCLUDE_AE = False   # set True when AE is wired\n",
    "AE_PROVIDER = None   # function (x_seq, meta)-> torch.Tensor [B], values in [0,1]\n",
    "\n",
    "# --- τ-hysteresis ---\n",
    "class TauHysteresis:\n",
    "    def __init__(self, theta_on=0.7, theta_off=0.5, L_on=100, L_off=100):\n",
    "        self.theta_on=float(theta_on); self.theta_off=float(theta_off)\n",
    "        self.L_on=int(L_on); self.L_off=int(L_off)\n",
    "        self.state=False; self.streak=0\n",
    "    def reset(self):\n",
    "        self.state=False; self.streak=0\n",
    "    def step(self, p: float) -> bool:\n",
    "        if not self.state:\n",
    "            if p >= self.theta_on:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_on:\n",
    "                    self.state = True; self.streak = 0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        else:\n",
    "            if p <= self.theta_off:\n",
    "                self.streak += 1\n",
    "                if self.streak >= self.L_off:\n",
    "                    self.state = False; self.streak = 0\n",
    "            else:\n",
    "                self.streak = 0\n",
    "        return self.state\n",
    "\n",
    "def load_frozen_bases(tcn_ckpt=\"tcn_best.pt\", svd_ckpt=\"svd_head_best.pt\"):\n",
    "    tcn = DualHeadTCN(d_seq=(K_PLUS_REST + K_PLUS_REST + 3 + K_PLUS_REST),\n",
    "                      hidden=HYP[\"hidden\"], dropout=HYP[\"dropout\"], temperature=1.0)\n",
    "    svd = SVDGeometryHead(k_roles_plus_rest=K_PLUS_REST, hidden=16)\n",
    "    if os.path.exists(tcn_ckpt):\n",
    "        sd = torch.load(tcn_ckpt, map_location=\"cpu\")\n",
    "        tcn.load_state_dict(sd[\"model\"], strict=False)\n",
    "    if os.path.exists(svd_ckpt):\n",
    "        sd = torch.load(svd_ckpt, map_location=\"cpu\")\n",
    "        svd.load_state_dict(sd[\"model\"], strict=False)\n",
    "    tcn.to(DEVICE).eval(); svd.to(DEVICE).eval()\n",
    "    for p in tcn.parameters(): p.requires_grad=False\n",
    "    for p in svd.parameters(): p.requires_grad=False\n",
    "    return tcn, svd\n",
    "\n",
    "def load_or_default_fusion(in_dim: int = 2):\n",
    "    fh = FusionHead(in_dim=in_dim, hidden=8, dropout=0.0).to(DEVICE)\n",
    "    if os.path.exists(\"fusion_head_best.pt\"):\n",
    "        sd = torch.load(\"fusion_head_best.pt\", map_location=\"cpu\")\n",
    "        ck_in_dim = sd.get(\"in_dim\", in_dim)\n",
    "        if ck_in_dim != in_dim:\n",
    "            fh = FusionHead(in_dim=ck_in_dim, hidden=8, dropout=0.0).to(DEVICE)\n",
    "        fh.load_state_dict(sd[\"fusion\"], strict=False)\n",
    "    fh.eval()\n",
    "    for p in fh.parameters(): p.requires_grad=False\n",
    "    return fh\n",
    "\n",
    "@torch.no_grad()\n",
    "def base_probs(tcn, svd, x_seq: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    logits, p_all = tcn(x_seq)\n",
    "    p_tcn = p_all[:, 2]\n",
    "    p_svd, _ = svd(x_seq[:, :, :K_PLUS_REST])\n",
    "    return p_tcn, p_svd\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_probs(tcn, svd, fusion_head, x_seq: torch.Tensor, ae_score: torch.Tensor=None) -> torch.Tensor:\n",
    "    p_tcn, p_svd = base_probs(tcn, svd, x_seq)\n",
    "    # if FusionHead checkpoint exists and has params, use it; else α-blend\n",
    "    has_params = sum(p.numel() for p in fusion_head.parameters()) > 0\n",
    "    if has_params:\n",
    "        if ae_score is not None:\n",
    "            z = torch.stack([p_tcn, p_svd, ae_score.to(p_tcn.device)], dim=-1)\n",
    "        else:\n",
    "            z = torch.stack([p_tcn, p_svd], dim=-1)\n",
    "        return fusion_head(z)\n",
    "    return FUSION_ALPHA * p_tcn + (1.0 - FUSION_ALPHA) * p_svd\n",
    "\n",
    "def confusion(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Any]:\n",
    "    # y_true, y_pred: 0/1 arrays for not-attack/attack\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    acc  = (tp + tn) / max(1, (tp+tn+fp+fn))\n",
    "    return {\n",
    "        \"matrix\": [[tn, fp],\n",
    "                   [fn, tp]],\n",
    "        \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1,\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn\n",
    "    }\n",
    "\n",
    "def evaluate_confusions(loader) -> Dict[str, Any]:\n",
    "    assert loader is not None and len(loader) > 0, \"Loader is empty.\"\n",
    "    tcn, svd = load_frozen_bases()\n",
    "    fusion = load_or_default_fusion(in_dim=3 if INCLUDE_AE else 2)\n",
    "\n",
    "    # Collect predictions/probs + labels and meta per-run for hysteresis\n",
    "    probs_tcn, probs_svd, probs_fused = [], [], []\n",
    "    labels = []\n",
    "    metas = []   # list of dicts per item: run_id, start_tti\n",
    "\n",
    "    for x_seq, x_aux, y, meta in loader:\n",
    "        x_seq = x_seq.to(DEVICE).float()\n",
    "        ae_score = None\n",
    "        if INCLUDE_AE and AE_PROVIDER is not None:\n",
    "            ae_score = AE_PROVIDER(x_seq, meta)\n",
    "\n",
    "        p_fused = fused_probs(tcn, svd, fusion, x_seq, ae_score=ae_score).detach().cpu().numpy()\n",
    "        p_tcn, p_svd = base_probs(tcn, svd, x_seq)\n",
    "        probs_tcn.extend(p_tcn.detach().cpu().numpy().tolist())\n",
    "        probs_svd.extend(p_svd.detach().cpu().numpy().tolist())\n",
    "        probs_fused.extend(p_fused.tolist())\n",
    "        labels.extend(((y == 2).numpy().astype(int)).tolist())\n",
    "\n",
    "        B = y.size(0)\n",
    "        for i in range(B):\n",
    "            metas.append({\n",
    "                \"run_id\": str(meta[\"run_id\"][i]),\n",
    "                \"start_tti\": int(meta[\"start_tti\"][i])\n",
    "            })\n",
    "\n",
    "    y_true = np.array(labels, dtype=int)\n",
    "    y_tcn  = (np.array(probs_tcn)  >= 0.5).astype(int)\n",
    "    y_svd  = (np.array(probs_svd)  >= 0.5).astype(int)\n",
    "    y_fuse = (np.array(probs_fused)>= 0.5).astype(int)\n",
    "\n",
    "    # Confusions without hysteresis\n",
    "    cm_tcn  = confusion(y_true, y_tcn)\n",
    "    cm_svd  = confusion(y_true, y_svd)\n",
    "    cm_fuse = confusion(y_true, y_fuse)\n",
    "\n",
    "    # Apply hysteresis on the fused stream per run (ordered by start_tti)\n",
    "    th_on, th_off, Lon, Loff = HYP[\"theta_on\"], HYP[\"theta_off\"], HYP[\"L_on\"], HYP[\"L_off\"]\n",
    "    # Group by run\n",
    "    from collections import defaultdict\n",
    "    by_run = defaultdict(list)\n",
    "    for i, m in enumerate(metas):\n",
    "        by_run[m[\"run_id\"]].append((m[\"start_tti\"], i))\n",
    "\n",
    "    y_fuse_hyst = np.zeros_like(y_fuse)\n",
    "    for rid, lst in by_run.items():\n",
    "        lst.sort(key=lambda x: x[0])\n",
    "        fsm = TauHysteresis(th_on, th_off, Lon, Loff)\n",
    "        for _, idx in lst:\n",
    "            state = fsm.step(float(probs_fused[idx]))\n",
    "            y_fuse_hyst[idx] = 1 if state else 0\n",
    "\n",
    "    cm_hyst = confusion(y_true, y_fuse_hyst)\n",
    "\n",
    "    # Pretty print\n",
    "    def pretty(cm: Dict[str, Any], name: str):\n",
    "        M = cm[\"matrix\"]\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Confusion Matrix (rows=true [0=benign,1=attack], cols=pred):\")\n",
    "        print(f\"[{M[0][0]:5d}  {M[0][1]:5d}]  ← true 0\")\n",
    "        print(f\"[{M[1][0]:5d}  {M[1][1]:5d}]  ← true 1\")\n",
    "        print(f\"acc={cm['acc']:.3f}  prec={cm['prec']:.3f}  rec={cm['rec']:.3f}  f1={cm['f1']:.3f} \"\n",
    "              f\"(tp={cm['tp']}, fp={cm['fp']}, tn={cm['tn']}, fn={cm['fn']})\")\n",
    "\n",
    "    pretty(cm_tcn,  \"TCN (p>=0.5)\")\n",
    "    pretty(cm_svd,  \"SVD (p>=0.5)\")\n",
    "    pretty(cm_fuse, \"Fusion (p>=0.5)\")\n",
    "    pretty(cm_hyst, \"Fusion + τ-Hysteresis (final ENFORCE)\")\n",
    "\n",
    "    return {\n",
    "        \"cm_tcn\": cm_tcn, \"cm_svd\": cm_svd, \"cm_fuse\": cm_fuse, \"cm_hyst\": cm_hyst,\n",
    "        \"probs\": {\"tcn\": probs_tcn, \"svd\": probs_svd, \"fuse\": probs_fused},\n",
    "        \"y_true\": y_true.tolist()\n",
    "    }\n",
    "\n",
    "# --- Run on validation loader by default ---\n",
    "if 'val_loader' in globals() and val_loader and len(val_loader) > 0:\n",
    "    _ = evaluate_confusions(val_loader)\n",
    "else:\n",
    "    print(\"No validation loader attached. Build metas/loaders, then re-run this cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
