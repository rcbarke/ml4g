{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c319e0-eeb6-448d-9167-59928fb81e0d",
   "metadata": {},
   "source": [
    "# PRB-Graph: Resource Management Reconstruction GNN  \n",
    "## CPSC 8810 — ML4G Term Project\n",
    "### Authored by Ryan Barker\n",
    "\n",
    "This notebook implements a minimal viable **PRB-Graph** model: an offline, UE-level graph neural network that reconstructs resource-management behavior from noisy per-TTI proportional-fair (PF) scheduler logs. Using the existing TTI-Trust Anamoly Detector + Classifier preprocessing pipeline, each sliding window of TTIs is converted into a **UE-contention graph** whose nodes represent UEs active in the window and whose edges capture co-scheduling (PRB contention) relationships. Node features summarize PRB usage statistics, fairness/PF surrogates, and starvation patterns, while a compact **GraphSAGE → global pooling → MLP** architecture performs **graph-level classification** to distinguish benign scheduling behavior from PF-based PRB starvation indicative of attack conditions. This MVP focuses on methodology rather than headline accuracy, acknowledging the dataset’s timing noise and simulator artifacts, and serves as a prototype for future training on GPU-native, slot-accurate Aerial/cuMAC traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f0191-075a-48a1-ab6b-82eeabe344e1",
   "metadata": {},
   "source": [
    "> <span style=\"color:red; font-weight:bold\">Disclaimer — Prototype PRB-Graph Results</span>  \n",
    ">  \n",
    "> The PRB-Graph results in this notebook were obtained **only on a small, noisy prototype dataset** derived from the original ~30 GB Open Air Interface simulator logs. Due to known timing noise within the low-latency experiment which generated the data, label corruption, and severe class imbalance in this dataset, **the model was deliberately *not* tuned or optimized to the full data volume**.  \n",
    ">  \n",
    "> These experiments are intended to validate the **model architecture and end-to-end graph construction pipeline only**. The reported metrics (including the best configuration  \n",
    "> `hidden_dim=32, num_layers=1, dropout=0.0, conv_type=\"sage\", aggr=\"mean\", lr=2e-4` with `F1 = 0.0`) **should not be interpreted as meaningful performance claims for PRB starvation detection in real RAN systems**.  \n",
    ">  \n",
    "> A proper hyperparameter search and performance evaluation will be conducted **only after more accurate PF ledgers are collected from the new NVIDIA AI RAN stack**, at which point the same PRB-GraphSAGE code will be re-used on cleaner, slot-accurate traces. The limitations of the current dataset and their impact on model behavior will be discussed in detail in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f84dda-2fe7-4959-80fa-d8a0d1fb75c6",
   "metadata": {},
   "source": [
    "### preproc_tti_trust.py\n",
    "\n",
    "This script is a preprocessing scaffold for the TTI-Trust project that converts lightweight attack/benign CSV “shims” into parquet datasets, derives scheduler-native, identity-agnostic features, and emits windowed shards plus cross-validation splits for PyTorch. It first materializes CSVs into partitioned parquet (by `run_id`), adding a relative time axis `sec_rel` if needed, then for each run it auto-detects UE resource-block columns, computes `total_rb`, normalizes RB usage into per-UE PRB shares, and derives a Jain’s fairness index `J`. On top of these per-UE shares it builds “roles” by tracking an exponential-moving-average of the shares, selecting the top-K dominant UEs per TTI, and aggregating everyone else into a `rest_share` and corresponding “small RB” indicators, plus 1-second presence flags to capture coarse duty cycle behavior. Using these role-based features and the `phase` labels, it slides long and short windows (e.g., 240- and 64-TTI) with multiple strides, labels each window as benign, attack, or ambiguous via a dominance test on the top role (with thresholds like `ATTACK_DOMINANCE`, `OTHERS_MAX`, and `DUTY_KEEP`), and batches them into compressed NumPy `.npz` shards alongside parquet meta tables that record window boundaries, labels, and summary statistics. The script writes enriched per-run parquet files, then populates a `win_shards` directory with both long and short window shards for attack and benign sources. Finally, it builds grouped K-fold splits where entire `run_id`s are kept together in either train or validation, returning index pairs for each fold and printing a brief summary so downstream training can stream shards or assemble cross-validation sets directly from the generated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb1b65f-ab4d-4e4b-a8d5-44c615516c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[attack] Found 13 run_id partitions.\n",
      "[attack] run_id=040933 → attack_enriched_040933.parquet  (rows=1,388,017)\n",
      "[attack] run_id=171614 → attack_enriched_171614.parquet  (rows=762,483)\n",
      "[attack] run_id=173450 → attack_enriched_173450.parquet  (rows=1,470,117)\n",
      "[attack] run_id=175206 → attack_enriched_175206.parquet  (rows=2,219,957)\n",
      "[attack] run_id=181054 → attack_enriched_181054.parquet  (rows=2,947,494)\n",
      "[attack] run_id=183046 → attack_enriched_183046.parquet  (rows=2,045,825)\n",
      "[attack] run_id=185206 → attack_enriched_185206.parquet  (rows=4,292,603)\n",
      "[attack] run_id=190930 → attack_enriched_190930.parquet  (rows=4,962,590)\n",
      "[attack] run_id=192814 → attack_enriched_192814.parquet  (rows=5,550,536)\n",
      "[attack] run_id=194737 → attack_enriched_194737.parquet  (rows=1,450,956)\n",
      "[attack] run_id=200821 → attack_enriched_200821.parquet  (rows=6,819,582)\n",
      "[attack] run_id=202815 → attack_enriched_202815.parquet  (rows=7,485,965)\n",
      "[attack] run_id=204929 → attack_enriched_204929.parquet  (rows=1,227,412)\n",
      "[benign] Found 109 run_id partitions.\n",
      "[benign] run_id=000600 → benign_enriched_000600.parquet  (rows=499,780)\n",
      "[benign] run_id=001708 → benign_enriched_001708.parquet  (rows=519,109)\n",
      "[benign] run_id=004912 → benign_enriched_004912.parquet  (rows=1,298,856)\n",
      "[benign] run_id=005107 → benign_enriched_005107.parquet  (rows=445,348)\n",
      "[benign] run_id=010225 → benign_enriched_010225.parquet  (rows=408,517)\n",
      "[benign] run_id=011315 → benign_enriched_011315.parquet  (rows=437,742)\n",
      "[benign] run_id=012352 → benign_enriched_012352.parquet  (rows=374,793)\n",
      "[benign] run_id=013225 → benign_enriched_013225.parquet  (rows=205,115)\n",
      "[benign] run_id=013910 → benign_enriched_013910.parquet  (rows=85,453)\n",
      "[benign] run_id=014216 → benign_enriched_014216.parquet  (rows=218,782)\n",
      "[benign] run_id=014928 → benign_enriched_014928.parquet  (rows=248,619)\n",
      "[benign] run_id=015704 → benign_enriched_015704.parquet  (rows=112,364)\n",
      "[benign] run_id=020020 → benign_enriched_020020.parquet  (rows=194,191)\n",
      "[benign] run_id=020539 → benign_enriched_020539.parquet  (rows=211,948)\n",
      "[benign] run_id=021147 → benign_enriched_021147.parquet  (rows=204,102)\n",
      "[benign] run_id=021742 → benign_enriched_021742.parquet  (rows=246,956)\n",
      "[benign] run_id=022443 → benign_enriched_022443.parquet  (rows=236,738)\n",
      "[benign] run_id=023253 → benign_enriched_023253.parquet  (rows=176,205)\n",
      "[benign] run_id=023835 → benign_enriched_023835.parquet  (rows=192,316)\n",
      "[benign] run_id=024546 → benign_enriched_024546.parquet  (rows=333,933)\n",
      "[benign] run_id=031313 → benign_enriched_031313.parquet  (rows=271,946)\n",
      "[benign] run_id=031931 → benign_enriched_031931.parquet  (rows=350,399)\n",
      "[benign] run_id=032701 → benign_enriched_032701.parquet  (rows=342,052)\n",
      "[benign] run_id=033542 → benign_enriched_033542.parquet  (rows=252,750)\n",
      "[benign] run_id=034135 → benign_enriched_034135.parquet  (rows=388,818)\n",
      "[benign] run_id=034943 → benign_enriched_034943.parquet  (rows=757,774)\n",
      "[benign] run_id=081026 → benign_enriched_081026.parquet  (rows=493,600)\n",
      "[benign] run_id=082051 → benign_enriched_082051.parquet  (rows=492,151)\n",
      "[benign] run_id=083133 → benign_enriched_083133.parquet  (rows=558,230)\n",
      "[benign] run_id=084310 → benign_enriched_084310.parquet  (rows=580,990)\n",
      "[benign] run_id=085438 → benign_enriched_085438.parquet  (rows=596,746)\n",
      "[benign] run_id=090520 → benign_enriched_090520.parquet  (rows=548,873)\n",
      "[benign] run_id=091619 → benign_enriched_091619.parquet  (rows=519,562)\n",
      "[benign] run_id=092603 → benign_enriched_092603.parquet  (rows=643,342)\n",
      "[benign] run_id=094110 → benign_enriched_094110.parquet  (rows=237,259)\n",
      "[benign] run_id=094736 → benign_enriched_094736.parquet  (rows=179,891)\n",
      "[benign] run_id=095222 → benign_enriched_095222.parquet  (rows=303,735)\n",
      "[benign] run_id=095922 → benign_enriched_095922.parquet  (rows=230,346)\n",
      "[benign] run_id=100505 → benign_enriched_100505.parquet  (rows=242,567)\n",
      "[benign] run_id=101038 → benign_enriched_101038.parquet  (rows=304,118)\n",
      "[benign] run_id=101703 → benign_enriched_101703.parquet  (rows=191,927)\n",
      "[benign] run_id=102152 → benign_enriched_102152.parquet  (rows=216,832)\n",
      "[benign] run_id=102652 → benign_enriched_102652.parquet  (rows=208,774)\n",
      "[benign] run_id=103131 → benign_enriched_103131.parquet  (rows=252,910)\n",
      "[benign] run_id=103824 → benign_enriched_103824.parquet  (rows=323,438)\n",
      "[benign] run_id=104623 → benign_enriched_104623.parquet  (rows=311,188)\n",
      "[benign] run_id=105352 → benign_enriched_105352.parquet  (rows=362,090)\n",
      "[benign] run_id=110233 → benign_enriched_110233.parquet  (rows=227,632)\n",
      "[benign] run_id=110755 → benign_enriched_110755.parquet  (rows=311,324)\n",
      "[benign] run_id=111431 → benign_enriched_111431.parquet  (rows=422,387)\n",
      "[benign] run_id=112443 → benign_enriched_112443.parquet  (rows=321,805)\n",
      "[benign] run_id=113211 → benign_enriched_113211.parquet  (rows=406,035)\n",
      "[benign] run_id=114052 → benign_enriched_114052.parquet  (rows=500,698)\n",
      "[benign] run_id=115313 → benign_enriched_115313.parquet  (rows=470,145)\n",
      "[benign] run_id=120347 → benign_enriched_120347.parquet  (rows=454,286)\n",
      "[benign] run_id=121336 → benign_enriched_121336.parquet  (rows=528,873)\n",
      "[benign] run_id=122608 → benign_enriched_122608.parquet  (rows=510,180)\n",
      "[benign] run_id=123651 → benign_enriched_123651.parquet  (rows=437,613)\n",
      "[benign] run_id=124530 → benign_enriched_124530.parquet  (rows=488,783)\n",
      "[benign] run_id=125623 → benign_enriched_125623.parquet  (rows=557,295)\n",
      "[benign] run_id=130800 → benign_enriched_130800.parquet  (rows=516,359)\n",
      "[benign] run_id=131809 → benign_enriched_131809.parquet  (rows=171,338)\n",
      "[benign] run_id=132304 → benign_enriched_132304.parquet  (rows=203,943)\n",
      "[benign] run_id=132819 → benign_enriched_132819.parquet  (rows=355,081)\n",
      "[benign] run_id=133630 → benign_enriched_133630.parquet  (rows=275,041)\n",
      "[benign] run_id=134319 → benign_enriched_134319.parquet  (rows=153,208)\n",
      "[benign] run_id=134703 → benign_enriched_134703.parquet  (rows=228,588)\n",
      "[benign] run_id=135204 → benign_enriched_135204.parquet  (rows=212,178)\n",
      "[benign] run_id=135723 → benign_enriched_135723.parquet  (rows=179,149)\n",
      "[benign] run_id=140138 → benign_enriched_140138.parquet  (rows=290,220)\n",
      "[benign] run_id=143552 → benign_enriched_143552.parquet  (rows=397,053)\n",
      "[benign] run_id=161300 → benign_enriched_161300.parquet  (rows=289,216)\n",
      "[benign] run_id=162043 → benign_enriched_162043.parquet  (rows=268,890)\n",
      "[benign] run_id=162752 → benign_enriched_162752.parquet  (rows=377,679)\n",
      "[benign] run_id=163656 → benign_enriched_163656.parquet  (rows=346,262)\n",
      "[benign] run_id=164450 → benign_enriched_164450.parquet  (rows=349,550)\n",
      "[benign] run_id=174456 → benign_enriched_174456.parquet  (rows=350,159)\n",
      "[benign] run_id=175325 → benign_enriched_175325.parquet  (rows=296,717)\n",
      "[benign] run_id=180036 → benign_enriched_180036.parquet  (rows=407,434)\n",
      "[benign] run_id=180920 → benign_enriched_180920.parquet  (rows=479,186)\n",
      "[benign] run_id=182057 → benign_enriched_182057.parquet  (rows=518,648)\n",
      "[benign] run_id=183230 → benign_enriched_183230.parquet  (rows=481,893)\n",
      "[benign] run_id=202930 → benign_enriched_202930.parquet  (rows=495,707)\n",
      "[benign] run_id=204038 → benign_enriched_204038.parquet  (rows=525,696)\n",
      "[benign] run_id=205129 → benign_enriched_205129.parquet  (rows=380,188)\n",
      "[benign] run_id=205908 → benign_enriched_205908.parquet  (rows=478,475)\n",
      "[benign] run_id=210950 → benign_enriched_210950.parquet  (rows=577,142)\n",
      "[benign] run_id=212143 → benign_enriched_212143.parquet  (rows=526,548)\n",
      "[benign] run_id=213208 → benign_enriched_213208.parquet  (rows=203,936)\n",
      "[benign] run_id=213740 → benign_enriched_213740.parquet  (rows=131,812)\n",
      "[benign] run_id=214119 → benign_enriched_214119.parquet  (rows=327,188)\n",
      "[benign] run_id=214928 → benign_enriched_214928.parquet  (rows=235,420)\n",
      "[benign] run_id=215514 → benign_enriched_215514.parquet  (rows=232,444)\n",
      "[benign] run_id=220032 → benign_enriched_220032.parquet  (rows=205,768)\n",
      "[benign] run_id=220509 → benign_enriched_220509.parquet  (rows=209,188)\n",
      "[benign] run_id=221019 → benign_enriched_221019.parquet  (rows=212,645)\n",
      "[benign] run_id=221509 → benign_enriched_221509.parquet  (rows=300,026)\n",
      "[benign] run_id=222134 → benign_enriched_222134.parquet  (rows=306,478)\n",
      "[benign] run_id=222931 → benign_enriched_222931.parquet  (rows=256,295)\n",
      "[benign] run_id=223602 → benign_enriched_223602.parquet  (rows=317,902)\n",
      "[benign] run_id=224320 → benign_enriched_224320.parquet  (rows=390,327)\n",
      "[benign] run_id=225237 → benign_enriched_225237.parquet  (rows=329,557)\n",
      "[benign] run_id=230000 → benign_enriched_230000.parquet  (rows=417,252)\n",
      "[benign] run_id=230851 → benign_enriched_230851.parquet  (rows=333,737)\n",
      "[benign] run_id=231656 → benign_enriched_231656.parquet  (rows=287,845)\n",
      "[benign] run_id=232322 → benign_enriched_232322.parquet  (rows=484,680)\n",
      "[benign] run_id=233332 → benign_enriched_233332.parquet  (rows=510,370)\n",
      "[benign] run_id=234618 → benign_enriched_234618.parquet  (rows=469,816)\n",
      "[benign] run_id=235640 → benign_enriched_235640.parquet  (rows=438,430)\n",
      "\n",
      "[long] meta=(17037036, 11), groups=122, folds=5\n",
      "[short] meta=(34082917, 11), groups=122, folds=5\n",
      "\n",
      "Preprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\n"
     ]
    }
   ],
   "source": [
    "# TTI-Trust preprocessing scaffold for PyTorch: identity-agnostic feature composer,\n",
    "# phase-aware windowing, and grouped K-fold splits.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, warnings, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa, pyarrow.csv as pv, pyarrow.parquet as pq\n",
    "\n",
    "# -------------------- DATASETS --------------------\n",
    "ATTACK_FN = \"data/full_dataset/prb_tti_evidence_ai_attack.csv\" # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "BENIGN_FN = \"data/full_dataset/prb_tti_evidence_ai_benign.csv\"  # Committing lightweight shims to the repo due to storage quotas, full 15GB dataset available upon request\n",
    "\n",
    "# -------------------- Constants aligned to paper --------------------\n",
    "TTI_SEC = 0.0005\n",
    "C_PRB   = 106\n",
    "\n",
    "W_LONG  = 240\n",
    "W_SHORT = 64\n",
    "\n",
    "STRIDES_LONG  = [8, 12]\n",
    "STRIDES_SHORT = [4, 6]\n",
    "\n",
    "TOP_K = 6\n",
    "SMALL_RB_EPS = 4\n",
    "\n",
    "ATTACK_DOMINANCE = 0.90\n",
    "OTHERS_MAX       = 0.10\n",
    "DUTY_KEEP        = 0.90\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def csv_to_parquet(src_csv: str, dest_root: str):\n",
    "    os.makedirs(dest_root, exist_ok=True)\n",
    "    for _root, _dirs, files in os.walk(dest_root):\n",
    "        if any(f.endswith(\".parquet\") for f in files):\n",
    "            return\n",
    "\n",
    "    convert_cols = {\"run_id\": pa.large_string(), \"phase\": pa.large_string()}\n",
    "    read_opts  = pv.ReadOptions(block_size=1<<26)\n",
    "    conv_opts  = pv.ConvertOptions(column_types=convert_cols, strings_can_be_null=True)\n",
    "    t = pv.read_csv(src_csv, read_options=read_opts, convert_options=conv_opts)\n",
    "\n",
    "    if \"run_id\" not in t.schema.names:\n",
    "        stem = os.path.splitext(os.path.basename(src_csv))[0]\n",
    "        run_id = pa.array(np.full(t.num_rows, stem), type=pa.large_string())\n",
    "        t = t.append_column(\"run_id\", run_id)\n",
    "\n",
    "    if \"sec_rel\" not in t.schema.names:\n",
    "        n = t.num_rows\n",
    "        sec_rel = pa.array((np.arange(n, dtype=np.int32) * np.float32(TTI_SEC)), type=pa.float32())\n",
    "        t = t.append_column(\"sec_rel\", sec_rel)\n",
    "\n",
    "    pq.write_to_dataset(t, root_path=dest_root, partition_cols=[\"run_id\"],\n",
    "                        compression=\"zstd\", use_dictionary=True)\n",
    "\n",
    "def _detect_ue_rb_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cand = [c for c in df.columns if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in df.columns if c.lower().endswith('_rb')]\n",
    "    def _key(c):\n",
    "        m = re.search(r'(\\d+)', c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "def _ensure_total_rb(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    if 'total_rb' not in df.columns:\n",
    "        df['total_rb'] = df[ue_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def _compute_shares_and_fairness(df: pd.DataFrame, ue_cols: List[str]) -> pd.DataFrame:\n",
    "    shares = (df[ue_cols].clip(lower=0).astype('float32') / np.float32(C_PRB))\n",
    "    shares.columns = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    for c in shares.columns:\n",
    "        df[c] = shares[c]\n",
    "    x = shares.to_numpy(dtype='float32', copy=False)\n",
    "    sum_x  = x.sum(axis=1, dtype='float32')\n",
    "    sum_x2 = (x*x).sum(axis=1, dtype='float32')\n",
    "    n = np.float32(max(1, len(ue_cols)))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        J = (sum_x*sum_x) / (n * sum_x2)\n",
    "    df['J'] = np.nan_to_num(J, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    return df\n",
    "\n",
    "def _compose_roles(df: pd.DataFrame, ue_cols: List[str], top_k: int = TOP_K) -> pd.DataFrame:\n",
    "    share_cols = [c.replace('_rb','_share') for c in ue_cols]\n",
    "    alpha = 2.0 / (20 + 1.0)\n",
    "    for c in share_cols:\n",
    "        df[c+\"_ema\"] = df[c].astype('float32').ewm(alpha=alpha, adjust=False).mean().astype('float32')\n",
    "\n",
    "    S   = df[share_cols].to_numpy(dtype='float32', copy=False)\n",
    "    E   = df[[c+\"_ema\" for c in share_cols]].to_numpy(dtype='float32', copy=False)\n",
    "    PRB = df[ue_cols].to_numpy(dtype='int32',    copy=False)\n",
    "\n",
    "    T, U = S.shape\n",
    "    k = min(top_k, U)\n",
    "    order  = np.argsort(-E, axis=1)\n",
    "    top_ix = order[:, :k]\n",
    "\n",
    "    roles       = np.take_along_axis(S,   top_ix, axis=1).astype('float32')\n",
    "    roles_small = np.take_along_axis((PRB < SMALL_RB_EPS).astype('float32'), top_ix, axis=1)\n",
    "\n",
    "    sum_all    = S.sum(axis=1, dtype='float32')\n",
    "    sum_topk   = roles.sum(axis=1, dtype='float32')\n",
    "    rest_share = (sum_all - sum_topk).clip(min=0).astype('float32')\n",
    "\n",
    "    if U > k:\n",
    "        row_ix = np.arange(T)[:, None]\n",
    "        mask = np.ones_like(S, dtype=bool); mask[row_ix, top_ix] = False\n",
    "        cnt  = mask.sum(axis=1)\n",
    "        rest_small = ((PRB < SMALL_RB_EPS).astype('float32') * mask).sum(axis=1) / np.maximum(cnt, 1)\n",
    "    else:\n",
    "        rest_small = np.zeros(T, dtype='float32')\n",
    "\n",
    "    for i in range(k):\n",
    "        df[f'role{i+1}_share']   = roles[:, i]\n",
    "        df[f'role{i+1}_smallrb'] = roles_small[:, i]\n",
    "    for i in range(k, top_k):\n",
    "        df[f'role{i+1}_share']   = np.float32(0.0)\n",
    "        df[f'role{i+1}_smallrb'] = np.float32(0.0)\n",
    "    df['rest_share']   = rest_share\n",
    "    df['rest_smallrb'] = rest_small\n",
    "\n",
    "    sec_bin = np.floor(df['sec_rel'].to_numpy(dtype='float32')).astype('int32')\n",
    "    max_sec = int(sec_bin.max(initial=0))\n",
    "    def sec_presence(vec_bool: np.ndarray) -> np.ndarray:\n",
    "        agg = np.zeros(max_sec + 1, dtype='uint8')\n",
    "        np.maximum.at(agg, sec_bin, vec_bool.astype('uint8'))\n",
    "        return agg[sec_bin]\n",
    "    for i in range(top_k):\n",
    "        df[f'role{i+1}_present_1s'] = sec_presence((df[f'role{i+1}_share'].to_numpy() > 0))\n",
    "    df['rest_present_1s'] = sec_presence((df['rest_share'].to_numpy() > 0))\n",
    "    return df\n",
    "\n",
    "def _label_window(shares_mat: np.ndarray) -> bool:\n",
    "    if shares_mat.size == 0:\n",
    "        return False\n",
    "    top    = shares_mat[:, 0]\n",
    "    others = shares_mat[:, 1:]\n",
    "    ok = (top >= ATTACK_DOMINANCE) & (np.max(others, axis=1) <= OTHERS_MAX)\n",
    "    return (ok.mean() >= DUTY_KEEP)\n",
    "\n",
    "@dataclass\n",
    "class WindowSpec:\n",
    "    length_tti: int\n",
    "    stride_tti: int\n",
    "\n",
    "def iter_windows(df: pd.DataFrame, run_id: str, spec: WindowSpec, role_cols: List[str],\n",
    "                 phases_attack={\"attack\"}, phases_benign={\"baseline\",\"recovery\",\"benign\"},\n",
    "                 batch_windows: int = 8192):\n",
    "    W, S = spec.length_tti, spec.stride_tti\n",
    "    R = df[role_cols].to_numpy(dtype='float32', copy=False)\n",
    "    phase = df['phase'].astype(str).to_numpy()\n",
    "    T = len(df)\n",
    "    starts = np.arange(0, max(0, T - W + 1), S, dtype=np.int64)\n",
    "\n",
    "    bufX, rows = [], []\n",
    "    for s in starts:\n",
    "        e = s + W\n",
    "        blk = R[s:e, :]\n",
    "        if blk.shape[0] != W:\n",
    "            continue\n",
    "        win_phase = phase[s:e]\n",
    "        frac_attack = np.isin(win_phase, list(phases_attack)).mean()\n",
    "        frac_benign = np.isin(win_phase, list(phases_benign)).mean()\n",
    "        looks_attacky = _label_window(blk)\n",
    "        if (frac_attack >= 0.90) and looks_attacky: y = 2\n",
    "        elif (frac_benign >= 0.90):                y = 0\n",
    "        else:                                      y = 1\n",
    "\n",
    "        bufX.append(blk)\n",
    "        rows.append((run_id, int(s), int(e), int(W), int(y),\n",
    "                     float(frac_attack), float(frac_benign), int(looks_attacky)))\n",
    "\n",
    "        if len(bufX) == batch_windows:\n",
    "            X = np.stack(bufX, axis=0)\n",
    "            meta = pd.DataFrame.from_records(\n",
    "                rows,\n",
    "                columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                         \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "            )\n",
    "            yield X, meta\n",
    "            bufX.clear(); rows.clear()\n",
    "\n",
    "    if bufX:\n",
    "        X = np.stack(bufX, axis=0)\n",
    "        meta = pd.DataFrame.from_records(\n",
    "            rows,\n",
    "            columns=[\"run_id\",\"start_tti\",\"end_tti\",\"window_len\",\"label\",\n",
    "                     \"frac_attack\",\"frac_benign\",\"looks_attacky\"]\n",
    "        )\n",
    "        yield X, meta\n",
    "\n",
    "# -------------------- Main entry: build datasets (partitioned, low RAM) --------------------\n",
    "\n",
    "csv_to_parquet(ATTACK_FN, \"parquet/attack\")\n",
    "csv_to_parquet(BENIGN_FN, \"parquet/benign\")\n",
    "\n",
    "def iter_run_partitions(root: str):\n",
    "    for run_dir in sorted(glob(os.path.join(root, \"run_id=*\"))):\n",
    "        rid = os.path.basename(run_dir).split(\"=\", 1)[1]\n",
    "        df = pd.read_parquet(run_dir, engine=\"pyarrow\")\n",
    "        if 'run_id' not in df.columns:\n",
    "            df['run_id'] = rid\n",
    "        if 'phase' not in df.columns:\n",
    "            df['phase'] = 'unknown'\n",
    "        if 'sec_rel' not in df.columns:\n",
    "            df['sec_rel'] = (np.arange(len(df), dtype=np.int32) * np.float32(TTI_SEC)).astype('float32')\n",
    "        yield rid, df\n",
    "\n",
    "def process_source(root: str, src_name: str):\n",
    "    if not os.path.isdir(root):\n",
    "        print(f\"[{src_name}] No parquet root found at {root} (skipping).\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"win_shards\", exist_ok=True)\n",
    "    shard_counter = itertools.count(0)\n",
    "\n",
    "    runs = list(iter_run_partitions(root))\n",
    "    if not runs:\n",
    "        print(f\"[{src_name}] No run_id partitions found in {root}.\")\n",
    "        return\n",
    "    print(f\"[{src_name}] Found {len(runs)} run_id partitions.\")\n",
    "\n",
    "    for rid, df in runs:\n",
    "        ue_cols = _detect_ue_rb_columns(df)\n",
    "        if not ue_cols:\n",
    "            warnings.warn(f\"[{src_name}] run_id={rid}: no UE *_rb columns; skipping\")\n",
    "            continue\n",
    "\n",
    "        df = _ensure_total_rb(df, ue_cols)\n",
    "        df = _compute_shares_and_fairness(df, ue_cols)\n",
    "        df = _compose_roles(df, ue_cols, top_k=TOP_K)\n",
    "\n",
    "        keep = [f'role{i}_share' for i in range(1, TOP_K+1)] + ['rest_share','J','sec_rel','run_id','phase']\n",
    "        enriched_out = f\"{src_name}_enriched_{rid}.parquet\"\n",
    "        df[keep].to_parquet(enriched_out, index=False)\n",
    "        print(f\"[{src_name}] run_id={rid} → {enriched_out}  (rows={len(df):,})\")\n",
    "\n",
    "        role_cols = [f'role{k}_share' for k in range(1, TOP_K+1)] + ['rest_share']\n",
    "        g = df.reset_index(drop=True)\n",
    "\n",
    "        for stride in STRIDES_LONG:\n",
    "            spec = WindowSpec(length_tti=W_LONG, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"long\"\n",
    "                meta.to_parquet(f\"win_shards/long_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "        for stride in STRIDES_SHORT:\n",
    "            spec = WindowSpec(length_tti=W_SHORT, stride_tti=stride)\n",
    "            for X, meta in iter_windows(g, rid, spec, role_cols):\n",
    "                sid = next(shard_counter)\n",
    "                np.savez_compressed(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}.npz\", X=X)\n",
    "                meta[\"source\"] = src_name; meta[\"stride\"] = stride; meta[\"kind\"] = \"short\"\n",
    "                meta.to_parquet(f\"win_shards/short_s{stride}_{src_name}_{rid}_{sid}_meta.parquet\", index=False)\n",
    "\n",
    "process_source(\"parquet/attack\", \"attack\")\n",
    "process_source(\"parquet/benign\", \"benign\")\n",
    "\n",
    "def load_all_meta(kind: str) -> pd.DataFrame:\n",
    "    metas = [pd.read_parquet(p) for p in glob(f\"win_shards/{kind}_*_meta.parquet\")]\n",
    "    return pd.concat(metas, ignore_index=True) if metas else pd.DataFrame()\n",
    "\n",
    "M_long  = load_all_meta(\"long\")\n",
    "M_short = load_all_meta(\"short\")\n",
    "\n",
    "def grouped_kfold(meta: pd.DataFrame, K: int = 5, seed: int = 42) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    if meta.empty:\n",
    "        return []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    run_ids = meta['run_id'].astype(str).unique().tolist()\n",
    "    rng.shuffle(run_ids)\n",
    "    folds = [set() for _ in range(K)]\n",
    "    for i, rid in enumerate(run_ids):\n",
    "        folds[i % K].add(rid)\n",
    "    splits = []\n",
    "    for i in range(K):\n",
    "        val_rids = folds[i]\n",
    "        train_idx = meta.index[~meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        val_idx   = meta.index[ meta['run_id'].astype(str).isin(val_rids)].to_numpy()\n",
    "        splits.append((train_idx, val_idx))\n",
    "    return splits\n",
    "\n",
    "splits_long  = grouped_kfold(M_long,  K=5, seed=2025)\n",
    "splits_short = grouped_kfold(M_short, K=5, seed=2025)\n",
    "\n",
    "if len(M_long):\n",
    "    print(f\"\\n[long] meta={M_long.shape}, groups={M_long['run_id'].nunique()}, folds={len(splits_long)}\")\n",
    "if len(M_short):\n",
    "    print(f\"[short] meta={M_short.shape}, groups={M_short['run_id'].nunique()}, folds={len(splits_short)}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete. Shards are in ./win_shards; use meta to assemble CV splits or stream for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb5f5-a951-47f5-99df-6b9d5a8601e9",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "This code builds a full PyTorch Geometric pipeline that turns PRB time-series windows into UE-level contention graphs and then into train/validation loaders for a PRB-GraphSAGE model. It starts by detecting UE RB columns (e.g., `UE1_rb`) in per-run parquet “PF ledger” data, then constructs node features for each UE in a window using PRB-based statistics such as mean/min/max/std share of PRBs, activity and zero fractions, and the longest zero (starvation) run. Edges are created between UEs that are co-scheduled at least once in the window, yielding an undirected contention graph. For each window in the preprocessed metadata (`win_shards/{kind}_*_meta.parquet`), `build_graph_from_window` slices the run dataframe, computes node features and edges, and assigns a graph label (binary or multiclass) from the window label, packaging everything into a `torch_geometric.data.Data` object plus a compact metadata dict. `build_prb_graphs` loads and filters all meta files, caches run-level parquet data and UE column detection by `(source, run_id)`, iterates over windows to construct graphs, and returns the list of graphs along with a metadata DataFrame. On top of this, `make_prb_graph_loaders` performs a grouped train/validation split by `run_id` (so windows from the same run don’t leak across splits), builds `GeoDataLoader` instances for training and validation, and returns them alongside split indices and run ID sets. Finally, the script instantiates these loaders for short windows, infers the node feature dimensionality with `detect_node_dim`, and prints it, providing everything needed to feed PRB-GraphSAGE with graph-structured inputs derived from scheduler PRB logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c496f167-721e-4197-9c1e-961bf4260af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using runs: ['040933', '171614', '000600', '001708']\n",
      "Train windows: 227881 Val windows: 72119\n",
      "Detected D_NODE: 7\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "import pyarrow.dataset as ds \n",
    "\n",
    "C_PRB = 106\n",
    "\n",
    "\n",
    "def _detect_ue_rb_columns_from_names(names: List[str]) -> List[str]:\n",
    "    cand = [c for c in names if re.fullmatch(r'UE\\d+_rb', c, flags=re.IGNORECASE)]\n",
    "    if not cand:\n",
    "        cand = [c for c in names if c.lower().endswith(\"_rb\")]\n",
    "\n",
    "    def _key(c):\n",
    "        m = re.search(r\"(\\d+)\", c)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "\n",
    "    return sorted(set(cand), key=_key)\n",
    "\n",
    "\n",
    "def _longest_zero_run(arr: np.ndarray) -> float:\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for v in arr:\n",
    "        if v == 0:\n",
    "            current += 1\n",
    "            longest = max(longest, current)\n",
    "        else:\n",
    "            current = 0\n",
    "    return float(longest)\n",
    "\n",
    "\n",
    "def build_node_features(prb_window: np.ndarray) -> np.ndarray:\n",
    "    # avoid extra copies when already float32\n",
    "    prb = np.asarray(prb_window, dtype=np.float32)  # view if possible\n",
    "    W, U = prb.shape\n",
    "    if U == 0:\n",
    "        return np.zeros((0, 1), dtype=np.float32)\n",
    "\n",
    "    share = prb / np.float32(C_PRB)\n",
    "\n",
    "    mean_share = share.mean(axis=0)\n",
    "    min_share = share.min(axis=0)\n",
    "    max_share = share.max(axis=0)\n",
    "    std_share = share.std(axis=0)\n",
    "\n",
    "    active = (prb > 0).astype(np.float32)\n",
    "    active_frac = active.mean(axis=0)\n",
    "    zero_frac = (prb == 0).mean(axis=0)\n",
    "    longest_zero = np.array([_longest_zero_run(prb[:, u]) for u in range(U)], dtype=np.float32)\n",
    "\n",
    "    return np.stack(\n",
    "        [mean_share, min_share, max_share, std_share, active_frac, zero_frac, longest_zero],\n",
    "        axis=1\n",
    "    ).astype(np.float32)\n",
    "\n",
    "\n",
    "def build_edges_fast(prb_window: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Vectorized replacement for the O(U^2*W) Python double-loop.\n",
    "    Builds undirected edges where two UEs are co-scheduled at least once.\n",
    "    \"\"\"\n",
    "    prb = np.asarray(prb_window)\n",
    "    W, U = prb.shape\n",
    "    if U <= 1:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    active = (prb > 0)\n",
    "    # counts of co-activity: cast so matmul counts instead of bool logic\n",
    "    a = active.astype(np.int16, copy=False)\n",
    "    co = a.T @ a  # [U, U] int16, co[i,j] = #TTIs both active\n",
    "    # keep upper triangle (no self loops), then mirror to make directed pairs\n",
    "    u, v = np.nonzero(np.triu(co, 1))\n",
    "    if u.size == 0:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    src = np.concatenate([u, v]).astype(np.int64, copy=False)\n",
    "    dst = np.concatenate([v, u]).astype(np.int64, copy=False)\n",
    "    return torch.from_numpy(np.stack([src, dst], axis=0))\n",
    "\n",
    "\n",
    "class PRBWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy, disk-backed dataset:\n",
    "      - stores only window metadata in RAM\n",
    "      - loads ONE run parquet at a time (LRU cache) and slices windows from it\n",
    "      - builds graph on demand in __getitem__\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        kind: str = \"short\",\n",
    "        meta_glob: Optional[str] = None,\n",
    "        parquet_root_attack: str = \"parquet/attack\",\n",
    "        parquet_root_benign: str = \"parquet/benign\",\n",
    "        label_mode: str = \"binary\",\n",
    "        restrict_sources: Optional[List[str]] = None,\n",
    "        cache_max_runs: int = 1,   # keep this small; each worker has its own cache\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kind = kind\n",
    "        self.parquet_root_attack = parquet_root_attack\n",
    "        self.parquet_root_benign = parquet_root_benign\n",
    "        self.label_mode = label_mode\n",
    "        self.restrict_sources = restrict_sources\n",
    "        self.cache_max_runs = int(cache_max_runs)\n",
    "\n",
    "        if meta_glob is None:\n",
    "            meta_glob = f\"win_shards/{kind}_*_meta.parquet\"\n",
    "        self.meta_files = sorted(glob.glob(meta_glob))\n",
    "        if not self.meta_files:\n",
    "            raise FileNotFoundError(f\"No meta parquet files found for pattern: {meta_glob}\")\n",
    "\n",
    "        # Load ONLY minimal meta columns\n",
    "        needed = [\"run_id\", \"source\", \"start_tti\", \"end_tti\", \"label\", \"kind\", \"stride\"]\n",
    "        parts = []\n",
    "        for mp in self.meta_files:\n",
    "            m = pd.read_parquet(mp)\n",
    "            # keep only columns that exist\n",
    "            keep = [c for c in needed if c in m.columns]\n",
    "            m = m[keep].copy()\n",
    "            if restrict_sources is not None and \"source\" in m.columns:\n",
    "                m = m[m[\"source\"].isin(restrict_sources)]\n",
    "            if len(m):\n",
    "                parts.append(m)\n",
    "\n",
    "        if not parts:\n",
    "            raise RuntimeError(\"No meta rows found after filtering.\")\n",
    "\n",
    "        meta = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "        # basic sanitation\n",
    "        meta[\"start_tti\"] = meta[\"start_tti\"].astype(np.int32)\n",
    "        meta[\"end_tti\"] = meta[\"end_tti\"].astype(np.int32)\n",
    "        if \"label\" not in meta.columns:\n",
    "            meta[\"label\"] = 0\n",
    "        meta[\"label\"] = meta[\"label\"].astype(np.int8)\n",
    "\n",
    "        # store run_id/source as categoricals to avoid repeating strings N times\n",
    "        run_cat = pd.Categorical(meta[\"run_id\"].astype(str))\n",
    "        src_cat = pd.Categorical(meta[\"source\"].astype(str))\n",
    "\n",
    "        self._run_categories = run_cat.categories.tolist()\n",
    "        self._src_categories = src_cat.categories.tolist()\n",
    "        self.run_code = run_cat.codes.astype(np.int32)\n",
    "        self.src_code = src_cat.codes.astype(np.int8)\n",
    "        self.start = meta[\"start_tti\"].to_numpy(np.int32)\n",
    "        self.end = meta[\"end_tti\"].to_numpy(np.int32)\n",
    "        self.label = meta[\"label\"].to_numpy(np.int8)\n",
    "\n",
    "        # tiny LRU cache: (src_str, run_id_str) -> (prb_run_uint8 [T,U], U)\n",
    "        self._run_cache: \"OrderedDict[Tuple[str,str], np.ndarray]\" = OrderedDict()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(self.start.shape[0])\n",
    "\n",
    "    def _run_path(self, source: str, run_id: str) -> str:\n",
    "        root = self.parquet_root_benign\n",
    "        if source == \"attack\":\n",
    "            root = self.parquet_root_attack\n",
    "        elif source == \"benign\":\n",
    "            root = self.parquet_root_benign\n",
    "\n",
    "        run_dir = os.path.join(root, f\"run_id={run_id}\")\n",
    "        if os.path.exists(run_dir):\n",
    "            return run_dir\n",
    "        if os.path.exists(run_dir + \".parquet\"):\n",
    "            return run_dir + \".parquet\"\n",
    "        raise FileNotFoundError(f\"Missing parquet for source={source}, run_id={run_id}: {run_dir}\")\n",
    "\n",
    "    def _load_run_prb_matrix(self, source: str, run_id: str) -> np.ndarray:\n",
    "        key = (source, run_id)\n",
    "        if key in self._run_cache:\n",
    "            self._run_cache.move_to_end(key)\n",
    "            return self._run_cache[key]\n",
    "\n",
    "        path = self._run_path(source, run_id)\n",
    "        dataset = ds.dataset(path, format=\"parquet\")\n",
    "        ue_cols = _detect_ue_rb_columns_from_names(dataset.schema.names)\n",
    "        if not ue_cols:\n",
    "            raise RuntimeError(f\"No UE*_rb columns found in run parquet: {path}\")\n",
    "\n",
    "        table = dataset.to_table(columns=ue_cols).combine_chunks()\n",
    "\n",
    "        T = table.num_rows\n",
    "        U = len(ue_cols)\n",
    "        prb = np.empty((T, U), dtype=np.uint8)\n",
    "        for k, c in enumerate(ue_cols):\n",
    "            col = table[c].to_numpy(zero_copy_only=False)\n",
    "            if np.issubdtype(col.dtype, np.floating):\n",
    "                col = np.nan_to_num(col, nan=0.0)\n",
    "            prb[:, k] = col.astype(np.uint8, copy=False)\n",
    "\n",
    "        self._run_cache[key] = prb\n",
    "        self._run_cache.move_to_end(key)\n",
    "        while len(self._run_cache) > self.cache_max_runs:\n",
    "            self._run_cache.popitem(last=False)  # evict LRU\n",
    "\n",
    "        return prb\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        run_id = self._run_categories[int(self.run_code[idx])]\n",
    "        source = self._src_categories[int(self.src_code[idx])]\n",
    "\n",
    "        prb_run = self._load_run_prb_matrix(source, run_id)\n",
    "        s = int(self.start[idx])\n",
    "        e = int(self.end[idx])\n",
    "        if e <= s:\n",
    "            # return safe empty graph (won't crash collate)\n",
    "            y = torch.tensor(0, dtype=torch.long)\n",
    "            return Data(\n",
    "                x=torch.zeros((0, 7), dtype=torch.float32),\n",
    "                edge_index=torch.empty((2, 0), dtype=torch.long),\n",
    "                y=y,\n",
    "            )\n",
    "\n",
    "        prb_window = prb_run[s:e, :]  # view into uint8 run matrix\n",
    "\n",
    "        x = torch.from_numpy(build_node_features(prb_window))\n",
    "        edge_index = build_edges_fast(prb_window)\n",
    "\n",
    "        label_raw = int(self.label[idx])\n",
    "        if self.label_mode == \"binary\":\n",
    "            y = torch.tensor(1 if label_raw == 2 else 0, dtype=torch.long)\n",
    "        elif self.label_mode == \"multiclass\":\n",
    "            y = torch.tensor(label_raw, dtype=torch.long)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label_mode: {self.label_mode}\")\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "def make_prb_graph_loaders_lazy(\n",
    "    kind: str = \"short\",\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 0,\n",
    "    seed: int = 2025,\n",
    "    val_ratio: float = 0.2,\n",
    "    label_mode: str = \"binary\",\n",
    "    restrict_sources: Optional[List[str]] = None,\n",
    "    cache_max_runs: int = 1,\n",
    "):\n",
    "    ds_all = PRBWindowDataset(\n",
    "        kind=kind,\n",
    "        label_mode=label_mode,\n",
    "        restrict_sources=restrict_sources,\n",
    "        cache_max_runs=cache_max_runs,\n",
    "    )\n",
    "\n",
    "    # group split by run_id code\n",
    "    run_codes = np.unique(ds_all.run_code)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(run_codes)\n",
    "    n_val = max(1, int(len(run_codes) * val_ratio))\n",
    "    val_codes = set(run_codes[:n_val].tolist())\n",
    "\n",
    "    idx_all = np.arange(len(ds_all), dtype=np.int64)\n",
    "    is_val = np.isin(ds_all.run_code, np.array(list(val_codes), dtype=np.int32))\n",
    "    val_idx = idx_all[is_val].tolist()\n",
    "    train_idx = idx_all[~is_val].tolist()\n",
    "\n",
    "    train_ds = Subset(ds_all, train_idx)\n",
    "    val_ds = Subset(ds_all, val_idx)\n",
    "\n",
    "    train_loader = GeoDataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = GeoDataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def make_baseline_loaders_from_ds(\n",
    "    ds_all,\n",
    "    batch_size=256,\n",
    "    seed=2025,\n",
    "    keep_attack_runs=2,\n",
    "    keep_benign_runs=2,\n",
    "    max_windows_total=300_000,\n",
    "    val_ratio=0.2,\n",
    "    num_workers=0,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    src_to_code = {s:i for i,s in enumerate(ds_all._src_categories)}\n",
    "    run_to_code = {r:i for i,r in enumerate(ds_all._run_categories)}\n",
    "\n",
    "    def first_n_runs(source, n):\n",
    "        sc = src_to_code[source]\n",
    "        rcs = np.unique(ds_all.run_code[ds_all.src_code == sc])\n",
    "        rids = sorted(ds_all._run_categories[int(rc)] for rc in rcs)\n",
    "        return rids[:n]\n",
    "\n",
    "    keep_run_ids = first_n_runs(\"attack\", keep_attack_runs) + first_n_runs(\"benign\", keep_benign_runs)\n",
    "    keep_codes = np.array([run_to_code[r] for r in keep_run_ids], dtype=np.int32)\n",
    "\n",
    "    idx = np.flatnonzero(np.isin(ds_all.run_code, keep_codes))\n",
    "\n",
    "    # downsample windows\n",
    "    if max_windows_total is not None and idx.size > max_windows_total:\n",
    "        idx = rng.choice(idx, size=max_windows_total, replace=False)\n",
    "\n",
    "    # grouped split by run_id (still)\n",
    "    idx_run_codes = ds_all.run_code[idx]\n",
    "    uniq_runs = np.unique(idx_run_codes)\n",
    "    rng.shuffle(uniq_runs)\n",
    "    n_val = max(1, int(len(uniq_runs) * val_ratio))\n",
    "    val_runs = set(uniq_runs[:n_val].tolist())\n",
    "    is_val = np.isin(idx_run_codes, np.fromiter(val_runs, dtype=np.int32))\n",
    "\n",
    "    train_idx = idx[~is_val]\n",
    "    val_idx   = idx[is_val]\n",
    "\n",
    "    train_ds = Subset(ds_all, train_idx)\n",
    "    val_ds   = Subset(ds_all, val_idx)\n",
    "\n",
    "    train_loader = GeoDataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "    val_loader   = GeoDataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, keep_run_ids, train_idx, val_idx\n",
    "    \n",
    "# train_loader, val_loader = make_prb_graph_loaders_lazy(\n",
    "#     kind=\"short\",\n",
    "#     batch_size=32,\n",
    "#     val_ratio=0.2,\n",
    "#     label_mode=\"binary\",\n",
    "#     restrict_sources=None,\n",
    "#     cache_max_runs=1,\n",
    "#     num_workers=0,\n",
    "# )\n",
    "\n",
    "ds_all = PRBWindowDataset(kind=\"short\", label_mode=\"binary\", cache_max_runs=256)\n",
    "train_loader, val_loader, keep_run_ids, train_idx, val_idx = make_baseline_loaders_from_ds(\n",
    "    ds_all,\n",
    "    batch_size=256,\n",
    "    max_windows_total=300_000,\n",
    "    keep_attack_runs=2,\n",
    "    keep_benign_runs=2,\n",
    ")\n",
    "print(\"Using runs:\", keep_run_ids)\n",
    "print(\"Train windows:\", len(train_idx), \"Val windows:\", len(val_idx))\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "D_NODE = sample_batch.x.size(-1)\n",
    "print(\"Detected D_NODE:\", D_NODE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a4651-9ce3-4d5c-9178-84f6701ec6cb",
   "metadata": {},
   "source": [
    "### Base Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1bacf-e010-4743-a55b-04e003af9c1f",
   "metadata": {},
   "source": [
    "#### PRB-GraphSAGE Architecture\n",
    "This code defines `PRBGraph`, a configurable PyTorch Geometric graph neural network for classifying UE-level contention graphs in a PRB starvation detection task. The model builds a GNN backbone with a user-specified number of layers (`num_layers`), hidden dimension, and convolution type (`SAGEConv`, `GCNConv`, `GraphConv`, or `GATv2Conv`), optionally followed by batch normalization and dropout after each layer. In the forward pass, node features `x` are iteratively updated using the chosen conv layers over edges `edge_index`, then aggregated into graph-level embeddings via a selectable global pooling strategy (`mean`, `max`, or concatenated `mean+max`). This pooled representation is passed through an MLP head whose depth and hidden size are also configurable, producing either a scalar logit per graph for binary classification (`num_classes == 1`) or a multi-dimensional logit vector for multi-class problems. The interface is designed to be hyperparameter-sweep friendly, with `edge_attr` included in the signature as a placeholder for future edge-aware enhancements, while the current implementation focuses on node features and graph-level pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd70b31c-f0ce-489f-92a2-f258c2632f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv,\n",
    "    GCNConv,\n",
    "    GraphConv,\n",
    "    GATv2Conv,\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    ")\n",
    "\n",
    "\n",
    "class PRBGraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    PRB-GraphSAGE: UE-level contention graph classifier for PRB starvation detection.\n",
    "\n",
    "    This model is designed to be easy to sweep over hyperparameters:\n",
    "      - in_dim:         node feature dimension\n",
    "      - hidden_dim:     hidden channel width\n",
    "      - num_layers:     number of GNN layers (>=1)\n",
    "      - conv_type:      GNN layer type: {\"sage\", \"gcn\", \"graph\", \"gat\"}\n",
    "      - aggr:           global pooling: {\"mean\", \"max\", \"mean+max\"}\n",
    "      - mlp_hidden_dim: hidden size of the final MLP head\n",
    "      - mlp_layers:     number of linear layers in the MLP head (>=1)\n",
    "      - dropout:        dropout probability applied after each conv & MLP layer\n",
    "      - num_classes:    1 for binary logit, >1 for multi-class logits\n",
    "\n",
    "    Forward signature:\n",
    "        logits = model(x, edge_index, batch, edge_attr=None)\n",
    "\n",
    "      - x:          [N, in_dim] node features\n",
    "      - edge_index: [2, E] COO edge indices\n",
    "      - batch:      [N] graph IDs for global pooling\n",
    "      - edge_attr:  [E, d_edge] (currently ignored; placeholder for future use)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        conv_type: str = \"sage\",\n",
    "        aggr: str = \"mean\",\n",
    "        mlp_hidden_dim: int = 64,\n",
    "        mlp_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 1,\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert num_layers >= 1, \"num_layers must be >= 1\"\n",
    "        assert mlp_layers >= 1, \"mlp_layers must be >= 1\"\n",
    "        assert conv_type in {\"sage\", \"gcn\", \"graph\", \"gat\"}, f\"Unknown conv_type: {conv_type}\"\n",
    "        assert aggr in {\"mean\", \"max\", \"mean+max\"}, f\"Unknown aggr: {aggr}\"\n",
    "        self.aggr = aggr\n",
    "        self.dropout = float(dropout)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.use_batchnorm = bool(use_batchnorm)\n",
    "\n",
    "        # ---- GNN backbone ----\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() if use_batchnorm else None\n",
    "\n",
    "        # input layer\n",
    "        self.convs.append(self._make_conv(conv_type, in_dim, hidden_dim))\n",
    "        if use_batchnorm:\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(self._make_conv(conv_type, hidden_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # ---- Readout (global pooling) ----\n",
    "        # mean+max concatenation doubles the channel dimension\n",
    "        readout_dim = hidden_dim if aggr in {\"mean\", \"max\"} else hidden_dim * 2\n",
    "\n",
    "        # ---- MLP classifier head ----\n",
    "        mlp_layers_list = []\n",
    "        in_mlp = readout_dim\n",
    "        for li in range(mlp_layers):\n",
    "            out_mlp = mlp_hidden_dim if li < mlp_layers - 1 else num_classes\n",
    "            mlp_layers_list.append(nn.Linear(in_mlp, out_mlp))\n",
    "            if li < mlp_layers - 1:\n",
    "                mlp_layers_list.append(nn.SiLU())\n",
    "                if dropout > 0:\n",
    "                    mlp_layers_list.append(nn.Dropout(dropout))\n",
    "            in_mlp = out_mlp\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_conv(conv_type: str, in_dim: int, out_dim: int):\n",
    "        \"\"\"Factory for different conv types to make hyperparameter sweeps easy.\"\"\"\n",
    "        if conv_type == \"sage\":\n",
    "            return SAGEConv(in_dim, out_dim)\n",
    "        if conv_type == \"gcn\":\n",
    "            return GCNConv(in_dim, out_dim)\n",
    "        if conv_type == \"graph\":\n",
    "            return GraphConv(in_dim, out_dim)\n",
    "        if conv_type == \"gat\":\n",
    "            return GATv2Conv(in_dim, out_dim, heads=1, concat=False)\n",
    "        raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        \"\"\"\n",
    "        x:          [N, in_dim]\n",
    "        edge_index: [2, E]\n",
    "        batch:      [N] graph IDs\n",
    "        edge_attr:  [E, d_edge] (ignored for now; reserved for future edge-aware layers)\n",
    "        \"\"\"\n",
    "        # ---- GNN backbone ----\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.silu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # ---- Global pooling ----\n",
    "        if self.aggr == \"mean\":\n",
    "            g = global_mean_pool(x, batch)\n",
    "        elif self.aggr == \"max\":\n",
    "            g = global_max_pool(x, batch)\n",
    "        else:  # \"mean+max\"\n",
    "            g_mean = global_mean_pool(x, batch)\n",
    "            g_max = global_max_pool(x, batch)\n",
    "            g = torch.cat([g_mean, g_max], dim=-1)\n",
    "\n",
    "        # ---- MLP head ----\n",
    "        logits = self.mlp(g)  # [num_graphs, num_classes]\n",
    "\n",
    "        # For binary classification, most training loops will want shape [num_graphs]\n",
    "        if self.num_classes == 1:\n",
    "            logits = logits.squeeze(-1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab57bc7-9215-4381-990f-435dcaf040ce",
   "metadata": {},
   "source": [
    "### Base Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f3d97-9ee8-48cc-8ab8-11d2d1b887ee",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "This code sets up initialization, configuration, and lightweight hyperparameter sweeping for a PRB-Graph GNN used in PRB starvation detection. It begins by seeding Python, NumPy, and PyTorch RNGs for reproducibility, choosing a CUDA or CPU device, and defining `D_NODE` as the node feature dimension (with a placeholder warning until real graph data is available). A `GNN_HYP` dictionary gathers all key hyperparameters in one place—training settings (batch size, epochs, learning rate and grid, weight decay), backbone architecture (hidden size, number of layers, dropout, conv type, pooling strategy, batchnorm), MLP head configuration, output dimension (binary vs multi-class), early-stopping criteria, and scheduler parameters (warmup and total steps). A helper `compute_pos_weight_from_labels` computes class weights for `BCEWithLogitsLoss` from imbalanced binary labels, with a default `pos_weight` of 1.0 until real labels are known. The `make_gnn_model` factory instantiates a `PRBGraphSAGE` model from a hyperparameter dict and moves it to the chosen device, while `count_params` reports trainable parameter count. Optimizer and scheduler builders create an AdamW optimizer and a LambdaLR scheduler implementing linear warmup followed by cosine decay over `total_steps`. The script then instantiates a default model, optimizer, scheduler, and a binary `BCEWithLogitsLoss` using the current `pos_weight`, printing summaries. Finally, `iter_gnn_configs` defines a simple grid-search generator over selected hyperparameters (learning rate, hidden size, number of layers, dropout, aggregation, and conv type), and the code demonstrates its use by instantiating and printing the first few candidate configurations along with their parameter counts, providing a ready-made loop for hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6977c6-e5cc-4ed9-b8ed-935262806065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GNN Hyperparameters: {\n",
      "  \"batch_size\": 32,\n",
      "  \"epochs\": 40,\n",
      "  \"lr\": 0.0003,\n",
      "  \"lr_grid\": [\n",
      "    0.0002,\n",
      "    0.0003,\n",
      "    0.0005\n",
      "  ],\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"hidden_dim\": 64,\n",
      "  \"num_layers\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"conv_type\": \"sage\",\n",
      "  \"aggr\": \"mean\",\n",
      "  \"use_batchnorm\": true,\n",
      "  \"hidden_dim_grid\": [\n",
      "    32,\n",
      "    64\n",
      "  ],\n",
      "  \"num_layers_grid\": [\n",
      "    1,\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"dropout_grid\": [\n",
      "    0.0,\n",
      "    0.1,\n",
      "    0.3\n",
      "  ],\n",
      "  \"aggr_grid\": [\n",
      "    \"mean\",\n",
      "    \"mean+max\"\n",
      "  ],\n",
      "  \"conv_type_grid\": [\n",
      "    \"sage\",\n",
      "    \"gcn\"\n",
      "  ],\n",
      "  \"mlp_hidden_dim\": 64,\n",
      "  \"mlp_layers\": 2,\n",
      "  \"num_classes\": 1,\n",
      "  \"early_stop\": {\n",
      "    \"monitor\": \"f1\",\n",
      "    \"mode\": \"max\",\n",
      "    \"patience\": 5,\n",
      "    \"min_delta\": 0.001\n",
      "  },\n",
      "  \"warmup_steps\": 200,\n",
      "  \"total_steps\": 10000\n",
      "}\n",
      "Initial pos_weight (binary): 1.0\n",
      "GNN params: 13,697\n",
      "GNN model, optimizer, scheduler, and loss are initialized.\n",
      "\n",
      "Initialization\n",
      "  Config 0: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=sage, params=2,721\n",
      "  Config 1: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean, conv_type=gcn, params=2,497\n",
      "  Config 2: lr=0.0002, hidden=32, layers=1, dropout=0.0, aggr=mean+max, conv_type=sage, params=4,769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ml4g/term_project/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==== CELL: GNN Model + Hyperparameter Initialization ====\n",
    "import os, math, json, random\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Reproducibility & device ----\n",
    "SEED = 2025\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_all(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- GNN Hyperparameters (with small grids for sweeps) ----\n",
    "GNN_HYP: Dict = {\n",
    "    # ---- Training-level hparams ----\n",
    "    \"batch_size\": 32, # Any changes must also be copied to the data loader              \n",
    "    \"epochs\": 40,                  \n",
    "    \"lr\": 3e-4,                    \n",
    "    \"lr_grid\": [2e-4, 3e-4, 5e-4], \n",
    "    \"weight_decay\": 1e-4,\n",
    "\n",
    "    # ---- Backbone architecture ----\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"conv_type\": \"sage\",           \n",
    "    \"aggr\": \"mean\",                \n",
    "    \"use_batchnorm\": True,\n",
    "\n",
    "    # Grids for a compact but meaningful sweep\n",
    "    \"hidden_dim_grid\": [32, 64],           \n",
    "    \"num_layers_grid\": [1, 2, 3],          \n",
    "    \"dropout_grid\": [0.0, 0.1, 0.3],       \n",
    "    \"aggr_grid\": [\"mean\", \"mean+max\"],     \n",
    "    \"conv_type_grid\": [\"sage\", \"gcn\"],     \n",
    "\n",
    "    # ---- MLP head ----\n",
    "    \"mlp_hidden_dim\": 64,\n",
    "    \"mlp_layers\": 2,\n",
    "\n",
    "    # ---- Output ----\n",
    "    # 1 = binary logit (BCEWithLogits); >1 = multi-class (CrossEntropy)\n",
    "    \"num_classes\": 1,\n",
    "\n",
    "    # ---- Early-stop config (used by your training loop) ----\n",
    "    \"early_stop\": {\n",
    "        \"monitor\": \"f1\",            \n",
    "        \"mode\": \"max\",              \n",
    "        \"patience\": 5,              \n",
    "        \"min_delta\": 1e-3,          \n",
    "    },\n",
    "\n",
    "    # ---- Scheduler placeholders (updated once loaders are known) ----\n",
    "    \"warmup_steps\": 200,\n",
    "    \"total_steps\": 10000,\n",
    "}\n",
    "\n",
    "print(\"GNN Hyperparameters:\", json.dumps(GNN_HYP, indent=2))\n",
    "\n",
    "\n",
    "# ---- Optional class weights for binary BCEWithLogitsLoss ----\n",
    "# You can adapt this to your graph meta if you materialize labels in a DataFrame.\n",
    "def compute_pos_weight_from_labels(labels: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pos_weight for BCEWithLogitsLoss from a binary label vector.\n",
    "    pos_weight = (#neg / #pos)\n",
    "    \"\"\"\n",
    "    labels = labels.astype(np.int64).ravel()\n",
    "    pos = (labels == 1).sum()\n",
    "    neg = (labels == 0).sum()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0, dtype=torch.float32)\n",
    "    return torch.tensor(float(neg) / float(pos), dtype=torch.float32)\n",
    "\n",
    "pos_weight = torch.tensor(1.0, dtype=torch.float32)\n",
    "print(\"Initial pos_weight (binary):\", float(pos_weight))\n",
    "\n",
    "\n",
    "# ---- PRB-GraphSAGE factory ----\n",
    "def make_gnn_model(hyp: Dict, in_dim: int) -> PRBGraphSAGE:\n",
    "    \"\"\"\n",
    "    Instantiate a PRBGraphSAGE model from a hyperparameter dict.\n",
    "    \"\"\"\n",
    "    model = PRBGraphSAGE(\n",
    "        in_dim=in_dim,\n",
    "        hidden_dim=hyp[\"hidden_dim\"],\n",
    "        num_layers=hyp[\"num_layers\"],\n",
    "        conv_type=hyp[\"conv_type\"],      \n",
    "        aggr=hyp[\"aggr\"],                \n",
    "        mlp_hidden_dim=hyp[\"mlp_hidden_dim\"],\n",
    "        mlp_layers=hyp[\"mlp_layers\"],\n",
    "        dropout=hyp[\"dropout\"],\n",
    "        num_classes=hyp[\"num_classes\"],\n",
    "        use_batchnorm=hyp[\"use_batchnorm\"],\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# ---- Optimizer & Scheduler helpers ----\n",
    "def make_gnn_optimizer(model: nn.Module, hyp: Dict) -> torch.optim.Optimizer:\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hyp[\"lr\"],\n",
    "        weight_decay=hyp[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "def make_scheduler(optimizer, warmup_steps: int, total_steps: int):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "# ---- Instantiate a default GNN model + optimizer/scheduler ----\n",
    "gnn_model = make_gnn_model(GNN_HYP, D_NODE)\n",
    "print(f\"GNN params: {count_params(gnn_model):,}\")\n",
    "\n",
    "gnn_optim = make_gnn_optimizer(gnn_model, GNN_HYP)\n",
    "gnn_scheduler = make_scheduler(gnn_optim, GNN_HYP[\"warmup_steps\"], GNN_HYP[\"total_steps\"])\n",
    "\n",
    "# Binary classification by default (y ∈ {0,1})\n",
    "gnn_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "\n",
    "print(\"GNN model, optimizer, scheduler, and loss are initialized.\")\n",
    "\n",
    "\n",
    "# ---- Hyperparameter sweep helper (config generator only) ----\n",
    "def iter_gnn_configs(base: Dict) -> Iterable[Dict]:\n",
    "    \"\"\"\n",
    "    Simple grid over a few key hyperparameters.\n",
    "    You can extend/prune this as needed.\n",
    "    \"\"\"\n",
    "    for lr in base[\"lr_grid\"]:\n",
    "        for hidden in base[\"hidden_dim_grid\"]:\n",
    "            for num_layers in base[\"num_layers_grid\"]:\n",
    "                for dropout in base[\"dropout_grid\"]:\n",
    "                    for aggr in base[\"aggr_grid\"]:\n",
    "                        for conv_type in base.get(\"conv_type_grid\", [base[\"conv_type\"]]):\n",
    "                            cfg = dict(base)  # shallow copy\n",
    "                            cfg[\"lr\"] = lr\n",
    "                            cfg[\"hidden_dim\"] = hidden\n",
    "                            cfg[\"num_layers\"] = num_layers\n",
    "                            cfg[\"dropout\"] = dropout\n",
    "                            cfg[\"aggr\"] = aggr\n",
    "                            cfg[\"conv_type\"] = conv_type\n",
    "                            yield cfg\n",
    "\n",
    "print(\"\\nInitialization\")\n",
    "for i, cfg in enumerate(iter_gnn_configs(GNN_HYP)):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    m = make_gnn_model(cfg, D_NODE)\n",
    "    print(\n",
    "        f\"  Config {i}: lr={cfg['lr']}, hidden={cfg['hidden_dim']}, \"\n",
    "        f\"layers={cfg['num_layers']}, dropout={cfg['dropout']}, \"\n",
    "        f\"aggr={cfg['aggr']}, conv_type={cfg['conv_type']}, \"\n",
    "        f\"params={count_params(m):,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdb14c-0f22-4a7f-97fc-c06bf43e041d",
   "metadata": {},
   "source": [
    "### Model Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbff31-a9a3-4872-b2a6-effa69065f94",
   "metadata": {},
   "source": [
    "#### PRB-Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4fd7f-efc1-4d51-b5bf-d124492e7d12",
   "metadata": {},
   "source": [
    "##### Helper Functions\n",
    "\n",
    "These helpers implement a compact training/evaluation loop for a binary graph classifier and standard performance metrics. compute_binary_metrics takes ground-truth binary labels and predicted probabilities (post-sigmoid), thresholds them into hard predictions, and computes accuracy, precision, recall, and F1 score with small numerical safeguards. run_epoch iterates over a PyTorch Geometric DataLoader for one epoch in either training or evaluation mode, moves each batch to the target device, runs the model forward on graph data (x, edge_index, batch), computes a scalar loss against flattened labels using a given criterion (e.g., BCEWithLogitsLoss), and in training mode performs backpropagation, optimizer steps, and optional scheduler updates. It accumulates loss weighted by batch size to return an average epoch loss, collects all sigmoid probabilities and labels across batches, and finally calls compute_binary_metrics to produce a metrics dictionary augmented with the averaged loss, or returns zeros if no samples were seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fb7f5c-cae6-4811-8b28-7f1e8f0795b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_binary_metrics(y_true: np.ndarray,\n",
    "                           y_prob: np.ndarray,\n",
    "                           threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    y_true: [N] in {0,1}\n",
    "    y_prob: [N] in [0,1]  (sigmoid outputs)\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64).ravel()\n",
    "    y_prob = y_prob.astype(np.float32).ravel()\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64)\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    eps = 1e-9\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    acc       = (tp + tn) / max(1, len(y_true))\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "    }\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion: nn.Module,\n",
    "              device: torch.device,\n",
    "              scheduler=None,\n",
    "              train: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Runs one epoch over loader.\n",
    "    Returns (avg_loss, metrics_dict).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_probs: List[float] = []\n",
    "    all_labels: List[int] = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        # y: [num_graphs] or [num_graphs, 1]\n",
    "        y = batch.y.view(-1).float()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(batch.x, batch.edge_index, batch.batch)  # [num_graphs] or [num_graphs,1]\n",
    "            logits = logits.view_as(y)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits.detach()).cpu().numpy()\n",
    "        labels = y.detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        return 0.0, {\"acc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    metrics = compute_binary_metrics(all_labels, all_probs, threshold=0.5)\n",
    "    metrics[\"loss\"] = float(avg_loss)\n",
    "    return avg_loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43f972-6356-4b02-984b-5012b5ad0c4c",
   "metadata": {},
   "source": [
    "##### Training Loop (with Early Stop)\n",
    "\n",
    "This code implements an early-stopping mechanism and a high-level training loop for a single GNN configuration. The `EarlyStopper` class tracks a chosen validation metric (e.g., F1) over epochs, deciding whether to stop training when the metric fails to improve by at least `min_delta` for a specified number of epochs (`patience`), and it keeps a deep copy of the best model state encountered. The `train_gnn_model` function wires everything together for one run: it constructs a GNN using `make_gnn_model`, sets up an AdamW optimizer and cosine–decay scheduler via `make_gnn_optimizer` and `make_scheduler`, and uses a shared loss (`gnn_criterion`, typically `BCEWithLogitsLoss` for binary classification). For each epoch, it calls `run_epoch` on both the training and validation loaders, capturing losses and metrics, logs progress with train/val loss and F1, appends snapshots into a `history` dict, and feeds validation metrics into `EarlyStopper` to determine if early stopping should trigger. When training ends—either by exhausting the epoch budget or by early stop—the best-performing weights (if any) are restored onto the model, and a result dictionary is returned containing the trained model, metric history, the best monitored metric value, its name, and the corresponding `state_dict` for downstream evaluation or checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63299bb3-02b7-45cb-b437-23dd9b996ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, monitor: str = \"f1\", mode: str = \"max\",\n",
    "                 patience: int = 5, min_delta: float = 1e-3):\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.best = -float(\"inf\")\n",
    "        elif mode == \"min\":\n",
    "            self.best = float(\"inf\")\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'max' or 'min'\")\n",
    "\n",
    "        self.num_bad = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metrics: Dict[str, float], model: nn.Module) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if we should stop (patience exhausted).\n",
    "        \"\"\"\n",
    "        current = metrics.get(self.monitor, None)\n",
    "        if current is None:\n",
    "            return False\n",
    "\n",
    "        improved = False\n",
    "        if self.mode == \"max\":\n",
    "            if current > self.best + self.min_delta:\n",
    "                improved = True\n",
    "        else:  # 'min'\n",
    "            if current < self.best - self.min_delta:\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            self.best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "\n",
    "        return self.num_bad >= self.patience\n",
    "\n",
    "\n",
    "def train_gnn_model(\n",
    "    hyp: Dict,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    in_dim: int,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    High-level training loop for a single GNN config.\n",
    "    Returns a dict with best metrics, history, and best_state_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    model = make_gnn_model(hyp, in_dim=in_dim)\n",
    "    optimizer = make_gnn_optimizer(model, hyp)\n",
    "    scheduler = make_scheduler(optimizer, hyp[\"warmup_steps\"], hyp[\"total_steps\"])\n",
    "\n",
    "    criterion = gnn_criterion \n",
    "\n",
    "    es_cfg = hyp[\"early_stop\"]\n",
    "    early_stopper = EarlyStopper(\n",
    "        monitor=es_cfg.get(\"monitor\", \"f1\"),\n",
    "        mode=es_cfg.get(\"mode\", \"max\"),\n",
    "        patience=es_cfg.get(\"patience\", 5),\n",
    "        min_delta=es_cfg.get(\"min_delta\", 1e-3),\n",
    "    )\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    for epoch in range(1, hyp[\"epochs\"] + 1):\n",
    "        train_loss, train_metrics = run_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, scheduler, train=True\n",
    "        )\n",
    "        val_loss, val_metrics = run_epoch(\n",
    "            model, val_loader, optimizer, criterion, device, scheduler=None, train=False\n",
    "        )\n",
    "\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:03d}] \"\n",
    "            f\"train_loss={train_metrics['loss']:.4f}, train_f1={train_metrics['f1']:.3f} | \"\n",
    "            f\"val_loss={val_metrics['loss']:.4f}, val_f1={val_metrics['f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "        stop = early_stopper.step(val_metrics, model)\n",
    "        if stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Best {early_stopper.monitor} = {early_stopper.best:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights if we got any improvement\n",
    "    if early_stopper.best_state is not None:\n",
    "        model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    result = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"best_metric\": early_stopper.best,\n",
    "        \"best_monitor\": early_stopper.monitor,\n",
    "        \"best_state_dict\": early_stopper.best_state,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fdd40-69fd-44b7-b30e-41a905f649d2",
   "metadata": {},
   "source": [
    "##### Initiate Training\n",
    "\n",
    "This code performs a simple grid search over GNN hyperparameters and tracks the best-performing configuration based on validation metrics. It iterates through each candidate config generated by `iter_gnn_configs(GNN_HYP)`, prints a short summary of that config (convolution type, hidden dimension, number of layers, dropout, aggregation, and learning rate), and trains a model with `train_gnn_model` on the given train/validation loaders and node feature dimension. The result `out` from each run includes the best monitored metric (e.g., validation F1) for that configuration; the loop keeps `best_overall` and `best_cfg` updated whenever a configuration surpasses the current best metric. After all configurations have been evaluated, it prints the best hyperparameter dictionary and the corresponding best validation score, effectively selecting the strongest model configuration discovered during the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6d6496-d571-4b5e-acde-c5c36ba86087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2898, train_f1=0.119 | val_loss=1.3020, val_f1=0.091\n",
      "[Epoch 002] train_loss=0.1074, train_f1=0.116 | val_loss=3.0173, val_f1=0.010\n",
      "[Epoch 003] train_loss=0.0887, train_f1=0.083 | val_loss=5.2647, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0839, train_f1=0.011 | val_loss=6.5238, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0823, train_f1=0.094 | val_loss=6.8929, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0815, train_f1=0.146 | val_loss=6.9423, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0911\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.3060, train_f1=0.128 | val_loss=1.6557, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.1038, train_f1=0.192 | val_loss=4.6488, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0918, train_f1=0.246 | val_loss=7.3088, val_f1=0.117\n",
      "[Epoch 004] train_loss=0.0870, train_f1=0.235 | val_loss=9.6750, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0843, train_f1=0.195 | val_loss=10.8791, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0827, train_f1=0.155 | val_loss=10.9561, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1181\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0002 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ml4g/term_project/.venv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] train_loss=0.2485, train_f1=0.044 | val_loss=1.5313, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.1006, train_f1=0.063 | val_loss=3.2868, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0880, train_f1=0.056 | val_loss=5.0401, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0833, train_f1=0.149 | val_loss=5.6700, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0814, train_f1=0.231 | val_loss=6.1568, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.253 | val_loss=5.7140, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0803, train_f1=0.284 | val_loss=5.7236, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0801, train_f1=0.291 | val_loss=5.5936, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0801, train_f1=0.284 | val_loss=5.3249, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2324, train_f1=0.146 | val_loss=3.7689, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0933, train_f1=0.270 | val_loss=8.8689, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0857, train_f1=0.189 | val_loss=11.4305, val_f1=0.109\n",
      "[Epoch 004] train_loss=0.0831, train_f1=0.081 | val_loss=11.8727, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0820, train_f1=0.120 | val_loss=11.9856, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0813, train_f1=0.170 | val_loss=11.2996, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0808, train_f1=0.196 | val_loss=10.9493, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0807, train_f1=0.196 | val_loss=10.1492, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0805, train_f1=0.217 | val_loss=10.2986, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0804, train_f1=0.230 | val_loss=10.1826, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0803, train_f1=0.251 | val_loss=10.1258, val_f1=0.000\n",
      "[Epoch 012] train_loss=0.0804, train_f1=0.227 | val_loss=9.9950, val_f1=0.000\n",
      "Early stopping triggered at epoch 12. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2910, train_f1=0.131 | val_loss=2.1190, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.1042, train_f1=0.208 | val_loss=5.4385, val_f1=0.118\n",
      "[Epoch 003] train_loss=0.0912, train_f1=0.253 | val_loss=8.9723, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0865, train_f1=0.266 | val_loss=10.3958, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0841, train_f1=0.265 | val_loss=11.4699, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0838, train_f1=0.260 | val_loss=11.3730, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0830, train_f1=0.262 | val_loss=10.7271, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1175\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2728, train_f1=0.081 | val_loss=3.2181, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0996, train_f1=0.253 | val_loss=7.3087, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0894, train_f1=0.270 | val_loss=10.2876, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0857, train_f1=0.240 | val_loss=12.5408, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0838, train_f1=0.243 | val_loss=12.7295, val_f1=0.109\n",
      "[Epoch 006] train_loss=0.0830, train_f1=0.267 | val_loss=12.3273, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0829, train_f1=0.275 | val_loss=12.6278, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1173\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2765, train_f1=0.032 | val_loss=1.2155, val_f1=0.091\n",
      "[Epoch 002] train_loss=0.1106, train_f1=0.185 | val_loss=3.5390, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0925, train_f1=0.225 | val_loss=7.1101, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0872, train_f1=0.237 | val_loss=9.2533, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0845, train_f1=0.244 | val_loss=10.6598, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0839, train_f1=0.267 | val_loss=11.0422, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0910\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2657, train_f1=0.122 | val_loss=1.4486, val_f1=0.091\n",
      "[Epoch 002] train_loss=0.1083, train_f1=0.193 | val_loss=4.5434, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0910, train_f1=0.174 | val_loss=8.1703, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0856, train_f1=0.168 | val_loss=10.4492, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0839, train_f1=0.200 | val_loss=11.3579, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0827, train_f1=0.254 | val_loss=12.1069, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0909\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2902, train_f1=0.080 | val_loss=2.0041, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.1103, train_f1=0.235 | val_loss=5.1574, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0925, train_f1=0.265 | val_loss=8.5750, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0877, train_f1=0.313 | val_loss=9.9665, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0868, train_f1=0.316 | val_loss=10.3028, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0855, train_f1=0.320 | val_loss=10.8245, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0853, train_f1=0.329 | val_loss=10.8497, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1173\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.3204, train_f1=0.128 | val_loss=2.2600, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.1099, train_f1=0.237 | val_loss=5.0104, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0945, train_f1=0.266 | val_loss=7.8346, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0895, train_f1=0.308 | val_loss=9.8187, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0872, train_f1=0.318 | val_loss=10.6077, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0861, train_f1=0.319 | val_loss=10.8102, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0095\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2846, train_f1=0.134 | val_loss=1.7506, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.1056, train_f1=0.208 | val_loss=5.3381, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0910, train_f1=0.247 | val_loss=7.8255, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0869, train_f1=0.277 | val_loss=9.3724, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0858, train_f1=0.293 | val_loss=10.0191, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0852, train_f1=0.310 | val_loss=10.7116, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2810, train_f1=0.135 | val_loss=1.9304, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.1058, train_f1=0.234 | val_loss=4.2943, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0922, train_f1=0.273 | val_loss=6.0035, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0878, train_f1=0.287 | val_loss=7.6220, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0864, train_f1=0.267 | val_loss=8.9628, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0851, train_f1=0.274 | val_loss=9.2817, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0096\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2517, train_f1=0.045 | val_loss=3.6704, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0946, train_f1=0.163 | val_loss=6.9322, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0860, train_f1=0.084 | val_loss=7.3098, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.205 | val_loss=5.8633, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0807, train_f1=0.299 | val_loss=5.3639, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0799, train_f1=0.358 | val_loss=4.3953, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0101\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2357, train_f1=0.125 | val_loss=3.5934, val_f1=0.001\n",
      "[Epoch 002] train_loss=0.0858, train_f1=0.048 | val_loss=4.1312, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0804, train_f1=0.223 | val_loss=3.6564, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0792, train_f1=0.312 | val_loss=3.7627, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0789, train_f1=0.312 | val_loss=3.6011, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0784, train_f1=0.301 | val_loss=3.4246, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0781, train_f1=0.328 | val_loss=3.4776, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0780, train_f1=0.318 | val_loss=3.3779, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2285, train_f1=0.176 | val_loss=5.1806, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0869, train_f1=0.073 | val_loss=6.6070, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0815, train_f1=0.231 | val_loss=4.8989, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.248 | val_loss=4.1541, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0793, train_f1=0.277 | val_loss=3.0185, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0789, train_f1=0.262 | val_loss=3.7162, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0785, train_f1=0.317 | val_loss=3.4354, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0784, train_f1=0.311 | val_loss=3.3469, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0782, train_f1=0.284 | val_loss=3.3279, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2245, train_f1=0.053 | val_loss=4.9618, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0827, train_f1=0.135 | val_loss=5.3915, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0802, train_f1=0.218 | val_loss=4.2622, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0792, train_f1=0.261 | val_loss=3.2134, val_f1=0.109\n",
      "[Epoch 005] train_loss=0.0787, train_f1=0.295 | val_loss=4.0515, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0784, train_f1=0.303 | val_loss=3.9240, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0783, train_f1=0.340 | val_loss=3.5184, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0780, train_f1=0.326 | val_loss=3.3872, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0781, train_f1=0.350 | val_loss=3.4477, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0779, train_f1=0.349 | val_loss=3.3651, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2774, train_f1=0.137 | val_loss=3.0243, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0972, train_f1=0.262 | val_loss=4.2316, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0876, train_f1=0.322 | val_loss=4.0107, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0846, train_f1=0.364 | val_loss=4.0679, val_f1=0.109\n",
      "[Epoch 005] train_loss=0.0835, train_f1=0.356 | val_loss=4.0307, val_f1=0.109\n",
      "[Epoch 006] train_loss=0.0826, train_f1=0.348 | val_loss=3.9812, val_f1=0.109\n",
      "[Epoch 007] train_loss=0.0822, train_f1=0.327 | val_loss=4.0119, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0816, train_f1=0.319 | val_loss=4.0690, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2605, train_f1=0.143 | val_loss=2.2721, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0983, train_f1=0.247 | val_loss=3.7051, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0892, train_f1=0.265 | val_loss=4.4319, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0851, train_f1=0.304 | val_loss=4.8084, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0836, train_f1=0.324 | val_loss=4.5653, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0825, train_f1=0.344 | val_loss=5.1620, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0822, train_f1=0.335 | val_loss=5.1112, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0818, train_f1=0.329 | val_loss=5.0348, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0816, train_f1=0.337 | val_loss=5.0079, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2231, train_f1=0.175 | val_loss=4.6831, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0896, train_f1=0.238 | val_loss=6.5278, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0841, train_f1=0.267 | val_loss=5.9500, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.305 | val_loss=5.3580, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0816, train_f1=0.323 | val_loss=5.0710, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0811, train_f1=0.326 | val_loss=4.7475, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0805, train_f1=0.320 | val_loss=4.8231, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0805, train_f1=0.313 | val_loss=4.7036, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0800, train_f1=0.298 | val_loss=4.6323, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0802, train_f1=0.334 | val_loss=4.6070, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2246, train_f1=0.129 | val_loss=3.8543, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0908, train_f1=0.279 | val_loss=5.7716, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0851, train_f1=0.292 | val_loss=5.8282, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0827, train_f1=0.280 | val_loss=4.8389, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0815, train_f1=0.262 | val_loss=4.5675, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0809, train_f1=0.285 | val_loss=4.0338, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0802, train_f1=0.302 | val_loss=4.0175, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0802, train_f1=0.304 | val_loss=4.0754, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0799, train_f1=0.312 | val_loss=4.0970, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0802, train_f1=0.313 | val_loss=4.0397, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0800, train_f1=0.315 | val_loss=4.0400, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2506, train_f1=0.149 | val_loss=4.1783, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0963, train_f1=0.282 | val_loss=6.1037, val_f1=0.109\n",
      "[Epoch 003] train_loss=0.0877, train_f1=0.358 | val_loss=6.3396, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0859, train_f1=0.376 | val_loss=5.4428, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0848, train_f1=0.370 | val_loss=5.4588, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0843, train_f1=0.371 | val_loss=5.3298, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0837, train_f1=0.372 | val_loss=5.1926, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0832, train_f1=0.367 | val_loss=5.4190, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2953, train_f1=0.125 | val_loss=1.9175, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.1075, train_f1=0.259 | val_loss=3.5860, val_f1=0.118\n",
      "[Epoch 003] train_loss=0.0947, train_f1=0.309 | val_loss=4.5763, val_f1=0.227\n",
      "[Epoch 004] train_loss=0.0898, train_f1=0.317 | val_loss=4.7318, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0878, train_f1=0.311 | val_loss=4.7546, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0862, train_f1=0.310 | val_loss=4.7400, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0853, train_f1=0.327 | val_loss=4.7381, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0851, train_f1=0.323 | val_loss=4.6217, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2265\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2501, train_f1=0.181 | val_loss=4.1175, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0943, train_f1=0.290 | val_loss=6.1077, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0872, train_f1=0.362 | val_loss=6.2729, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0851, train_f1=0.367 | val_loss=5.9059, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0839, train_f1=0.365 | val_loss=5.9896, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0831, train_f1=0.369 | val_loss=5.7270, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0825, train_f1=0.367 | val_loss=5.9342, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2687, train_f1=0.124 | val_loss=2.4665, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.1090, train_f1=0.222 | val_loss=4.2006, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0935, train_f1=0.264 | val_loss=4.6990, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0878, train_f1=0.323 | val_loss=4.9981, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0859, train_f1=0.340 | val_loss=4.9924, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0847, train_f1=0.327 | val_loss=5.2255, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0845, train_f1=0.347 | val_loss=5.1473, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0842, train_f1=0.337 | val_loss=5.0772, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2360, train_f1=0.192 | val_loss=5.0039, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0855, train_f1=0.300 | val_loss=4.9590, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0819, train_f1=0.357 | val_loss=4.8171, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.387 | val_loss=4.2853, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.395 | val_loss=3.5239, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0794, train_f1=0.399 | val_loss=3.8058, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2409, train_f1=0.204 | val_loss=3.6075, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0835, train_f1=0.294 | val_loss=3.6350, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0809, train_f1=0.367 | val_loss=3.8165, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0797, train_f1=0.377 | val_loss=3.7746, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0794, train_f1=0.384 | val_loss=3.2213, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0791, train_f1=0.378 | val_loss=3.4979, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0789, train_f1=0.379 | val_loss=3.2161, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0784, train_f1=0.392 | val_loss=3.3718, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0784, train_f1=0.376 | val_loss=3.5208, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0782, train_f1=0.378 | val_loss=3.3138, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2191, train_f1=0.173 | val_loss=5.0984, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0823, train_f1=0.292 | val_loss=3.6575, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0803, train_f1=0.364 | val_loss=3.6409, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0795, train_f1=0.379 | val_loss=3.4279, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0789, train_f1=0.378 | val_loss=3.8594, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0785, train_f1=0.388 | val_loss=3.4231, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0783, train_f1=0.378 | val_loss=3.4195, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1990, train_f1=0.255 | val_loss=4.1295, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0825, train_f1=0.355 | val_loss=4.2719, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.392 | val_loss=3.6419, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0795, train_f1=0.381 | val_loss=3.6938, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0793, train_f1=0.392 | val_loss=3.1259, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.383 | val_loss=3.3411, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0783, train_f1=0.377 | val_loss=3.0316, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0778, train_f1=0.380 | val_loss=3.2817, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0778, train_f1=0.365 | val_loss=3.2291, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2187\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2613, train_f1=0.126 | val_loss=3.3903, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0963, train_f1=0.256 | val_loss=5.3442, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0888, train_f1=0.236 | val_loss=5.0343, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0858, train_f1=0.268 | val_loss=3.8545, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0847, train_f1=0.272 | val_loss=3.4220, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0835, train_f1=0.282 | val_loss=3.7874, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0091\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2555, train_f1=0.098 | val_loss=3.7927, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0901, train_f1=0.297 | val_loss=4.4563, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0840, train_f1=0.369 | val_loss=3.8183, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.372 | val_loss=3.7172, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0817, train_f1=0.380 | val_loss=3.6130, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0811, train_f1=0.387 | val_loss=3.5946, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0807, train_f1=0.389 | val_loss=3.3602, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0804, train_f1=0.384 | val_loss=3.5637, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2138, train_f1=0.182 | val_loss=5.0595, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0916, train_f1=0.203 | val_loss=6.1073, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0868, train_f1=0.153 | val_loss=5.2062, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0846, train_f1=0.207 | val_loss=4.4275, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.229 | val_loss=3.6168, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0825, train_f1=0.256 | val_loss=3.7170, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0810, train_f1=0.304 | val_loss=3.3447, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0808, train_f1=0.310 | val_loss=3.3595, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0806, train_f1=0.311 | val_loss=3.3601, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0803, train_f1=0.331 | val_loss=3.3356, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0806, train_f1=0.329 | val_loss=3.3526, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2206, train_f1=0.222 | val_loss=3.3340, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0924, train_f1=0.241 | val_loss=4.2459, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0877, train_f1=0.275 | val_loss=4.6388, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0855, train_f1=0.271 | val_loss=4.9834, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0841, train_f1=0.284 | val_loss=4.6358, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0834, train_f1=0.295 | val_loss=5.0113, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0830, train_f1=0.304 | val_loss=5.0257, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0825, train_f1=0.296 | val_loss=4.6423, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0823, train_f1=0.299 | val_loss=4.9620, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0822, train_f1=0.303 | val_loss=4.8897, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.3093, train_f1=0.126 | val_loss=3.4884, val_f1=0.109\n",
      "[Epoch 002] train_loss=0.0989, train_f1=0.355 | val_loss=4.2434, val_f1=0.226\n",
      "[Epoch 003] train_loss=0.0901, train_f1=0.403 | val_loss=4.5232, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0870, train_f1=0.414 | val_loss=4.0877, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0851, train_f1=0.406 | val_loss=4.2736, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0844, train_f1=0.415 | val_loss=4.0901, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0841, train_f1=0.398 | val_loss=4.0024, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2259\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.3144, train_f1=0.152 | val_loss=2.1277, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.1046, train_f1=0.270 | val_loss=4.0334, val_f1=0.226\n",
      "[Epoch 003] train_loss=0.0924, train_f1=0.354 | val_loss=5.2169, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0890, train_f1=0.370 | val_loss=5.3235, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0875, train_f1=0.377 | val_loss=5.5299, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0857, train_f1=0.371 | val_loss=5.8196, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0859, train_f1=0.359 | val_loss=5.4803, val_f1=0.219\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2259\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2887, train_f1=0.118 | val_loss=3.3813, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0995, train_f1=0.345 | val_loss=4.2347, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0894, train_f1=0.395 | val_loss=4.9030, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0863, train_f1=0.395 | val_loss=4.5532, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0844, train_f1=0.392 | val_loss=4.4429, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0840, train_f1=0.377 | val_loss=4.4667, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0831, train_f1=0.365 | val_loss=4.4840, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2736, train_f1=0.133 | val_loss=1.5702, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.1064, train_f1=0.203 | val_loss=2.6864, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0964, train_f1=0.242 | val_loss=3.4132, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0926, train_f1=0.259 | val_loss=4.6347, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0900, train_f1=0.251 | val_loss=5.1678, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0884, train_f1=0.236 | val_loss=5.4505, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0877, train_f1=0.248 | val_loss=5.2987, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0871, train_f1=0.239 | val_loss=5.3261, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1220\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2510, train_f1=0.069 | val_loss=1.6484, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.1008, train_f1=0.208 | val_loss=3.7802, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0877, train_f1=0.185 | val_loss=5.7035, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.089 | val_loss=6.3037, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0812, train_f1=0.147 | val_loss=5.6049, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0805, train_f1=0.222 | val_loss=5.0459, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0801, train_f1=0.241 | val_loss=4.7503, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0799, train_f1=0.245 | val_loss=4.4888, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0796, train_f1=0.252 | val_loss=4.5177, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0795, train_f1=0.270 | val_loss=4.3810, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0796, train_f1=0.249 | val_loss=4.3026, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2475, train_f1=0.064 | val_loss=2.9168, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0959, train_f1=0.223 | val_loss=6.9775, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0856, train_f1=0.171 | val_loss=9.4910, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.132 | val_loss=10.0412, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0813, train_f1=0.210 | val_loss=9.9731, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0809, train_f1=0.227 | val_loss=9.7643, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0806, train_f1=0.256 | val_loss=9.6903, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0806, train_f1=0.256 | val_loss=9.3740, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0804, train_f1=0.293 | val_loss=8.9816, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2196, train_f1=0.177 | val_loss=4.7196, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0915, train_f1=0.259 | val_loss=10.6701, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0854, train_f1=0.236 | val_loss=13.4247, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0830, train_f1=0.201 | val_loss=13.1450, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0818, train_f1=0.216 | val_loss=11.2911, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0811, train_f1=0.265 | val_loss=10.8770, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0806, train_f1=0.257 | val_loss=10.5228, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0803, train_f1=0.280 | val_loss=9.8258, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0803, train_f1=0.287 | val_loss=9.2060, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2149, train_f1=0.152 | val_loss=1.9677, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0955, train_f1=0.258 | val_loss=6.3080, val_f1=0.118\n",
      "[Epoch 003] train_loss=0.0861, train_f1=0.200 | val_loss=9.5693, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0828, train_f1=0.186 | val_loss=9.2563, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0813, train_f1=0.216 | val_loss=8.3511, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0807, train_f1=0.294 | val_loss=7.4299, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0804, train_f1=0.291 | val_loss=6.9863, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0801, train_f1=0.305 | val_loss=6.4806, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0800, train_f1=0.304 | val_loss=6.0518, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2644, train_f1=0.100 | val_loss=3.1345, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0966, train_f1=0.253 | val_loss=7.1661, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0875, train_f1=0.234 | val_loss=9.5602, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0843, train_f1=0.269 | val_loss=9.9616, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.298 | val_loss=9.8714, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0832, train_f1=0.290 | val_loss=9.8473, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0094\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2684, train_f1=0.134 | val_loss=2.0738, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.1038, train_f1=0.231 | val_loss=6.5919, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0887, train_f1=0.206 | val_loss=9.0188, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0845, train_f1=0.137 | val_loss=10.3698, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0830, train_f1=0.152 | val_loss=10.4912, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0823, train_f1=0.208 | val_loss=10.6264, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0820, train_f1=0.229 | val_loss=11.1569, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1174\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2377, train_f1=0.143 | val_loss=2.2023, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0949, train_f1=0.194 | val_loss=3.6412, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0862, train_f1=0.190 | val_loss=4.7257, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0842, train_f1=0.203 | val_loss=5.4135, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.232 | val_loss=5.8025, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0829, train_f1=0.247 | val_loss=6.0680, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0827, train_f1=0.255 | val_loss=6.2260, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1173\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2168, train_f1=0.131 | val_loss=4.6350, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0943, train_f1=0.281 | val_loss=7.7829, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0862, train_f1=0.255 | val_loss=9.6894, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0837, train_f1=0.226 | val_loss=10.0679, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0829, train_f1=0.248 | val_loss=9.5917, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0822, train_f1=0.275 | val_loss=9.6272, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0820, train_f1=0.284 | val_loss=9.1701, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0820, train_f1=0.282 | val_loss=9.4359, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0818, train_f1=0.278 | val_loss=9.5392, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0819, train_f1=0.287 | val_loss=9.6639, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2501, train_f1=0.129 | val_loss=2.9087, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.1013, train_f1=0.224 | val_loss=6.0774, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0896, train_f1=0.230 | val_loss=8.1658, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0864, train_f1=0.229 | val_loss=9.4369, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0850, train_f1=0.234 | val_loss=10.2322, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0844, train_f1=0.253 | val_loss=10.4390, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2552, train_f1=0.107 | val_loss=4.0016, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0983, train_f1=0.290 | val_loss=8.1197, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0892, train_f1=0.315 | val_loss=10.2081, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0862, train_f1=0.324 | val_loss=11.7515, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0849, train_f1=0.326 | val_loss=11.8339, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0844, train_f1=0.314 | val_loss=11.2004, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0837, train_f1=0.322 | val_loss=11.6946, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0835, train_f1=0.320 | val_loss=11.5449, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0837, train_f1=0.312 | val_loss=11.8326, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0833, train_f1=0.308 | val_loss=11.7232, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0832, train_f1=0.293 | val_loss=11.6616, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1221\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2115, train_f1=0.163 | val_loss=4.9190, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0914, train_f1=0.228 | val_loss=8.7555, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0860, train_f1=0.232 | val_loss=10.7832, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0841, train_f1=0.264 | val_loss=11.3415, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0839, train_f1=0.279 | val_loss=11.8653, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0834, train_f1=0.271 | val_loss=11.7454, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0831, train_f1=0.280 | val_loss=11.2019, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0831, train_f1=0.255 | val_loss=11.7478, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0826, train_f1=0.277 | val_loss=11.4096, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0830, train_f1=0.276 | val_loss=11.2776, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0832, train_f1=0.272 | val_loss=12.0361, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2201, train_f1=0.181 | val_loss=4.8323, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0911, train_f1=0.244 | val_loss=8.7081, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0860, train_f1=0.258 | val_loss=9.6185, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0850, train_f1=0.259 | val_loss=10.0466, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0842, train_f1=0.268 | val_loss=10.8840, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0839, train_f1=0.274 | val_loss=9.9741, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1173\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2168, train_f1=0.061 | val_loss=4.0907, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0865, train_f1=0.093 | val_loss=6.3450, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0817, train_f1=0.278 | val_loss=5.8814, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.292 | val_loss=5.2726, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0794, train_f1=0.332 | val_loss=4.8603, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0791, train_f1=0.337 | val_loss=4.5186, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2047, train_f1=0.074 | val_loss=4.9242, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0838, train_f1=0.121 | val_loss=6.1693, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0808, train_f1=0.237 | val_loss=4.8330, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0796, train_f1=0.300 | val_loss=4.2727, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0789, train_f1=0.337 | val_loss=3.6323, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0784, train_f1=0.321 | val_loss=3.2340, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1765, train_f1=0.100 | val_loss=4.8370, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0834, train_f1=0.227 | val_loss=4.1181, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0812, train_f1=0.317 | val_loss=4.2205, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0802, train_f1=0.328 | val_loss=3.7285, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0797, train_f1=0.353 | val_loss=3.6386, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0791, train_f1=0.352 | val_loss=3.5970, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1827, train_f1=0.071 | val_loss=5.0835, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0832, train_f1=0.202 | val_loss=5.8763, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.312 | val_loss=5.2251, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0796, train_f1=0.337 | val_loss=4.1473, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0789, train_f1=0.347 | val_loss=4.1211, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.357 | val_loss=3.7459, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0780, train_f1=0.358 | val_loss=3.5759, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0780, train_f1=0.357 | val_loss=3.5167, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2264, train_f1=0.145 | val_loss=3.9259, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0906, train_f1=0.270 | val_loss=5.8334, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0849, train_f1=0.312 | val_loss=6.2865, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0827, train_f1=0.334 | val_loss=5.0478, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0817, train_f1=0.315 | val_loss=4.8548, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.327 | val_loss=4.0905, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0804, train_f1=0.331 | val_loss=3.9452, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0799, train_f1=0.324 | val_loss=3.9049, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0798, train_f1=0.335 | val_loss=3.8160, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2156, train_f1=0.141 | val_loss=3.8560, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0924, train_f1=0.283 | val_loss=5.0999, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0868, train_f1=0.281 | val_loss=4.3526, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0834, train_f1=0.299 | val_loss=4.2601, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0821, train_f1=0.334 | val_loss=4.2194, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0810, train_f1=0.347 | val_loss=4.0399, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0806, train_f1=0.349 | val_loss=4.0260, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0802, train_f1=0.361 | val_loss=3.9862, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0805, train_f1=0.349 | val_loss=3.9024, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0803, train_f1=0.348 | val_loss=3.7488, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0803, train_f1=0.343 | val_loss=3.8469, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1940, train_f1=0.085 | val_loss=4.0330, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0876, train_f1=0.216 | val_loss=6.0119, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0833, train_f1=0.275 | val_loss=7.0438, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0821, train_f1=0.283 | val_loss=5.8463, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0810, train_f1=0.303 | val_loss=5.1623, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.300 | val_loss=4.9325, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0801, train_f1=0.306 | val_loss=4.7289, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0799, train_f1=0.315 | val_loss=4.8646, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0801, train_f1=0.276 | val_loss=4.7465, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2133, train_f1=0.166 | val_loss=3.6221, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0912, train_f1=0.308 | val_loss=4.8736, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0869, train_f1=0.292 | val_loss=5.3860, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0844, train_f1=0.309 | val_loss=5.3028, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0834, train_f1=0.317 | val_loss=5.1253, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0828, train_f1=0.324 | val_loss=4.8205, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2578, train_f1=0.163 | val_loss=3.1110, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0938, train_f1=0.264 | val_loss=4.0790, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0858, train_f1=0.319 | val_loss=4.3774, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0839, train_f1=0.335 | val_loss=4.1987, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0828, train_f1=0.342 | val_loss=4.3454, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0819, train_f1=0.344 | val_loss=4.3711, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0821, train_f1=0.349 | val_loss=4.2521, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0819, train_f1=0.336 | val_loss=4.0427, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0819, train_f1=0.343 | val_loss=4.1540, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2696, train_f1=0.137 | val_loss=3.5620, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0966, train_f1=0.296 | val_loss=5.8442, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0881, train_f1=0.322 | val_loss=6.0985, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0850, train_f1=0.340 | val_loss=5.3817, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0839, train_f1=0.333 | val_loss=5.5660, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0834, train_f1=0.336 | val_loss=4.9648, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0827, train_f1=0.341 | val_loss=4.9202, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0828, train_f1=0.335 | val_loss=4.7812, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2211, train_f1=0.193 | val_loss=3.4165, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0956, train_f1=0.255 | val_loss=6.0884, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0899, train_f1=0.214 | val_loss=7.2869, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0870, train_f1=0.230 | val_loss=8.0738, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0859, train_f1=0.230 | val_loss=7.6400, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0849, train_f1=0.244 | val_loss=6.7591, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0840, train_f1=0.263 | val_loss=6.3843, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0836, train_f1=0.268 | val_loss=6.3001, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0833, train_f1=0.273 | val_loss=6.0487, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0828, train_f1=0.283 | val_loss=6.0557, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0831, train_f1=0.269 | val_loss=6.3828, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2295, train_f1=0.150 | val_loss=2.9049, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0944, train_f1=0.198 | val_loss=5.2308, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0873, train_f1=0.210 | val_loss=6.4544, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0846, train_f1=0.251 | val_loss=6.6427, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.277 | val_loss=6.3257, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0827, train_f1=0.295 | val_loss=6.6180, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0825, train_f1=0.303 | val_loss=5.8977, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0822, train_f1=0.308 | val_loss=5.8985, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0823, train_f1=0.306 | val_loss=5.8080, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1886, train_f1=0.179 | val_loss=3.5662, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0812, train_f1=0.325 | val_loss=3.3760, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0796, train_f1=0.367 | val_loss=3.6743, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0785, train_f1=0.378 | val_loss=2.6852, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0779, train_f1=0.382 | val_loss=2.9670, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0776, train_f1=0.377 | val_loss=3.2980, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2128, train_f1=0.232 | val_loss=3.5885, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0814, train_f1=0.365 | val_loss=3.3506, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0796, train_f1=0.381 | val_loss=3.9498, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0788, train_f1=0.392 | val_loss=3.9416, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0784, train_f1=0.383 | val_loss=3.1371, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0779, train_f1=0.388 | val_loss=3.4063, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0776, train_f1=0.380 | val_loss=3.4815, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0772, train_f1=0.379 | val_loss=3.4349, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1828, train_f1=0.165 | val_loss=3.4366, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0810, train_f1=0.365 | val_loss=3.3842, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0796, train_f1=0.367 | val_loss=4.0747, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0785, train_f1=0.383 | val_loss=3.2841, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0778, train_f1=0.399 | val_loss=3.4682, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0774, train_f1=0.397 | val_loss=2.8990, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0769, train_f1=0.378 | val_loss=3.0778, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.1722, train_f1=0.226 | val_loss=3.5601, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0804, train_f1=0.358 | val_loss=3.0374, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0791, train_f1=0.379 | val_loss=3.0233, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0781, train_f1=0.379 | val_loss=3.1467, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0777, train_f1=0.389 | val_loss=2.7644, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0774, train_f1=0.377 | val_loss=3.0915, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0772, train_f1=0.370 | val_loss=3.1068, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0767, train_f1=0.378 | val_loss=3.1051, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2084, train_f1=0.226 | val_loss=5.1692, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0886, train_f1=0.230 | val_loss=4.8738, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0848, train_f1=0.288 | val_loss=4.0397, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0818, train_f1=0.339 | val_loss=3.5722, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.378 | val_loss=3.2019, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.392 | val_loss=2.9636, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0793, train_f1=0.379 | val_loss=3.0364, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2203, train_f1=0.200 | val_loss=4.7977, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0881, train_f1=0.281 | val_loss=4.5707, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.292 | val_loss=4.2483, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0826, train_f1=0.300 | val_loss=4.0253, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0812, train_f1=0.338 | val_loss=3.5705, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.350 | val_loss=3.5018, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0798, train_f1=0.332 | val_loss=3.6692, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0795, train_f1=0.290 | val_loss=3.6607, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0792, train_f1=0.310 | val_loss=3.6842, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2072, train_f1=0.208 | val_loss=4.6795, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0853, train_f1=0.269 | val_loss=3.9883, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0812, train_f1=0.290 | val_loss=3.5429, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0798, train_f1=0.323 | val_loss=3.7162, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0793, train_f1=0.336 | val_loss=3.1724, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0791, train_f1=0.322 | val_loss=3.6268, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0787, train_f1=0.334 | val_loss=3.0935, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0785, train_f1=0.312 | val_loss=3.2787, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0784, train_f1=0.318 | val_loss=3.1695, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2135, train_f1=0.195 | val_loss=4.3661, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0880, train_f1=0.321 | val_loss=4.6410, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0841, train_f1=0.329 | val_loss=4.6064, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0817, train_f1=0.348 | val_loss=4.3389, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0807, train_f1=0.328 | val_loss=4.3544, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0797, train_f1=0.320 | val_loss=3.7919, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0794, train_f1=0.304 | val_loss=3.8553, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0790, train_f1=0.300 | val_loss=3.5939, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0787, train_f1=0.283 | val_loss=3.6537, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2617, train_f1=0.150 | val_loss=3.6383, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0913, train_f1=0.339 | val_loss=4.4032, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0851, train_f1=0.384 | val_loss=3.9497, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0833, train_f1=0.384 | val_loss=3.6704, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0825, train_f1=0.384 | val_loss=3.3797, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0818, train_f1=0.370 | val_loss=3.5276, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0814, train_f1=0.364 | val_loss=3.4778, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2477, train_f1=0.158 | val_loss=3.7799, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0927, train_f1=0.322 | val_loss=5.0703, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0872, train_f1=0.361 | val_loss=4.7812, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0849, train_f1=0.379 | val_loss=4.5913, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0831, train_f1=0.393 | val_loss=4.1672, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0822, train_f1=0.384 | val_loss=3.8655, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0819, train_f1=0.397 | val_loss=3.8586, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0817, train_f1=0.377 | val_loss=3.6092, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2291, train_f1=0.169 | val_loss=2.5615, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0940, train_f1=0.158 | val_loss=4.7850, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0870, train_f1=0.201 | val_loss=4.8055, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0834, train_f1=0.254 | val_loss=4.2826, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0825, train_f1=0.293 | val_loss=3.8247, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0814, train_f1=0.292 | val_loss=3.7933, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0809, train_f1=0.299 | val_loss=3.8175, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0807, train_f1=0.280 | val_loss=3.7433, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0805, train_f1=0.303 | val_loss=3.4352, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0002 ===\n",
      "[Epoch 001] train_loss=0.2206, train_f1=0.178 | val_loss=4.3002, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0903, train_f1=0.342 | val_loss=4.7043, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0851, train_f1=0.377 | val_loss=4.4082, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.374 | val_loss=4.3512, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0822, train_f1=0.371 | val_loss=3.8822, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0818, train_f1=0.361 | val_loss=3.8882, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0812, train_f1=0.375 | val_loss=3.8278, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2475, train_f1=0.182 | val_loss=3.1636, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0940, train_f1=0.278 | val_loss=8.3798, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0854, train_f1=0.174 | val_loss=12.0013, val_f1=0.109\n",
      "[Epoch 004] train_loss=0.0828, train_f1=0.095 | val_loss=12.3305, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0817, train_f1=0.149 | val_loss=12.1625, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.184 | val_loss=11.6011, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1179\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2432, train_f1=0.054 | val_loss=2.1800, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0944, train_f1=0.184 | val_loss=4.7252, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0841, train_f1=0.087 | val_loss=6.2259, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0817, train_f1=0.148 | val_loss=6.1670, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0808, train_f1=0.234 | val_loss=5.8275, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.255 | val_loss=5.3534, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0801, train_f1=0.290 | val_loss=5.1891, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0798, train_f1=0.272 | val_loss=5.0243, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0798, train_f1=0.273 | val_loss=4.9391, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0796, train_f1=0.256 | val_loss=4.9268, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2340, train_f1=0.046 | val_loss=2.5002, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0931, train_f1=0.181 | val_loss=7.6496, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0840, train_f1=0.171 | val_loss=9.5011, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0815, train_f1=0.243 | val_loss=8.9811, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0808, train_f1=0.289 | val_loss=8.5591, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.302 | val_loss=7.9825, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0800, train_f1=0.294 | val_loss=7.2465, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0797, train_f1=0.298 | val_loss=6.9568, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0795, train_f1=0.310 | val_loss=6.7867, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0794, train_f1=0.297 | val_loss=6.9310, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2146, train_f1=0.178 | val_loss=5.9308, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0876, train_f1=0.206 | val_loss=11.3705, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0832, train_f1=0.214 | val_loss=12.8417, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0819, train_f1=0.273 | val_loss=12.5877, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0814, train_f1=0.311 | val_loss=11.3355, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0810, train_f1=0.294 | val_loss=10.8323, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0809, train_f1=0.283 | val_loss=9.3851, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0804, train_f1=0.279 | val_loss=8.8628, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2476, train_f1=0.151 | val_loss=3.9178, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0946, train_f1=0.228 | val_loss=8.5621, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0867, train_f1=0.296 | val_loss=10.7191, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0838, train_f1=0.286 | val_loss=12.2372, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0830, train_f1=0.301 | val_loss=11.5416, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0825, train_f1=0.304 | val_loss=11.4467, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2788, train_f1=0.102 | val_loss=1.6369, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.1011, train_f1=0.229 | val_loss=5.5666, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0877, train_f1=0.263 | val_loss=8.4450, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0835, train_f1=0.276 | val_loss=9.9580, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0828, train_f1=0.309 | val_loss=9.5326, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0821, train_f1=0.320 | val_loss=8.5834, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0819, train_f1=0.316 | val_loss=9.1519, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0817, train_f1=0.321 | val_loss=8.9540, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0814, train_f1=0.329 | val_loss=8.6739, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0814, train_f1=0.315 | val_loss=8.6827, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2238, train_f1=0.167 | val_loss=8.5899, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0918, train_f1=0.289 | val_loss=16.0031, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0864, train_f1=0.257 | val_loss=17.2843, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0841, train_f1=0.232 | val_loss=14.9068, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0827, train_f1=0.249 | val_loss=13.0208, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0821, train_f1=0.269 | val_loss=11.3975, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1173\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2202, train_f1=0.181 | val_loss=5.6968, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0891, train_f1=0.237 | val_loss=11.5938, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.281 | val_loss=12.9773, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0834, train_f1=0.312 | val_loss=12.2768, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0825, train_f1=0.317 | val_loss=11.3949, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0823, train_f1=0.300 | val_loss=11.2466, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0823, train_f1=0.307 | val_loss=10.9976, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0818, train_f1=0.273 | val_loss=10.6281, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0816, train_f1=0.298 | val_loss=10.7943, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2510, train_f1=0.107 | val_loss=3.5755, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0956, train_f1=0.293 | val_loss=8.6542, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0882, train_f1=0.326 | val_loss=10.9334, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0859, train_f1=0.347 | val_loss=12.9409, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0851, train_f1=0.320 | val_loss=13.0831, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0846, train_f1=0.320 | val_loss=12.3975, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0840, train_f1=0.305 | val_loss=13.1177, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0846, train_f1=0.309 | val_loss=12.3996, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2446, train_f1=0.123 | val_loss=5.5085, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0943, train_f1=0.301 | val_loss=10.6520, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0872, train_f1=0.338 | val_loss=12.6546, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0852, train_f1=0.346 | val_loss=12.9013, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0846, train_f1=0.340 | val_loss=12.3620, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0842, train_f1=0.341 | val_loss=12.9468, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0840, train_f1=0.337 | val_loss=12.2885, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0839, train_f1=0.315 | val_loss=12.0268, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0837, train_f1=0.325 | val_loss=11.9998, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2111, train_f1=0.153 | val_loss=5.4451, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0907, train_f1=0.293 | val_loss=10.1716, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0861, train_f1=0.319 | val_loss=11.3149, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0846, train_f1=0.330 | val_loss=12.1186, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0836, train_f1=0.311 | val_loss=11.7747, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0833, train_f1=0.304 | val_loss=11.4590, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1173\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2254, train_f1=0.175 | val_loss=4.1401, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0928, train_f1=0.269 | val_loss=7.6429, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0865, train_f1=0.311 | val_loss=9.0917, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0850, train_f1=0.292 | val_loss=9.6862, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0843, train_f1=0.297 | val_loss=9.2636, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0837, train_f1=0.285 | val_loss=10.2550, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0836, train_f1=0.265 | val_loss=10.5413, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0835, train_f1=0.276 | val_loss=10.6835, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0835, train_f1=0.266 | val_loss=10.7347, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2017, train_f1=0.053 | val_loss=4.0123, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0828, train_f1=0.145 | val_loss=5.0362, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0803, train_f1=0.282 | val_loss=3.7825, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0794, train_f1=0.301 | val_loss=4.1154, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0787, train_f1=0.327 | val_loss=3.6954, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.319 | val_loss=4.2341, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0780, train_f1=0.318 | val_loss=3.7930, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2268, train_f1=0.211 | val_loss=3.8105, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0855, train_f1=0.213 | val_loss=4.8080, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0812, train_f1=0.310 | val_loss=3.3905, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.344 | val_loss=3.5976, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0793, train_f1=0.359 | val_loss=3.3899, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0791, train_f1=0.369 | val_loss=3.2390, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0787, train_f1=0.364 | val_loss=3.3820, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0787, train_f1=0.365 | val_loss=3.3504, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0784, train_f1=0.364 | val_loss=3.3163, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0781, train_f1=0.376 | val_loss=3.2654, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1772, train_f1=0.060 | val_loss=5.1622, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0829, train_f1=0.231 | val_loss=7.7351, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0801, train_f1=0.328 | val_loss=6.0651, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0790, train_f1=0.355 | val_loss=3.9878, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0781, train_f1=0.369 | val_loss=3.8585, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0778, train_f1=0.374 | val_loss=3.2944, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2044, train_f1=0.155 | val_loss=4.5560, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0852, train_f1=0.274 | val_loss=5.9799, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0820, train_f1=0.364 | val_loss=5.3912, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0809, train_f1=0.381 | val_loss=4.1099, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0798, train_f1=0.391 | val_loss=3.7947, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0799, train_f1=0.397 | val_loss=3.0140, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.386 | val_loss=3.5097, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0789, train_f1=0.390 | val_loss=3.5546, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0790, train_f1=0.379 | val_loss=3.3969, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0784, train_f1=0.373 | val_loss=3.3809, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0785, train_f1=0.399 | val_loss=3.3041, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2128, train_f1=0.204 | val_loss=3.3999, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0862, train_f1=0.263 | val_loss=3.8353, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0829, train_f1=0.324 | val_loss=3.6601, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0815, train_f1=0.350 | val_loss=3.7117, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0806, train_f1=0.356 | val_loss=3.6469, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.359 | val_loss=3.6453, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0801, train_f1=0.363 | val_loss=3.5148, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0798, train_f1=0.350 | val_loss=3.5805, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0798, train_f1=0.357 | val_loss=3.3918, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2228, train_f1=0.126 | val_loss=5.5024, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0882, train_f1=0.213 | val_loss=6.5172, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0828, train_f1=0.317 | val_loss=6.1886, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0816, train_f1=0.334 | val_loss=5.4318, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0810, train_f1=0.346 | val_loss=4.7545, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0803, train_f1=0.351 | val_loss=4.5292, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0800, train_f1=0.357 | val_loss=4.1744, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2044, train_f1=0.165 | val_loss=4.0606, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0861, train_f1=0.261 | val_loss=4.7458, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0819, train_f1=0.309 | val_loss=4.7578, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0804, train_f1=0.326 | val_loss=4.6728, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0801, train_f1=0.344 | val_loss=4.2067, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.330 | val_loss=3.8537, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0794, train_f1=0.368 | val_loss=3.9490, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0793, train_f1=0.353 | val_loss=4.0536, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0793, train_f1=0.367 | val_loss=4.0126, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2030, train_f1=0.206 | val_loss=5.1616, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0878, train_f1=0.287 | val_loss=7.0401, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0832, train_f1=0.310 | val_loss=5.7796, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0816, train_f1=0.317 | val_loss=5.1236, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0808, train_f1=0.337 | val_loss=4.9764, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0801, train_f1=0.350 | val_loss=4.4032, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0799, train_f1=0.342 | val_loss=4.4267, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0798, train_f1=0.360 | val_loss=4.2190, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0797, train_f1=0.342 | val_loss=4.2980, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2223, train_f1=0.191 | val_loss=4.2275, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0903, train_f1=0.352 | val_loss=5.8244, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0860, train_f1=0.385 | val_loss=5.6233, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0842, train_f1=0.376 | val_loss=5.3259, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0832, train_f1=0.369 | val_loss=4.8782, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0826, train_f1=0.366 | val_loss=5.0462, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0827, train_f1=0.349 | val_loss=4.8528, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2393, train_f1=0.193 | val_loss=3.8988, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0906, train_f1=0.379 | val_loss=4.7781, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0866, train_f1=0.380 | val_loss=5.1198, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0848, train_f1=0.385 | val_loss=5.1918, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0836, train_f1=0.383 | val_loss=5.0573, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0837, train_f1=0.366 | val_loss=4.8268, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0828, train_f1=0.351 | val_loss=4.8302, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2201, train_f1=0.189 | val_loss=3.0582, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0906, train_f1=0.333 | val_loss=4.2975, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0855, train_f1=0.375 | val_loss=4.7579, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0845, train_f1=0.354 | val_loss=4.5912, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0831, train_f1=0.357 | val_loss=4.5326, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0827, train_f1=0.347 | val_loss=4.6687, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0824, train_f1=0.351 | val_loss=4.5558, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0826, train_f1=0.342 | val_loss=4.5868, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2347, train_f1=0.143 | val_loss=3.0712, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0955, train_f1=0.235 | val_loss=4.9571, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0890, train_f1=0.264 | val_loss=7.3191, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0869, train_f1=0.262 | val_loss=7.7955, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0860, train_f1=0.247 | val_loss=7.6946, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0855, train_f1=0.224 | val_loss=7.5768, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0844, train_f1=0.258 | val_loss=7.5595, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1221\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1981, train_f1=0.086 | val_loss=6.6540, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0820, train_f1=0.339 | val_loss=3.4842, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0795, train_f1=0.375 | val_loss=3.6770, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0787, train_f1=0.393 | val_loss=4.1598, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0779, train_f1=0.399 | val_loss=3.7101, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0773, train_f1=0.389 | val_loss=4.0836, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1951, train_f1=0.110 | val_loss=5.3017, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0820, train_f1=0.325 | val_loss=3.8457, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0801, train_f1=0.355 | val_loss=3.4484, val_f1=0.109\n",
      "[Epoch 004] train_loss=0.0790, train_f1=0.368 | val_loss=2.8077, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0781, train_f1=0.368 | val_loss=2.9283, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0775, train_f1=0.383 | val_loss=2.8388, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0773, train_f1=0.370 | val_loss=3.2948, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0770, train_f1=0.376 | val_loss=3.1609, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0767, train_f1=0.364 | val_loss=3.2482, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0766, train_f1=0.370 | val_loss=3.2096, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1919, train_f1=0.239 | val_loss=6.1349, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0812, train_f1=0.370 | val_loss=5.0595, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0792, train_f1=0.393 | val_loss=3.5503, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0786, train_f1=0.392 | val_loss=4.6877, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0780, train_f1=0.389 | val_loss=3.8070, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0775, train_f1=0.398 | val_loss=3.1154, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0769, train_f1=0.397 | val_loss=3.6635, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0768, train_f1=0.371 | val_loss=3.3086, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1864, train_f1=0.186 | val_loss=4.2497, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0803, train_f1=0.372 | val_loss=3.6201, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0795, train_f1=0.391 | val_loss=3.7878, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0789, train_f1=0.375 | val_loss=3.8317, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0781, train_f1=0.404 | val_loss=3.5440, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0779, train_f1=0.401 | val_loss=3.4178, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0772, train_f1=0.395 | val_loss=3.1445, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2167, train_f1=0.228 | val_loss=3.3123, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0843, train_f1=0.329 | val_loss=3.3239, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0811, train_f1=0.353 | val_loss=3.6039, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0805, train_f1=0.396 | val_loss=3.7146, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.377 | val_loss=3.5111, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0799, train_f1=0.375 | val_loss=3.4360, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0796, train_f1=0.377 | val_loss=3.3425, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2214, train_f1=0.232 | val_loss=4.1305, val_f1=0.109\n",
      "[Epoch 002] train_loss=0.0841, train_f1=0.373 | val_loss=4.1866, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0813, train_f1=0.397 | val_loss=3.7280, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0809, train_f1=0.389 | val_loss=3.5984, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0801, train_f1=0.383 | val_loss=3.4223, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.391 | val_loss=3.5732, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0797, train_f1=0.386 | val_loss=3.3621, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0793, train_f1=0.377 | val_loss=3.2711, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0795, train_f1=0.387 | val_loss=3.2580, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1886, train_f1=0.168 | val_loss=5.1840, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0865, train_f1=0.308 | val_loss=4.5364, val_f1=0.226\n",
      "[Epoch 003] train_loss=0.0822, train_f1=0.337 | val_loss=3.6848, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0805, train_f1=0.363 | val_loss=3.2388, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0797, train_f1=0.370 | val_loss=3.0536, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0793, train_f1=0.381 | val_loss=3.1400, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0792, train_f1=0.373 | val_loss=3.1326, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2260\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1974, train_f1=0.187 | val_loss=4.2687, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0843, train_f1=0.330 | val_loss=3.6260, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0814, train_f1=0.368 | val_loss=3.4713, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0806, train_f1=0.372 | val_loss=3.1972, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.361 | val_loss=3.3363, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.351 | val_loss=3.1226, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0798, train_f1=0.346 | val_loss=3.1355, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2547, train_f1=0.172 | val_loss=4.0302, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0925, train_f1=0.377 | val_loss=4.5803, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0868, train_f1=0.406 | val_loss=4.6385, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0849, train_f1=0.395 | val_loss=4.4000, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0840, train_f1=0.387 | val_loss=4.3159, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0831, train_f1=0.385 | val_loss=3.9394, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0826, train_f1=0.380 | val_loss=3.9985, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2664, train_f1=0.144 | val_loss=2.5344, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.1000, train_f1=0.268 | val_loss=3.7990, val_f1=0.123\n",
      "[Epoch 003] train_loss=0.0907, train_f1=0.311 | val_loss=3.9508, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0879, train_f1=0.327 | val_loss=4.2532, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0860, train_f1=0.327 | val_loss=3.8855, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0852, train_f1=0.318 | val_loss=3.7412, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0842, train_f1=0.315 | val_loss=3.7135, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0836, train_f1=0.337 | val_loss=3.8021, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2189\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2241, train_f1=0.155 | val_loss=4.7050, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0896, train_f1=0.396 | val_loss=5.3355, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0857, train_f1=0.404 | val_loss=4.4324, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0836, train_f1=0.397 | val_loss=3.9282, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0825, train_f1=0.387 | val_loss=3.8471, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0822, train_f1=0.377 | val_loss=3.6599, val_f1=0.219\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2573, train_f1=0.160 | val_loss=4.4767, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0931, train_f1=0.347 | val_loss=5.7467, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0881, train_f1=0.348 | val_loss=5.3693, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0859, train_f1=0.318 | val_loss=5.1050, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0846, train_f1=0.302 | val_loss=4.7306, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0839, train_f1=0.317 | val_loss=4.6935, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0835, train_f1=0.304 | val_loss=4.4882, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1975, train_f1=0.184 | val_loss=6.1022, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0857, train_f1=0.164 | val_loss=10.1535, val_f1=0.109\n",
      "[Epoch 003] train_loss=0.0821, train_f1=0.165 | val_loss=9.2860, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0807, train_f1=0.270 | val_loss=8.4726, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0804, train_f1=0.292 | val_loss=7.4720, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0800, train_f1=0.287 | val_loss=6.9886, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0797, train_f1=0.293 | val_loss=6.8852, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0794, train_f1=0.295 | val_loss=6.5602, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0795, train_f1=0.297 | val_loss=6.4924, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0794, train_f1=0.287 | val_loss=6.1868, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0792, train_f1=0.299 | val_loss=6.1508, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2234, train_f1=0.105 | val_loss=4.0548, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0907, train_f1=0.253 | val_loss=7.7405, val_f1=0.226\n",
      "[Epoch 003] train_loss=0.0833, train_f1=0.236 | val_loss=8.4701, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0808, train_f1=0.219 | val_loss=6.8551, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.284 | val_loss=6.2525, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.300 | val_loss=5.6278, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0793, train_f1=0.277 | val_loss=5.3534, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2259\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1894, train_f1=0.165 | val_loss=3.8035, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0904, train_f1=0.280 | val_loss=6.9820, val_f1=0.009\n",
      "[Epoch 003] train_loss=0.0850, train_f1=0.281 | val_loss=6.4965, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0821, train_f1=0.304 | val_loss=4.8677, val_f1=0.226\n",
      "[Epoch 005] train_loss=0.0806, train_f1=0.315 | val_loss=4.2575, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.348 | val_loss=3.6972, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.339 | val_loss=3.7802, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0793, train_f1=0.339 | val_loss=3.8116, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0792, train_f1=0.350 | val_loss=3.6215, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2258\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1940, train_f1=0.104 | val_loss=4.8059, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0900, train_f1=0.294 | val_loss=10.2318, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0850, train_f1=0.258 | val_loss=11.1703, val_f1=0.226\n",
      "[Epoch 004] train_loss=0.0822, train_f1=0.267 | val_loss=9.9850, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0811, train_f1=0.315 | val_loss=8.3501, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0804, train_f1=0.332 | val_loss=7.5959, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0799, train_f1=0.328 | val_loss=6.6874, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0797, train_f1=0.316 | val_loss=6.3152, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2258\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2198, train_f1=0.110 | val_loss=2.7799, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0936, train_f1=0.228 | val_loss=6.5420, val_f1=0.109\n",
      "[Epoch 003] train_loss=0.0853, train_f1=0.231 | val_loss=9.5111, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0836, train_f1=0.224 | val_loss=9.6960, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0829, train_f1=0.259 | val_loss=9.5528, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0823, train_f1=0.244 | val_loss=9.5252, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0819, train_f1=0.278 | val_loss=9.9503, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0820, train_f1=0.258 | val_loss=9.7847, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0816, train_f1=0.283 | val_loss=9.5256, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0816, train_f1=0.263 | val_loss=9.3815, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0818, train_f1=0.273 | val_loss=9.6016, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2081, train_f1=0.151 | val_loss=8.5611, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0873, train_f1=0.254 | val_loss=12.5764, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0839, train_f1=0.236 | val_loss=13.1706, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.263 | val_loss=12.6226, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0821, train_f1=0.268 | val_loss=11.3910, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0815, train_f1=0.284 | val_loss=9.8349, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0814, train_f1=0.264 | val_loss=9.8440, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0811, train_f1=0.265 | val_loss=9.5716, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0812, train_f1=0.253 | val_loss=9.2521, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0810, train_f1=0.258 | val_loss=9.3856, val_f1=0.000\n",
      "[Epoch 011] train_loss=0.0811, train_f1=0.237 | val_loss=9.5523, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1843, train_f1=0.197 | val_loss=6.8788, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0871, train_f1=0.181 | val_loss=12.8780, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0837, train_f1=0.219 | val_loss=13.3875, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0828, train_f1=0.271 | val_loss=12.7632, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0826, train_f1=0.274 | val_loss=11.9839, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0817, train_f1=0.290 | val_loss=12.1497, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1174\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1794, train_f1=0.182 | val_loss=10.4398, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0852, train_f1=0.242 | val_loss=13.6434, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0830, train_f1=0.245 | val_loss=12.6019, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0821, train_f1=0.261 | val_loss=10.6341, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0815, train_f1=0.250 | val_loss=10.0900, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0813, train_f1=0.261 | val_loss=10.1739, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0808, train_f1=0.277 | val_loss=10.8902, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0808, train_f1=0.265 | val_loss=10.0643, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2589, train_f1=0.133 | val_loss=5.1471, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0950, train_f1=0.254 | val_loss=11.0724, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0870, train_f1=0.239 | val_loss=13.7739, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0853, train_f1=0.252 | val_loss=14.1705, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0842, train_f1=0.276 | val_loss=13.9381, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0837, train_f1=0.281 | val_loss=13.7141, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0837, train_f1=0.271 | val_loss=12.9961, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0832, train_f1=0.266 | val_loss=13.3967, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0832, train_f1=0.288 | val_loss=13.7837, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0830, train_f1=0.280 | val_loss=13.4866, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0830, train_f1=0.281 | val_loss=14.1551, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2369, train_f1=0.157 | val_loss=4.7035, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0928, train_f1=0.251 | val_loss=10.1298, val_f1=0.109\n",
      "[Epoch 003] train_loss=0.0872, train_f1=0.297 | val_loss=12.3380, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0847, train_f1=0.284 | val_loss=13.8531, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0845, train_f1=0.267 | val_loss=13.3432, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0836, train_f1=0.274 | val_loss=13.2331, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1174\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2030, train_f1=0.125 | val_loss=4.0316, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0895, train_f1=0.185 | val_loss=7.6999, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0856, train_f1=0.253 | val_loss=9.1631, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0845, train_f1=0.246 | val_loss=9.9250, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0836, train_f1=0.266 | val_loss=10.4037, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0835, train_f1=0.254 | val_loss=9.7677, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0835, train_f1=0.245 | val_loss=9.8372, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0827, train_f1=0.244 | val_loss=9.7963, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2206, train_f1=0.082 | val_loss=5.4787, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0891, train_f1=0.222 | val_loss=9.5054, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0853, train_f1=0.223 | val_loss=10.8720, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0840, train_f1=0.226 | val_loss=12.8397, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0835, train_f1=0.229 | val_loss=12.6724, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0836, train_f1=0.254 | val_loss=11.3798, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1739, train_f1=0.099 | val_loss=6.0817, val_f1=0.109\n",
      "[Epoch 002] train_loss=0.0811, train_f1=0.240 | val_loss=3.9099, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0792, train_f1=0.320 | val_loss=3.3767, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0787, train_f1=0.322 | val_loss=2.7058, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0780, train_f1=0.355 | val_loss=4.1712, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0780, train_f1=0.364 | val_loss=3.5268, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0775, train_f1=0.349 | val_loss=3.5603, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1809, train_f1=0.108 | val_loss=4.4216, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0825, train_f1=0.226 | val_loss=4.6114, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.324 | val_loss=4.7706, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0796, train_f1=0.352 | val_loss=4.3610, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0790, train_f1=0.359 | val_loss=4.0406, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0787, train_f1=0.349 | val_loss=3.6605, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0781, train_f1=0.344 | val_loss=3.6568, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1684, train_f1=0.128 | val_loss=5.3925, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0821, train_f1=0.278 | val_loss=4.4394, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0797, train_f1=0.341 | val_loss=4.3631, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0790, train_f1=0.351 | val_loss=3.5439, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0784, train_f1=0.367 | val_loss=3.5909, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0777, train_f1=0.372 | val_loss=3.2945, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0773, train_f1=0.367 | val_loss=3.1837, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1623, train_f1=0.076 | val_loss=4.6686, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0818, train_f1=0.313 | val_loss=4.4846, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0802, train_f1=0.359 | val_loss=3.9643, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0793, train_f1=0.365 | val_loss=3.9522, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0786, train_f1=0.380 | val_loss=3.6794, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0783, train_f1=0.389 | val_loss=3.6893, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0777, train_f1=0.376 | val_loss=3.5196, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1939, train_f1=0.138 | val_loss=4.9912, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0858, train_f1=0.213 | val_loss=6.1446, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0821, train_f1=0.275 | val_loss=5.3333, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0808, train_f1=0.315 | val_loss=4.8378, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.341 | val_loss=4.5078, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0797, train_f1=0.351 | val_loss=3.8392, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0793, train_f1=0.364 | val_loss=3.8437, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0791, train_f1=0.366 | val_loss=3.9469, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0792, train_f1=0.368 | val_loss=3.8208, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0789, train_f1=0.367 | val_loss=3.8935, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1949, train_f1=0.243 | val_loss=4.4589, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0884, train_f1=0.289 | val_loss=6.1742, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.313 | val_loss=5.0806, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.332 | val_loss=5.2376, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0817, train_f1=0.344 | val_loss=4.1513, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0809, train_f1=0.341 | val_loss=5.0264, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0805, train_f1=0.344 | val_loss=4.3617, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0800, train_f1=0.346 | val_loss=4.2778, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0799, train_f1=0.356 | val_loss=4.1184, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0799, train_f1=0.358 | val_loss=4.1948, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1829, train_f1=0.197 | val_loss=5.3155, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0876, train_f1=0.262 | val_loss=5.7471, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.293 | val_loss=5.8617, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0831, train_f1=0.305 | val_loss=3.9626, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0819, train_f1=0.330 | val_loss=3.8258, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.339 | val_loss=4.4475, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0806, train_f1=0.328 | val_loss=4.2142, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0800, train_f1=0.350 | val_loss=3.9776, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0796, train_f1=0.350 | val_loss=3.9991, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0799, train_f1=0.337 | val_loss=4.0505, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0796, train_f1=0.348 | val_loss=4.0102, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1763, train_f1=0.184 | val_loss=4.9052, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0853, train_f1=0.286 | val_loss=5.0664, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0830, train_f1=0.288 | val_loss=5.3641, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0815, train_f1=0.296 | val_loss=4.9829, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0804, train_f1=0.312 | val_loss=4.5768, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0797, train_f1=0.323 | val_loss=4.9447, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0794, train_f1=0.320 | val_loss=4.3281, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2155, train_f1=0.183 | val_loss=4.1772, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0873, train_f1=0.319 | val_loss=4.3948, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0842, train_f1=0.342 | val_loss=4.3926, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.348 | val_loss=4.4213, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0824, train_f1=0.337 | val_loss=4.2433, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0816, train_f1=0.345 | val_loss=4.1898, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0813, train_f1=0.340 | val_loss=4.1725, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2324, train_f1=0.153 | val_loss=3.9093, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0933, train_f1=0.242 | val_loss=6.6603, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0879, train_f1=0.196 | val_loss=7.6019, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0873, train_f1=0.166 | val_loss=7.6405, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0857, train_f1=0.182 | val_loss=7.9941, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0854, train_f1=0.193 | val_loss=7.0974, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2027, train_f1=0.164 | val_loss=5.0644, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0855, train_f1=0.312 | val_loss=5.3914, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0825, train_f1=0.333 | val_loss=4.5760, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0817, train_f1=0.334 | val_loss=4.5651, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0809, train_f1=0.340 | val_loss=4.7910, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0811, train_f1=0.321 | val_loss=4.6992, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2229, train_f1=0.130 | val_loss=3.3744, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0922, train_f1=0.183 | val_loss=5.9466, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0867, train_f1=0.162 | val_loss=6.0989, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0850, train_f1=0.193 | val_loss=6.2745, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0837, train_f1=0.217 | val_loss=6.5035, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0832, train_f1=0.244 | val_loss=6.3390, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0827, train_f1=0.284 | val_loss=6.1732, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0824, train_f1=0.277 | val_loss=6.3178, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0821, train_f1=0.290 | val_loss=6.4024, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0825, train_f1=0.298 | val_loss=6.3122, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0822, train_f1=0.287 | val_loss=6.2371, val_f1=0.122\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1833, train_f1=0.225 | val_loss=3.4782, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0814, train_f1=0.382 | val_loss=3.8927, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0801, train_f1=0.385 | val_loss=4.1029, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0789, train_f1=0.391 | val_loss=4.0775, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0782, train_f1=0.385 | val_loss=3.5614, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0774, train_f1=0.380 | val_loss=3.4300, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0769, train_f1=0.376 | val_loss=3.3610, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0767, train_f1=0.368 | val_loss=3.6058, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1980, train_f1=0.173 | val_loss=3.1793, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0813, train_f1=0.378 | val_loss=2.9997, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0803, train_f1=0.387 | val_loss=3.4018, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0795, train_f1=0.393 | val_loss=3.7921, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0789, train_f1=0.387 | val_loss=3.5850, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0783, train_f1=0.379 | val_loss=2.8189, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0778, train_f1=0.382 | val_loss=3.1582, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1597, train_f1=0.270 | val_loss=4.9057, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0806, train_f1=0.375 | val_loss=3.7920, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0788, train_f1=0.380 | val_loss=2.7186, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0779, train_f1=0.385 | val_loss=3.7648, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0776, train_f1=0.379 | val_loss=3.3360, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0769, train_f1=0.382 | val_loss=3.1870, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0766, train_f1=0.366 | val_loss=3.0362, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0762, train_f1=0.346 | val_loss=3.1344, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1558, train_f1=0.168 | val_loss=3.5201, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0794, train_f1=0.369 | val_loss=3.6658, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0784, train_f1=0.389 | val_loss=3.1823, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0778, train_f1=0.386 | val_loss=3.4043, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0771, train_f1=0.363 | val_loss=3.1918, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0768, train_f1=0.370 | val_loss=3.7398, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0763, train_f1=0.340 | val_loss=3.3126, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0759, train_f1=0.349 | val_loss=3.4119, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0759, train_f1=0.313 | val_loss=3.3392, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2187\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1858, train_f1=0.184 | val_loss=4.8509, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0867, train_f1=0.259 | val_loss=4.6417, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0828, train_f1=0.312 | val_loss=3.5748, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.374 | val_loss=4.0122, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0793, train_f1=0.367 | val_loss=3.6301, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.355 | val_loss=3.4858, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0784, train_f1=0.350 | val_loss=3.3512, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0780, train_f1=0.348 | val_loss=3.6783, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0780, train_f1=0.317 | val_loss=3.4763, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1925, train_f1=0.186 | val_loss=4.3595, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0845, train_f1=0.326 | val_loss=4.1496, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0817, train_f1=0.362 | val_loss=2.9114, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.374 | val_loss=3.7135, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0794, train_f1=0.381 | val_loss=3.6652, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0790, train_f1=0.375 | val_loss=3.2796, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0786, train_f1=0.374 | val_loss=3.3508, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0786, train_f1=0.368 | val_loss=3.2810, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1643, train_f1=0.199 | val_loss=6.4038, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0843, train_f1=0.266 | val_loss=4.0643, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0812, train_f1=0.311 | val_loss=4.0172, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0797, train_f1=0.335 | val_loss=3.5245, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0792, train_f1=0.320 | val_loss=3.1380, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.324 | val_loss=2.8778, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0785, train_f1=0.312 | val_loss=3.4252, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0781, train_f1=0.325 | val_loss=3.4187, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1672, train_f1=0.224 | val_loss=3.8805, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0820, train_f1=0.342 | val_loss=3.4205, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0802, train_f1=0.359 | val_loss=3.7720, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0795, train_f1=0.348 | val_loss=3.5578, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0791, train_f1=0.345 | val_loss=3.3849, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0787, train_f1=0.348 | val_loss=3.2108, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1981, train_f1=0.183 | val_loss=3.8023, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0874, train_f1=0.365 | val_loss=3.9426, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0838, train_f1=0.393 | val_loss=3.3705, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0818, train_f1=0.381 | val_loss=3.4832, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0815, train_f1=0.377 | val_loss=3.3093, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0812, train_f1=0.355 | val_loss=3.4050, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0804, train_f1=0.369 | val_loss=3.1376, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2202, train_f1=0.185 | val_loss=4.3424, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0874, train_f1=0.362 | val_loss=4.0256, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0839, train_f1=0.375 | val_loss=3.6724, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0821, train_f1=0.385 | val_loss=3.3296, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0817, train_f1=0.360 | val_loss=3.6487, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0810, train_f1=0.360 | val_loss=3.3906, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.2017, train_f1=0.187 | val_loss=4.3802, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0864, train_f1=0.326 | val_loss=4.0060, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0830, train_f1=0.334 | val_loss=3.3136, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0815, train_f1=0.338 | val_loss=3.5620, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0806, train_f1=0.326 | val_loss=3.7219, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0807, train_f1=0.320 | val_loss=3.8009, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0003 ===\n",
      "[Epoch 001] train_loss=0.1945, train_f1=0.177 | val_loss=3.6641, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0896, train_f1=0.243 | val_loss=6.1977, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0851, train_f1=0.269 | val_loss=6.6414, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0832, train_f1=0.268 | val_loss=5.9164, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0822, train_f1=0.279 | val_loss=5.2669, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0816, train_f1=0.291 | val_loss=4.9958, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0808, train_f1=0.308 | val_loss=4.9050, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2344, train_f1=0.015 | val_loss=2.3112, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0887, train_f1=0.059 | val_loss=6.5929, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0820, train_f1=0.230 | val_loss=6.9504, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0808, train_f1=0.281 | val_loss=5.4375, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0802, train_f1=0.302 | val_loss=4.7776, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.318 | val_loss=4.6034, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.305 | val_loss=4.2488, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0793, train_f1=0.300 | val_loss=4.1338, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2143, train_f1=0.133 | val_loss=5.7945, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0860, train_f1=0.129 | val_loss=10.6421, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0824, train_f1=0.194 | val_loss=11.1631, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0811, train_f1=0.294 | val_loss=9.9732, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0804, train_f1=0.283 | val_loss=8.1863, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0799, train_f1=0.299 | val_loss=6.6607, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0793, train_f1=0.281 | val_loss=5.8492, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0789, train_f1=0.306 | val_loss=4.6597, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1941, train_f1=0.122 | val_loss=6.9677, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0878, train_f1=0.288 | val_loss=9.4628, val_f1=0.117\n",
      "[Epoch 003] train_loss=0.0842, train_f1=0.296 | val_loss=9.9138, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0818, train_f1=0.296 | val_loss=8.3445, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0802, train_f1=0.298 | val_loss=6.8639, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.323 | val_loss=5.4539, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0792, train_f1=0.325 | val_loss=4.7798, val_f1=0.219\n",
      "[Epoch 008] train_loss=0.0788, train_f1=0.323 | val_loss=4.5543, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0789, train_f1=0.299 | val_loss=4.4387, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0785, train_f1=0.329 | val_loss=4.3686, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1835, train_f1=0.197 | val_loss=13.3032, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0834, train_f1=0.212 | val_loss=14.0564, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0811, train_f1=0.277 | val_loss=10.1907, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0801, train_f1=0.298 | val_loss=7.0063, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0795, train_f1=0.305 | val_loss=5.1828, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0790, train_f1=0.317 | val_loss=4.3493, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0788, train_f1=0.304 | val_loss=3.7132, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0785, train_f1=0.300 | val_loss=3.8429, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2051, train_f1=0.169 | val_loss=8.4500, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0872, train_f1=0.283 | val_loss=12.4247, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0831, train_f1=0.300 | val_loss=13.1896, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0823, train_f1=0.319 | val_loss=11.3783, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0818, train_f1=0.313 | val_loss=10.1487, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0812, train_f1=0.323 | val_loss=9.6845, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0809, train_f1=0.310 | val_loss=9.5865, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0807, train_f1=0.311 | val_loss=9.4018, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2326, train_f1=0.085 | val_loss=3.7662, val_f1=0.118\n",
      "[Epoch 002] train_loss=0.0872, train_f1=0.222 | val_loss=8.2345, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0829, train_f1=0.293 | val_loss=9.0054, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0822, train_f1=0.302 | val_loss=9.0550, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0818, train_f1=0.319 | val_loss=8.6536, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0819, train_f1=0.302 | val_loss=8.2739, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0815, train_f1=0.298 | val_loss=8.5365, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0810, train_f1=0.297 | val_loss=8.2974, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0812, train_f1=0.305 | val_loss=8.7339, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1792, train_f1=0.214 | val_loss=11.8262, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0851, train_f1=0.241 | val_loss=14.3495, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0828, train_f1=0.274 | val_loss=14.3127, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0820, train_f1=0.270 | val_loss=12.6546, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0813, train_f1=0.315 | val_loss=11.4712, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0813, train_f1=0.288 | val_loss=10.6969, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0812, train_f1=0.297 | val_loss=10.7696, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0811, train_f1=0.290 | val_loss=10.1473, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0805, train_f1=0.290 | val_loss=10.1532, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1887, train_f1=0.162 | val_loss=6.4194, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0853, train_f1=0.270 | val_loss=9.5329, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0834, train_f1=0.310 | val_loss=9.4464, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0826, train_f1=0.294 | val_loss=9.4585, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0821, train_f1=0.305 | val_loss=9.7462, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0817, train_f1=0.306 | val_loss=9.7877, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0814, train_f1=0.277 | val_loss=9.8023, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1221\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2207, train_f1=0.167 | val_loss=8.8973, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0896, train_f1=0.325 | val_loss=14.5621, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0858, train_f1=0.320 | val_loss=14.5245, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0843, train_f1=0.319 | val_loss=15.0005, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0840, train_f1=0.321 | val_loss=14.1677, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0837, train_f1=0.291 | val_loss=13.3847, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1174\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1963, train_f1=0.221 | val_loss=11.1181, val_f1=0.109\n",
      "[Epoch 002] train_loss=0.0879, train_f1=0.330 | val_loss=14.4235, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0847, train_f1=0.320 | val_loss=14.9453, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0841, train_f1=0.318 | val_loss=13.2058, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0833, train_f1=0.306 | val_loss=12.6606, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0831, train_f1=0.302 | val_loss=12.4059, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0829, train_f1=0.293 | val_loss=11.5867, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0826, train_f1=0.304 | val_loss=11.0874, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2026, train_f1=0.204 | val_loss=10.0026, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0868, train_f1=0.330 | val_loss=14.1194, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0841, train_f1=0.335 | val_loss=14.2669, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0830, train_f1=0.339 | val_loss=12.1005, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0826, train_f1=0.299 | val_loss=12.0025, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0826, train_f1=0.292 | val_loss=10.2391, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0828, train_f1=0.290 | val_loss=10.6105, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2050, train_f1=0.197 | val_loss=8.5879, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0875, train_f1=0.318 | val_loss=12.3612, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0850, train_f1=0.305 | val_loss=12.3872, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0836, train_f1=0.302 | val_loss=12.8044, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0836, train_f1=0.293 | val_loss=11.9071, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0829, train_f1=0.279 | val_loss=11.7546, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0830, train_f1=0.279 | val_loss=11.8086, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0823, train_f1=0.262 | val_loss=11.7360, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1825, train_f1=0.123 | val_loss=4.2526, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0826, train_f1=0.251 | val_loss=4.7813, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0801, train_f1=0.350 | val_loss=4.4936, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0787, train_f1=0.370 | val_loss=3.4868, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0780, train_f1=0.376 | val_loss=3.8964, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0775, train_f1=0.386 | val_loss=3.5251, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1868, train_f1=0.247 | val_loss=5.9584, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0813, train_f1=0.291 | val_loss=4.4087, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0793, train_f1=0.322 | val_loss=3.6119, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0781, train_f1=0.357 | val_loss=3.2780, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0776, train_f1=0.356 | val_loss=3.4951, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0771, train_f1=0.353 | val_loss=3.1678, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0767, train_f1=0.361 | val_loss=3.2075, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0766, train_f1=0.345 | val_loss=3.1698, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0765, train_f1=0.324 | val_loss=3.2477, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1610, train_f1=0.169 | val_loss=5.1570, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0809, train_f1=0.301 | val_loss=4.2381, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0792, train_f1=0.336 | val_loss=3.7151, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0782, train_f1=0.361 | val_loss=3.2843, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0777, train_f1=0.367 | val_loss=2.7078, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0776, train_f1=0.372 | val_loss=3.2767, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0771, train_f1=0.370 | val_loss=3.5149, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0768, train_f1=0.366 | val_loss=3.3617, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1758, train_f1=0.161 | val_loss=5.3750, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0826, train_f1=0.220 | val_loss=4.1754, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0794, train_f1=0.368 | val_loss=3.3686, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0784, train_f1=0.371 | val_loss=3.5461, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0777, train_f1=0.389 | val_loss=3.1429, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0774, train_f1=0.378 | val_loss=3.8104, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0773, train_f1=0.383 | val_loss=3.1240, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1795, train_f1=0.228 | val_loss=4.8237, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0832, train_f1=0.317 | val_loss=4.7709, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0810, train_f1=0.348 | val_loss=3.9937, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0802, train_f1=0.341 | val_loss=3.6008, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0796, train_f1=0.364 | val_loss=3.9896, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.348 | val_loss=3.7049, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.360 | val_loss=3.7234, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1894, train_f1=0.129 | val_loss=5.6548, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0847, train_f1=0.312 | val_loss=5.8593, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0814, train_f1=0.348 | val_loss=4.7701, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0805, train_f1=0.353 | val_loss=4.8329, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.357 | val_loss=4.5569, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0796, train_f1=0.354 | val_loss=4.1347, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0794, train_f1=0.354 | val_loss=4.2440, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0791, train_f1=0.368 | val_loss=4.1009, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0791, train_f1=0.350 | val_loss=4.2211, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1656, train_f1=0.197 | val_loss=5.3677, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0826, train_f1=0.312 | val_loss=4.3261, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0805, train_f1=0.334 | val_loss=4.2814, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0796, train_f1=0.352 | val_loss=4.1345, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0792, train_f1=0.355 | val_loss=3.8667, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0793, train_f1=0.330 | val_loss=4.7372, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0791, train_f1=0.359 | val_loss=3.8725, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1799, train_f1=0.154 | val_loss=3.5431, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0880, train_f1=0.264 | val_loss=4.7854, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0845, train_f1=0.301 | val_loss=6.0067, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.303 | val_loss=5.7759, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0814, train_f1=0.327 | val_loss=4.9804, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0807, train_f1=0.337 | val_loss=5.1025, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0802, train_f1=0.336 | val_loss=4.9822, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0803, train_f1=0.322 | val_loss=4.9604, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0798, train_f1=0.347 | val_loss=5.2332, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0797, train_f1=0.327 | val_loss=5.0936, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2300, train_f1=0.141 | val_loss=4.4796, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0881, train_f1=0.388 | val_loss=6.1129, val_f1=0.109\n",
      "[Epoch 003] train_loss=0.0850, train_f1=0.392 | val_loss=5.4669, val_f1=0.109\n",
      "[Epoch 004] train_loss=0.0834, train_f1=0.381 | val_loss=4.9710, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0827, train_f1=0.366 | val_loss=4.8028, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0829, train_f1=0.355 | val_loss=4.8146, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2088, train_f1=0.173 | val_loss=4.0343, val_f1=0.226\n",
      "[Epoch 002] train_loss=0.0881, train_f1=0.365 | val_loss=4.2871, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0851, train_f1=0.388 | val_loss=4.3377, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0842, train_f1=0.369 | val_loss=4.1801, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0832, train_f1=0.362 | val_loss=4.0620, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0828, train_f1=0.347 | val_loss=3.7125, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2259\n",
      "\n",
      "=== Config: sage 32 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1924, train_f1=0.183 | val_loss=4.6565, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0887, train_f1=0.346 | val_loss=7.4669, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0856, train_f1=0.342 | val_loss=7.1999, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0836, train_f1=0.342 | val_loss=6.8843, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0825, train_f1=0.343 | val_loss=7.1602, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0823, train_f1=0.318 | val_loss=5.9995, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0822, train_f1=0.315 | val_loss=6.2122, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2187\n",
      "\n",
      "=== Config: gcn 32 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1870, train_f1=0.219 | val_loss=5.5674, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0857, train_f1=0.381 | val_loss=5.8472, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0835, train_f1=0.372 | val_loss=4.6919, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0827, train_f1=0.345 | val_loss=5.9933, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0820, train_f1=0.327 | val_loss=5.7024, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0815, train_f1=0.342 | val_loss=5.4624, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0807, train_f1=0.342 | val_loss=5.0431, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0811, train_f1=0.334 | val_loss=5.2406, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0809, train_f1=0.325 | val_loss=5.1500, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0808, train_f1=0.332 | val_loss=5.1389, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1871, train_f1=0.227 | val_loss=4.2574, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0806, train_f1=0.326 | val_loss=3.9721, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0786, train_f1=0.329 | val_loss=3.3938, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0780, train_f1=0.362 | val_loss=3.8520, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0778, train_f1=0.381 | val_loss=3.1556, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0772, train_f1=0.372 | val_loss=3.5967, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0769, train_f1=0.396 | val_loss=3.1497, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0765, train_f1=0.386 | val_loss=3.6180, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0765, train_f1=0.399 | val_loss=3.2090, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1789, train_f1=0.259 | val_loss=3.2141, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0793, train_f1=0.373 | val_loss=2.5922, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0781, train_f1=0.372 | val_loss=3.6237, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0775, train_f1=0.395 | val_loss=3.0846, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0767, train_f1=0.409 | val_loss=3.2371, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0765, train_f1=0.383 | val_loss=3.3194, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1655, train_f1=0.221 | val_loss=4.0482, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0788, train_f1=0.358 | val_loss=3.3767, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0779, train_f1=0.367 | val_loss=2.6477, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0774, train_f1=0.377 | val_loss=3.7454, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0772, train_f1=0.352 | val_loss=3.1821, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0767, train_f1=0.379 | val_loss=3.2596, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0764, train_f1=0.352 | val_loss=3.7205, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0762, train_f1=0.320 | val_loss=3.6689, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 32 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1510, train_f1=0.276 | val_loss=3.4038, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0797, train_f1=0.364 | val_loss=3.7750, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0782, train_f1=0.388 | val_loss=3.6772, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0773, train_f1=0.374 | val_loss=3.1763, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0767, train_f1=0.360 | val_loss=3.5801, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0763, train_f1=0.331 | val_loss=4.0553, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1806, train_f1=0.214 | val_loss=3.7914, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0829, train_f1=0.351 | val_loss=3.5902, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.363 | val_loss=3.6711, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.375 | val_loss=3.3935, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0796, train_f1=0.368 | val_loss=3.1536, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0792, train_f1=0.367 | val_loss=2.7317, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0787, train_f1=0.361 | val_loss=3.1813, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2025, train_f1=0.187 | val_loss=3.7443, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0837, train_f1=0.337 | val_loss=3.1685, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0817, train_f1=0.352 | val_loss=3.3764, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.365 | val_loss=3.3840, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0799, train_f1=0.375 | val_loss=3.1622, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0794, train_f1=0.366 | val_loss=3.3624, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0791, train_f1=0.366 | val_loss=3.3007, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 32 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1791, train_f1=0.142 | val_loss=5.8593, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0872, train_f1=0.214 | val_loss=5.1148, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0827, train_f1=0.262 | val_loss=4.0361, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0800, train_f1=0.323 | val_loss=3.3155, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0792, train_f1=0.340 | val_loss=3.6010, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0786, train_f1=0.322 | val_loss=3.5360, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0787, train_f1=0.314 | val_loss=3.4209, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0783, train_f1=0.272 | val_loss=3.5958, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0781, train_f1=0.316 | val_loss=3.6961, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 32 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1738, train_f1=0.196 | val_loss=3.9976, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0821, train_f1=0.345 | val_loss=3.6544, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.367 | val_loss=3.7316, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0794, train_f1=0.369 | val_loss=2.7389, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0791, train_f1=0.360 | val_loss=3.0678, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0790, train_f1=0.376 | val_loss=3.4548, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2076, train_f1=0.228 | val_loss=3.9330, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0881, train_f1=0.398 | val_loss=4.5862, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.410 | val_loss=4.1825, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.384 | val_loss=3.4731, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0820, train_f1=0.369 | val_loss=4.0136, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0816, train_f1=0.350 | val_loss=3.7644, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0808, train_f1=0.342 | val_loss=3.5796, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1221\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2298, train_f1=0.163 | val_loss=2.8417, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0943, train_f1=0.288 | val_loss=4.7616, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0875, train_f1=0.322 | val_loss=4.7876, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0847, train_f1=0.348 | val_loss=5.0639, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0837, train_f1=0.350 | val_loss=4.6421, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0831, train_f1=0.319 | val_loss=4.5512, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0826, train_f1=0.327 | val_loss=4.7268, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0819, train_f1=0.321 | val_loss=4.4250, val_f1=0.219\n",
      "[Epoch 009] train_loss=0.0816, train_f1=0.324 | val_loss=4.4385, val_f1=0.219\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 32 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1866, train_f1=0.249 | val_loss=5.0634, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0860, train_f1=0.390 | val_loss=3.4215, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0835, train_f1=0.367 | val_loss=3.5486, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.346 | val_loss=4.1812, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0815, train_f1=0.325 | val_loss=2.9226, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0811, train_f1=0.343 | val_loss=3.7686, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2189\n",
      "\n",
      "=== Config: gcn 32 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1998, train_f1=0.178 | val_loss=4.1791, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0889, train_f1=0.365 | val_loss=5.1004, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0848, train_f1=0.361 | val_loss=5.0508, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0836, train_f1=0.344 | val_loss=4.6455, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0824, train_f1=0.325 | val_loss=4.5940, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0816, train_f1=0.333 | val_loss=4.5442, val_f1=0.219\n",
      "[Epoch 007] train_loss=0.0813, train_f1=0.325 | val_loss=4.2614, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0811, train_f1=0.315 | val_loss=4.0194, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1955, train_f1=0.137 | val_loss=6.9817, val_f1=0.010\n",
      "[Epoch 002] train_loss=0.0851, train_f1=0.120 | val_loss=12.7923, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0820, train_f1=0.284 | val_loss=12.2690, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0808, train_f1=0.307 | val_loss=9.9844, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0803, train_f1=0.313 | val_loss=8.2568, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0799, train_f1=0.311 | val_loss=6.3762, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.297 | val_loss=5.5441, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1900, train_f1=0.171 | val_loss=4.5621, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0879, train_f1=0.258 | val_loss=6.5899, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0822, train_f1=0.278 | val_loss=5.6188, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.321 | val_loss=4.1220, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0794, train_f1=0.358 | val_loss=3.7776, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0792, train_f1=0.348 | val_loss=3.7684, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0790, train_f1=0.338 | val_loss=3.9504, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0788, train_f1=0.337 | val_loss=3.6683, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0786, train_f1=0.335 | val_loss=3.8594, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0784, train_f1=0.317 | val_loss=3.7078, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0784, train_f1=0.334 | val_loss=3.6386, val_f1=0.000\n",
      "Early stopping triggered at epoch 11. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1616, train_f1=0.132 | val_loss=13.0776, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0833, train_f1=0.246 | val_loss=14.4305, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0819, train_f1=0.286 | val_loss=11.5522, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0809, train_f1=0.322 | val_loss=8.2972, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0799, train_f1=0.312 | val_loss=5.8228, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0795, train_f1=0.320 | val_loss=4.9371, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0788, train_f1=0.321 | val_loss=3.9670, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0787, train_f1=0.311 | val_loss=3.6322, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0782, train_f1=0.300 | val_loss=3.6267, val_f1=0.122\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 1 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1720, train_f1=0.125 | val_loss=4.2387, val_f1=0.117\n",
      "[Epoch 002] train_loss=0.0875, train_f1=0.286 | val_loss=4.9634, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0817, train_f1=0.347 | val_loss=4.2048, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0805, train_f1=0.375 | val_loss=3.9970, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0800, train_f1=0.362 | val_loss=3.7128, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.352 | val_loss=3.6468, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0793, train_f1=0.355 | val_loss=3.3952, val_f1=0.219\n",
      "[Epoch 008] train_loss=0.0792, train_f1=0.335 | val_loss=3.7770, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0790, train_f1=0.356 | val_loss=3.5931, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0789, train_f1=0.321 | val_loss=3.6086, val_f1=0.122\n",
      "[Epoch 011] train_loss=0.0789, train_f1=0.304 | val_loss=3.6819, val_f1=0.122\n",
      "[Epoch 012] train_loss=0.0788, train_f1=0.326 | val_loss=3.7179, val_f1=0.122\n",
      "Early stopping triggered at epoch 12. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2003, train_f1=0.180 | val_loss=8.2685, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0848, train_f1=0.224 | val_loss=12.2036, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0828, train_f1=0.281 | val_loss=11.6767, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0820, train_f1=0.290 | val_loss=10.9496, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0816, train_f1=0.282 | val_loss=10.8424, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0815, train_f1=0.283 | val_loss=10.1893, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1743, train_f1=0.129 | val_loss=6.7446, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0853, train_f1=0.220 | val_loss=10.6901, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0834, train_f1=0.256 | val_loss=11.1403, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0826, train_f1=0.288 | val_loss=10.1137, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0819, train_f1=0.281 | val_loss=9.7603, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0814, train_f1=0.289 | val_loss=9.7536, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0812, train_f1=0.280 | val_loss=9.4537, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1745, train_f1=0.164 | val_loss=5.2267, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0854, train_f1=0.229 | val_loss=6.5079, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0834, train_f1=0.251 | val_loss=7.2752, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0828, train_f1=0.272 | val_loss=7.0482, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0822, train_f1=0.272 | val_loss=7.4590, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0818, train_f1=0.286 | val_loss=7.3559, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0092\n",
      "\n",
      "=== Config: gcn 64 1 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1579, train_f1=0.186 | val_loss=10.9846, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0841, train_f1=0.264 | val_loss=12.6326, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0823, train_f1=0.275 | val_loss=13.0901, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0821, train_f1=0.294 | val_loss=11.1355, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0816, train_f1=0.282 | val_loss=10.7993, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0810, train_f1=0.273 | val_loss=10.7242, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0810, train_f1=0.286 | val_loss=10.2740, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0810, train_f1=0.239 | val_loss=10.6269, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1944, train_f1=0.165 | val_loss=7.3703, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0872, train_f1=0.303 | val_loss=10.6575, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0844, train_f1=0.327 | val_loss=11.4801, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0838, train_f1=0.291 | val_loss=11.4349, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0831, train_f1=0.303 | val_loss=11.3936, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0830, train_f1=0.280 | val_loss=10.6589, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0826, train_f1=0.284 | val_loss=11.2601, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.2010, train_f1=0.167 | val_loss=8.8640, val_f1=0.009\n",
      "[Epoch 002] train_loss=0.0880, train_f1=0.316 | val_loss=12.5157, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0847, train_f1=0.284 | val_loss=12.5282, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0841, train_f1=0.278 | val_loss=12.1569, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0829, train_f1=0.292 | val_loss=12.4744, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0829, train_f1=0.276 | val_loss=11.8138, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0826, train_f1=0.279 | val_loss=11.8110, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0823, train_f1=0.274 | val_loss=10.8061, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0823, train_f1=0.287 | val_loss=11.5932, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1735, train_f1=0.184 | val_loss=10.5019, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0851, train_f1=0.287 | val_loss=12.1805, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0832, train_f1=0.311 | val_loss=12.6667, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0824, train_f1=0.300 | val_loss=12.4487, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0823, train_f1=0.307 | val_loss=11.6844, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0821, train_f1=0.265 | val_loss=10.9619, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0817, train_f1=0.272 | val_loss=10.6596, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 1 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1751, train_f1=0.131 | val_loss=7.4929, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0860, train_f1=0.147 | val_loss=10.6033, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0842, train_f1=0.220 | val_loss=10.3359, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0829, train_f1=0.240 | val_loss=11.4240, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0827, train_f1=0.241 | val_loss=11.1879, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0822, train_f1=0.237 | val_loss=11.7799, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0823, train_f1=0.231 | val_loss=11.1338, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0822, train_f1=0.200 | val_loss=10.5424, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0821, train_f1=0.221 | val_loss=10.5950, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1583, train_f1=0.182 | val_loss=5.7472, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0812, train_f1=0.333 | val_loss=4.7381, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0790, train_f1=0.358 | val_loss=3.2770, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0781, train_f1=0.387 | val_loss=3.4276, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0773, train_f1=0.382 | val_loss=3.7580, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0770, train_f1=0.383 | val_loss=3.4913, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0768, train_f1=0.370 | val_loss=3.2629, val_f1=0.219\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1682, train_f1=0.182 | val_loss=5.4066, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0803, train_f1=0.376 | val_loss=4.5771, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0792, train_f1=0.397 | val_loss=3.2484, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0786, train_f1=0.395 | val_loss=3.7534, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0781, train_f1=0.386 | val_loss=3.6001, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0776, train_f1=0.386 | val_loss=3.4245, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: sage 64 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1518, train_f1=0.155 | val_loss=4.9524, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0808, train_f1=0.341 | val_loss=4.0790, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0789, train_f1=0.396 | val_loss=3.0348, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0783, train_f1=0.397 | val_loss=3.3484, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0776, train_f1=0.393 | val_loss=3.7687, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0773, train_f1=0.399 | val_loss=3.4689, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0769, train_f1=0.383 | val_loss=3.5459, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1465, train_f1=0.153 | val_loss=4.7686, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0812, train_f1=0.339 | val_loss=4.2303, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0797, train_f1=0.364 | val_loss=3.7599, val_f1=0.219\n",
      "[Epoch 004] train_loss=0.0792, train_f1=0.380 | val_loss=3.7032, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0782, train_f1=0.375 | val_loss=3.0449, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0777, train_f1=0.395 | val_loss=3.5783, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0775, train_f1=0.381 | val_loss=3.3181, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0770, train_f1=0.377 | val_loss=3.2835, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1823, train_f1=0.171 | val_loss=6.1820, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0858, train_f1=0.281 | val_loss=4.9333, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0824, train_f1=0.307 | val_loss=4.7649, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0808, train_f1=0.338 | val_loss=4.6774, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0803, train_f1=0.346 | val_loss=4.9067, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0798, train_f1=0.329 | val_loss=4.4518, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.339 | val_loss=4.0441, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0792, train_f1=0.346 | val_loss=4.0004, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1762, train_f1=0.174 | val_loss=5.5026, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0841, train_f1=0.281 | val_loss=4.7547, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0811, train_f1=0.341 | val_loss=4.3667, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0803, train_f1=0.353 | val_loss=4.8564, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0795, train_f1=0.356 | val_loss=4.5896, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0793, train_f1=0.354 | val_loss=4.0133, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0788, train_f1=0.370 | val_loss=4.2220, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0787, train_f1=0.350 | val_loss=3.7591, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0787, train_f1=0.363 | val_loss=3.8104, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0788, train_f1=0.353 | val_loss=3.9590, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1466, train_f1=0.188 | val_loss=6.6106, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0837, train_f1=0.237 | val_loss=6.8402, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0816, train_f1=0.280 | val_loss=4.7889, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0802, train_f1=0.310 | val_loss=5.0690, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0797, train_f1=0.297 | val_loss=5.0146, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0792, train_f1=0.331 | val_loss=4.0691, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0787, train_f1=0.306 | val_loss=4.5618, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0785, train_f1=0.334 | val_loss=4.2854, val_f1=0.122\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1532, train_f1=0.198 | val_loss=5.6404, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0829, train_f1=0.316 | val_loss=4.8270, val_f1=0.219\n",
      "[Epoch 003] train_loss=0.0806, train_f1=0.310 | val_loss=4.4846, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0795, train_f1=0.324 | val_loss=4.4875, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0790, train_f1=0.339 | val_loss=4.0874, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0787, train_f1=0.349 | val_loss=3.5711, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0786, train_f1=0.334 | val_loss=4.0715, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1773, train_f1=0.219 | val_loss=5.7633, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0839, train_f1=0.368 | val_loss=4.5493, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0821, train_f1=0.370 | val_loss=5.0141, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0813, train_f1=0.358 | val_loss=4.8372, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0811, train_f1=0.353 | val_loss=4.2634, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0806, train_f1=0.338 | val_loss=4.5238, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0807, train_f1=0.341 | val_loss=4.3173, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1958, train_f1=0.161 | val_loss=4.5982, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0882, train_f1=0.265 | val_loss=6.6304, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0855, train_f1=0.270 | val_loss=7.5197, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0842, train_f1=0.281 | val_loss=7.5414, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0834, train_f1=0.265 | val_loss=6.6358, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0826, train_f1=0.289 | val_loss=6.4855, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0822, train_f1=0.295 | val_loss=7.1020, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1754, train_f1=0.163 | val_loss=5.4716, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0857, train_f1=0.311 | val_loss=6.1029, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0827, train_f1=0.320 | val_loss=5.1033, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0814, train_f1=0.315 | val_loss=5.0776, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0811, train_f1=0.337 | val_loss=4.8224, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0809, train_f1=0.316 | val_loss=4.8206, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0803, train_f1=0.328 | val_loss=4.9488, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 2 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1599, train_f1=0.205 | val_loss=6.6573, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0852, train_f1=0.289 | val_loss=7.3497, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0831, train_f1=0.310 | val_loss=6.7315, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0816, train_f1=0.325 | val_loss=7.2044, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0813, train_f1=0.315 | val_loss=6.5001, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.312 | val_loss=5.6017, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0805, train_f1=0.321 | val_loss=5.5188, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0802, train_f1=0.327 | val_loss=5.1669, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0800, train_f1=0.310 | val_loss=5.4016, val_f1=0.122\n",
      "[Epoch 010] train_loss=0.0799, train_f1=0.324 | val_loss=5.3249, val_f1=0.122\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1480, train_f1=0.208 | val_loss=4.2889, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0797, train_f1=0.370 | val_loss=4.4191, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0783, train_f1=0.372 | val_loss=3.2129, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0771, train_f1=0.370 | val_loss=3.5230, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0765, train_f1=0.359 | val_loss=3.9261, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0761, train_f1=0.293 | val_loss=3.6165, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1526, train_f1=0.297 | val_loss=3.5676, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0799, train_f1=0.384 | val_loss=3.7823, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0783, train_f1=0.373 | val_loss=3.1198, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0770, train_f1=0.374 | val_loss=3.4001, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0764, train_f1=0.300 | val_loss=3.3168, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0760, train_f1=0.272 | val_loss=3.5704, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0757, train_f1=0.228 | val_loss=3.5139, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0755, train_f1=0.211 | val_loss=3.1417, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0755, train_f1=0.219 | val_loss=3.4986, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0754, train_f1=0.110 | val_loss=3.1924, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1425, train_f1=0.260 | val_loss=5.3279, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0794, train_f1=0.373 | val_loss=4.0909, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0777, train_f1=0.388 | val_loss=3.2977, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0769, train_f1=0.378 | val_loss=3.3195, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0763, train_f1=0.370 | val_loss=3.8853, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0761, train_f1=0.306 | val_loss=3.8256, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.0000\n",
      "\n",
      "=== Config: gcn 64 3 0.0 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1447, train_f1=0.286 | val_loss=4.4792, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0800, train_f1=0.387 | val_loss=3.5065, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0779, train_f1=0.377 | val_loss=3.6277, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0769, train_f1=0.373 | val_loss=3.4628, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0764, train_f1=0.325 | val_loss=3.0592, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0761, train_f1=0.258 | val_loss=4.0965, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0758, train_f1=0.280 | val_loss=3.6649, val_f1=0.000\n",
      "[Epoch 008] train_loss=0.0758, train_f1=0.224 | val_loss=3.1198, val_f1=0.000\n",
      "[Epoch 009] train_loss=0.0756, train_f1=0.093 | val_loss=3.3265, val_f1=0.000\n",
      "[Epoch 010] train_loss=0.0755, train_f1=0.179 | val_loss=3.1140, val_f1=0.000\n",
      "Early stopping triggered at epoch 10. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1584, train_f1=0.215 | val_loss=4.8571, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0831, train_f1=0.340 | val_loss=3.6614, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0798, train_f1=0.380 | val_loss=3.0925, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0790, train_f1=0.386 | val_loss=3.8629, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0784, train_f1=0.364 | val_loss=3.3854, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0780, train_f1=0.343 | val_loss=3.8206, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0777, train_f1=0.329 | val_loss=3.3707, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0773, train_f1=0.327 | val_loss=3.4196, val_f1=0.000\n",
      "Early stopping triggered at epoch 8. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1798, train_f1=0.230 | val_loss=3.4619, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0819, train_f1=0.362 | val_loss=3.6275, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0799, train_f1=0.386 | val_loss=3.5483, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0789, train_f1=0.372 | val_loss=3.8752, val_f1=0.219\n",
      "[Epoch 005] train_loss=0.0787, train_f1=0.360 | val_loss=3.4912, val_f1=0.219\n",
      "[Epoch 006] train_loss=0.0780, train_f1=0.353 | val_loss=3.2935, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0779, train_f1=0.287 | val_loss=3.2678, val_f1=0.122\n",
      "[Epoch 008] train_loss=0.0776, train_f1=0.305 | val_loss=3.2563, val_f1=0.122\n",
      "[Epoch 009] train_loss=0.0776, train_f1=0.298 | val_loss=3.3463, val_f1=0.000\n",
      "Early stopping triggered at epoch 9. Best f1 = 0.2188\n",
      "\n",
      "=== Config: sage 64 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1376, train_f1=0.271 | val_loss=3.4379, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0802, train_f1=0.360 | val_loss=2.8032, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0793, train_f1=0.360 | val_loss=3.4792, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0788, train_f1=0.345 | val_loss=3.7921, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0782, train_f1=0.333 | val_loss=3.8342, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0776, train_f1=0.347 | val_loss=3.7687, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0777, train_f1=0.277 | val_loss=3.4906, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.1 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1480, train_f1=0.235 | val_loss=4.0951, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0811, train_f1=0.368 | val_loss=3.0371, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0797, train_f1=0.381 | val_loss=3.7405, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0794, train_f1=0.368 | val_loss=2.9432, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0788, train_f1=0.346 | val_loss=3.5876, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0785, train_f1=0.334 | val_loss=4.0882, val_f1=0.122\n",
      "[Epoch 007] train_loss=0.0780, train_f1=0.330 | val_loss=3.3887, val_f1=0.122\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1781, train_f1=0.239 | val_loss=4.8799, val_f1=0.219\n",
      "[Epoch 002] train_loss=0.0838, train_f1=0.376 | val_loss=2.9262, val_f1=0.000\n",
      "[Epoch 003] train_loss=0.0816, train_f1=0.374 | val_loss=2.8273, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0811, train_f1=0.335 | val_loss=2.8975, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0808, train_f1=0.308 | val_loss=3.4720, val_f1=0.000\n",
      "[Epoch 006] train_loss=0.0802, train_f1=0.307 | val_loss=3.2961, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.2188\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1910, train_f1=0.224 | val_loss=4.7106, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0846, train_f1=0.384 | val_loss=5.1287, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0826, train_f1=0.363 | val_loss=3.9456, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0817, train_f1=0.329 | val_loss=4.2042, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0807, train_f1=0.327 | val_loss=4.2793, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0808, train_f1=0.298 | val_loss=4.0603, val_f1=0.122\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1222\n",
      "\n",
      "=== Config: sage 64 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1776, train_f1=0.160 | val_loss=5.5587, val_f1=0.000\n",
      "[Epoch 002] train_loss=0.0854, train_f1=0.240 | val_loss=4.5239, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0818, train_f1=0.309 | val_loss=3.7609, val_f1=0.122\n",
      "[Epoch 004] train_loss=0.0809, train_f1=0.288 | val_loss=4.5107, val_f1=0.000\n",
      "[Epoch 005] train_loss=0.0805, train_f1=0.270 | val_loss=3.6410, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0797, train_f1=0.280 | val_loss=3.5054, val_f1=0.000\n",
      "[Epoch 007] train_loss=0.0795, train_f1=0.274 | val_loss=3.8832, val_f1=0.000\n",
      "Early stopping triggered at epoch 7. Best f1 = 0.1222\n",
      "\n",
      "=== Config: gcn 64 3 0.3 mean+max lr 0.0005 ===\n",
      "[Epoch 001] train_loss=0.1694, train_f1=0.238 | val_loss=4.4318, val_f1=0.122\n",
      "[Epoch 002] train_loss=0.0840, train_f1=0.330 | val_loss=4.1102, val_f1=0.122\n",
      "[Epoch 003] train_loss=0.0816, train_f1=0.343 | val_loss=4.0084, val_f1=0.000\n",
      "[Epoch 004] train_loss=0.0809, train_f1=0.301 | val_loss=3.9635, val_f1=0.122\n",
      "[Epoch 005] train_loss=0.0804, train_f1=0.271 | val_loss=3.4816, val_f1=0.122\n",
      "[Epoch 006] train_loss=0.0800, train_f1=0.265 | val_loss=3.8231, val_f1=0.000\n",
      "Early stopping triggered at epoch 6. Best f1 = 0.1222\n",
      "\n",
      "Best config: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.3, 'conv_type': 'gcn', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best validation f1 = 0.22654218634316972\n"
     ]
    }
   ],
   "source": [
    "best_overall = None\n",
    "best_cfg = None\n",
    "\n",
    "for cfg in iter_gnn_configs(GNN_HYP):\n",
    "    print(\"\\n=== Config:\", cfg[\"conv_type\"], cfg[\"hidden_dim\"], cfg[\"num_layers\"],\n",
    "          cfg[\"dropout\"], cfg[\"aggr\"], \"lr\", cfg[\"lr\"], \"===\")\n",
    "    out = train_gnn_model(cfg, train_loader, val_loader, in_dim=D_NODE, device=DEVICE)\n",
    "\n",
    "    if best_overall is None or out[\"best_metric\"] > best_overall[\"best_metric\"]:\n",
    "        best_overall = out\n",
    "        best_cfg = cfg\n",
    "\n",
    "print(\"\\nBest config:\", best_cfg)\n",
    "print(\"Best validation\", best_overall[\"best_monitor\"], \"=\", best_overall[\"best_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107dc2c-c14c-46d9-a86d-dfdc6f7a80ba",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f1eea-8bf1-4d6b-8620-b3567760851d",
   "metadata": {},
   "source": [
    "#### PRB-Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c2a31-c14e-4dac-a7fb-cef109a5d9b4",
   "metadata": {},
   "source": [
    "##### Helper Functions\n",
    "\n",
    "This code provides evaluation utilities for a trained PRB-GraphSAGE model, computing metrics and confusion statistics on a given dataset. The `evaluate_gnn_model` function runs in no-grad mode, sets the model to eval, and iterates over a PyTorch Geometric loader, moving each batch to the target device, forwarding node features and graph structure through the model to obtain logits, and computing a loss with the same `gnn_criterion` used during training. It accumulates batch-weighted loss, collects sigmoid-transformed probabilities and true labels, and at the end concatenates them to compute average loss and standard binary classification metrics (accuracy, precision, recall, F1) via the existing `compute_binary_metrics` helper, returning both the metrics dictionary and the full `y_true` and `y_prob` arrays. The `summarize_eval_split` helper prints these metrics in a compact, human-readable line for a named split (e.g., “train”, “val”, “test”). Finally, `confusion_from_probs` turns true labels and predicted probabilities into a small 2×2 confusion matrix (tp, tn, fp, fn) using a configurable threshold, making it easy to inspect the model’s error profile on any evaluation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499402f4-f608-4b91-bdd8-a2a11fd4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL: PRB-GraphSAGE Evaluation (Best Model) ====\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gnn_model(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    device: torch.device,\n",
    "    threshold: float = 0.5,\n",
    ") -> Tuple[Dict[str, float], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained PRB-GraphSAGE model on a given loader.\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict with loss, acc, precision, recall, f1\n",
    "        y_true:  np.ndarray of true labels (0/1)\n",
    "        y_prob:  np.ndarray of predicted probabilities p(y=1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = batch.y.view(-1).float()            # [B]\n",
    "\n",
    "        logits = model(batch.x, batch.edge_index, batch.batch)  # [B] or [B,1]\n",
    "        logits = logits.view_as(y)\n",
    "        loss = gnn_criterion(logits, y)\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if total_samples == 0:\n",
    "        metrics = {\"loss\": 0.0, \"acc\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        return metrics, np.array([]), np.array([])\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    # Reuse the helper you already defined earlier\n",
    "    metrics = compute_binary_metrics(all_labels, all_probs, threshold=threshold)\n",
    "    metrics[\"loss\"] = float(avg_loss)\n",
    "    return metrics, all_labels, all_probs\n",
    "\n",
    "\n",
    "def summarize_eval_split(name: str, metrics: Dict[str, float]):\n",
    "    print(\n",
    "        f\"[{name}] \"\n",
    "        f\"loss={metrics['loss']:.4f}  \"\n",
    "        f\"acc={metrics['acc']:.3f}  \"\n",
    "        f\"precision={metrics['precision']:.3f}  \"\n",
    "        f\"recall={metrics['recall']:.3f}  \"\n",
    "        f\"f1={metrics['f1']:.3f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def confusion_from_probs(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Tiny 2x2 confusion matrix helper for reporting.\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64).ravel()\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64).ravel()\n",
    "\n",
    "    tp = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    tn = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    fp = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "    fn = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "\n",
    "    return {\"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5041e3-36bb-4d10-af3a-ce861facec6c",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "\n",
    "This code evaluates the best-performing PRB-Graph model found during the hyperparameter sweep on both the training and validation splits to assess fit and generalization. It first retrieves the model with its restored best weights from `best_overall`, moves it to the active device, switches it to evaluation mode, and prints the corresponding best hyperparameter configuration and monitored metric (e.g., validation F1). It then calls `evaluate_gnn_model` on the training DataLoader to compute loss and binary metrics, uses `summarize_eval_split` to print a concise summary line, and derives a 2×2 confusion matrix with `confusion_from_probs` to inspect true/false positives and negatives. The same process is repeated on the validation DataLoader, with the validation metrics and confusion matrix serving as the primary performance numbers to report in experiments or papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40e0633-d5f0-49b5-96ae-3672e7621312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.3, 'conv_type': 'gcn', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best monitored metric (f1): 0.2265\n",
      "\n",
      "[train] loss=0.0857  acc=0.940  precision=0.452  recall=0.979  f1=0.618\n",
      "  train confusion: {'tp': 11133, 'tn': 202990, 'fp': 13522, 'fn': 236}\n",
      "[val] loss=4.5763  acc=0.385  precision=0.991  recall=0.128  f1=0.227\n",
      "  val confusion: {'tp': 6491, 'tn': 21305, 'fp': 60, 'fn': 44263}\n"
     ]
    }
   ],
   "source": [
    "# Grab the best model (weights are already restored in train_gnn_model)\n",
    "best_model = best_overall[\"model\"].to(DEVICE)\n",
    "best_model.eval()\n",
    "\n",
    "print(\"Best config:\", best_cfg)\n",
    "print(f\"Best monitored metric ({best_overall['best_monitor']}): {best_overall['best_metric']:.4f}\\n\")\n",
    "\n",
    "# Evaluate on training split (to see overfit / underfit)\n",
    "train_metrics, train_y, train_p = evaluate_gnn_model(best_model, train_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"train\", train_metrics)\n",
    "train_cm = confusion_from_probs(train_y, train_p, threshold=0.5)\n",
    "print(\"  train confusion:\", train_cm)\n",
    "\n",
    "# Evaluate on validation split (primary number for the report)\n",
    "val_metrics, val_y, val_p = evaluate_gnn_model(best_model, val_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"val\", val_metrics)\n",
    "val_cm = confusion_from_probs(val_y, val_p, threshold=0.5)\n",
    "print(\"  val confusion:\", val_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cf206-79e8-436e-a047-b0fff9227e57",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73cd19b0-c9ba-4582-8425-3687e26c94d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.3, 'conv_type': 'gcn', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best monitored metric (f1): 0.2265\n",
      "\n",
      "[train] loss=0.0857  acc=0.940  precision=0.452  recall=0.979  f1=0.618\n",
      "  train confusion: {'tp': 11133, 'tn': 202990, 'fp': 13522, 'fn': 236}\n",
      "[val] loss=4.5763  acc=0.385  precision=0.991  recall=0.128  f1=0.227\n",
      "  val confusion: {'tp': 6491, 'tn': 21305, 'fp': 60, 'fn': 44263}\n",
      "\n",
      "Saved best model checkpoint to: models/best_gnn_model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "EXPORT_DIR = \"models\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "EXPORT_PATH = os.path.join(EXPORT_DIR, \"best_gnn_model.pt\")\n",
    "\n",
    "best_model = best_overall[\"model\"].to(DEVICE)\n",
    "best_model.eval()\n",
    "\n",
    "print(\"Best config:\", best_cfg)\n",
    "print(f\"Best monitored metric ({best_overall['best_monitor']}): {best_overall['best_metric']:.4f}\\n\")\n",
    "\n",
    "train_metrics, train_y, train_p = evaluate_gnn_model(best_model, train_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"train\", train_metrics)\n",
    "train_cm = confusion_from_probs(train_y, train_p, threshold=0.5)\n",
    "print(\"  train confusion:\", train_cm)\n",
    "\n",
    "val_metrics, val_y, val_p = evaluate_gnn_model(best_model, val_loader, DEVICE, threshold=0.5)\n",
    "summarize_eval_split(\"val\", val_metrics)\n",
    "val_cm = confusion_from_probs(val_y, val_p, threshold=0.5)\n",
    "print(\"  val confusion:\", val_cm)\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": best_model.state_dict(),\n",
    "    \"config\": best_cfg,                         # whatever you used to build the model\n",
    "    \"best_monitor\": best_overall[\"best_monitor\"],\n",
    "    \"best_metric\": best_overall[\"best_metric\"],\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, EXPORT_PATH)\n",
    "print(f\"\\nSaved best model checkpoint to: {EXPORT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afca6c1-1459-40e8-a079-996ac03fe16e",
   "metadata": {},
   "source": [
    "### Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1cc1a03-f379-4ef6-9e8d-341e637bbde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best config from checkpoint: {'batch_size': 32, 'epochs': 40, 'lr': 0.0002, 'lr_grid': [0.0002, 0.0003, 0.0005], 'weight_decay': 0.0001, 'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.3, 'conv_type': 'gcn', 'aggr': 'mean', 'use_batchnorm': True, 'hidden_dim_grid': [32, 64], 'num_layers_grid': [1, 2, 3], 'dropout_grid': [0.0, 0.1, 0.3], 'aggr_grid': ['mean', 'mean+max'], 'conv_type_grid': ['sage', 'gcn'], 'mlp_hidden_dim': 64, 'mlp_layers': 2, 'num_classes': 1, 'early_stop': {'monitor': 'f1', 'mode': 'max', 'patience': 5, 'min_delta': 0.001}, 'warmup_steps': 200, 'total_steps': 10000}\n",
      "Best validation metric stored: f1 = 0.22654218634316972\n",
      "\n",
      "[VAL] Metrics at threshold 0.5 used during training:\n",
      "{'acc': 0.3854185443503099, 'precision': 0.9908410929627551, 'recall': 0.1278913977223445, 'f1': 0.22654218634316972, 'loss': 4.576302989548515}\n",
      "\n",
      "[VAL] Best F1 over PR curve ≈ 0.842 at threshold ≈ 0.0003\n",
      "\n",
      "[VAL] Confusion matrix at threshold ≈ 0.0003:\n",
      "TN: 2425 FP: 18940 FN: 37 TP: 50717\n",
      "\n",
      "Saved PR curve + confusion matrix figure to: figures/fig_pr_curve_and_confusion_val.png\n",
      "\n",
      "Using benign example at ds_all index 0, attack example at ds_all index 30196 for attributions.\n",
      "Saved gradient-based attribution heatmaps to: figures/fig_gnn_gradient_attributions.png\n"
     ]
    }
   ],
   "source": [
    "# ==== CELL: Visualization – PR Curve, Confusion Matrix, and Gradient-Based Attributions ====\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load best model from checkpoint\n",
    "# -----------------------------------------------------------------------------\n",
    "FIG_DIR = \"figures\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "CKPT_PATH = \"models/best_gnn_model.pt\"\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "\n",
    "best_cfg = checkpoint[\"config\"]\n",
    "print(\"Loaded best config from checkpoint:\", best_cfg)\n",
    "print(\"Best validation metric stored:\", checkpoint.get(\"best_monitor\"), \"=\", checkpoint.get(\"best_metric\"))\n",
    "\n",
    "# Rebuild model architecture and load weights\n",
    "viz_model = make_gnn_model(best_cfg, D_NODE)\n",
    "viz_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "viz_model.to(DEVICE)\n",
    "viz_model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Precision–Recall curve + confusion matrix on held-out validation data\n",
    "# -----------------------------------------------------------------------------\n",
    "val_metrics, y_true_val, y_prob_val = evaluate_gnn_model(\n",
    "    viz_model,\n",
    "    val_loader,\n",
    "    DEVICE,\n",
    "    threshold=0.5,  # threshold here is only for metrics; we'll sweep separately\n",
    ")\n",
    "\n",
    "print(\"\\n[VAL] Metrics at threshold 0.5 used during training:\")\n",
    "print(val_metrics)\n",
    "\n",
    "# Compute PR curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_true_val, y_prob_val)\n",
    "ap = average_precision_score(y_true_val, y_prob_val)\n",
    "\n",
    "# Find threshold that maximizes F1 over the PR curve points\n",
    "# Note: precision_recall_curve returns len(thresholds)+1 points; F1 is computed at those points.\n",
    "eps = 1e-9\n",
    "f1_points = 2 * precision * recall / (precision + recall + eps)\n",
    "best_idx = int(np.nanargmax(f1_points))\n",
    "best_f1_est = float(f1_points[best_idx])\n",
    "\n",
    "if best_idx == 0:\n",
    "    # first PR point has no associated threshold; fall back to 0.5\n",
    "    best_thr = 0.5\n",
    "else:\n",
    "    best_thr = float(pr_thresholds[best_idx - 1])\n",
    "\n",
    "print(f\"\\n[VAL] Best F1 over PR curve ≈ {best_f1_est:.3f} at threshold ≈ {best_thr:.4f}\")\n",
    "\n",
    "# Confusion matrix at best_thr\n",
    "y_pred_best = (y_prob_val >= best_thr).astype(int)\n",
    "cm = confusion_matrix(y_true_val, y_pred_best, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\n[VAL] Confusion matrix at threshold ≈ {:.4f}:\".format(best_thr))\n",
    "print(\"TN:\", tn, \"FP:\", fp, \"FN:\", fn, \"TP:\", tp)\n",
    "\n",
    "# Plot PR curve + confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- PR curve ---\n",
    "ax_pr = axes[0]\n",
    "ax_pr.plot(recall, precision, drawstyle=\"steps-post\")\n",
    "ax_pr.set_xlabel(\"Recall\")\n",
    "ax_pr.set_ylabel(\"Precision\")\n",
    "ax_pr.set_title(f\"Validation Precision–Recall (AP = {ap:.3f})\")\n",
    "ax_pr.set_xlim([0.0, 1.0])\n",
    "ax_pr.set_ylim([0.0, 1.05])\n",
    "\n",
    "# Mark best F1 point\n",
    "ax_pr.scatter(recall[best_idx], precision[best_idx], color=\"red\", label=f\"Best F1≈{best_f1_est:.3f}\")\n",
    "ax_pr.legend(loc=\"lower left\")\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "ax_cm = axes[1]\n",
    "im = ax_cm.imshow(cm, cmap=\"Blues\")\n",
    "ax_cm.set_xticks([0, 1], labels=[\"Pred 0 (benign)\", \"Pred 1 (attack)\"], rotation=15, ha=\"right\")\n",
    "ax_cm.set_yticks([0, 1], labels=[\"True 0 (benign)\", \"True 1 (attack)\"])\n",
    "ax_cm.set_title(f\"Confusion Matrix @ threshold ≈ {best_thr:.4f}\")\n",
    "\n",
    "# Annotate each cell\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax_cm.text(\n",
    "            j,\n",
    "            i,\n",
    "            str(cm[i, j]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "fig.colorbar(im, ax=ax_cm, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "pr_cm_path = os.path.join(FIG_DIR, \"fig_pr_curve_and_confusion_val.png\")\n",
    "plt.savefig(pr_cm_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"\\nSaved PR curve + confusion matrix figure to: {pr_cm_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Gradient-based node×feature attributions (GNNExplainer-style)\n",
    "# -----------------------------------------------------------------------------\n",
    "def find_example_by_label(dataset, target_label: int):\n",
    "    \"\"\"\n",
    "    Return the first graph Data object in `dataset` whose y == target_label and has nodes.\n",
    "    \"\"\"\n",
    "    for idx in range(len(dataset)):\n",
    "        d = dataset[idx]\n",
    "        if d.x is None or d.x.numel() == 0:\n",
    "            continue\n",
    "        if d.edge_index is None:\n",
    "            continue\n",
    "        if int(d.y.item()) == target_label:\n",
    "            return d, idx\n",
    "    return None, None\n",
    "\n",
    "def compute_node_feature_importance(model: torch.nn.Module, data) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient×input node feature importance:\n",
    "      importance[node, feature] = |grad * x|\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs with gradient tracking only on x\n",
    "    x = data.x.to(DEVICE).clone().detach()\n",
    "    x.requires_grad_(True)\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    batch = torch.zeros(x.size(0), dtype=torch.long, device=DEVICE)  # single graph\n",
    "\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    logits = model(x, edge_index, batch)  # scalar logit for binary classification\n",
    "    logit = logits.view(-1)[0]\n",
    "    logit.backward()\n",
    "\n",
    "    grad = x.grad  # same shape as x\n",
    "    # Detach *after* grad×input\n",
    "    importance = (grad * x).abs().detach().cpu().numpy()  # [N, F]\n",
    "    return importance\n",
    "\n",
    "# Feature names must match build_node_features() ordering:\n",
    "feature_names = [\n",
    "    \"mean_share\",\n",
    "    \"min_share\",\n",
    "    \"max_share\",\n",
    "    \"std_share\",\n",
    "    \"active_frac\",\n",
    "    \"zero_frac\",\n",
    "    \"longest_zero_run\",\n",
    "]\n",
    "\n",
    "# Find representative benign (0) and attack (1) graphs\n",
    "benign_data, benign_idx = find_example_by_label(ds_all, target_label=0)\n",
    "attack_data, attack_idx = find_example_by_label(ds_all, target_label=1)\n",
    "\n",
    "if benign_data is None or attack_data is None:\n",
    "    print(\"\\n[WARN] Could not find both benign and attack examples in ds_all; \"\n",
    "          \"skipping attribution heatmaps.\")\n",
    "else:\n",
    "    print(f\"\\nUsing benign example at ds_all index {benign_idx}, \"\n",
    "          f\"attack example at ds_all index {attack_idx} for attributions.\")\n",
    "\n",
    "    benign_imp = compute_node_feature_importance(viz_model, benign_data)\n",
    "    attack_imp = compute_node_feature_importance(viz_model, attack_data)\n",
    "\n",
    "    # Optionally, sort nodes by total importance to keep the visualization compact\n",
    "    def top_nodes_by_importance(imp: np.ndarray, max_nodes: int = 32):\n",
    "        scores = imp.sum(axis=1)\n",
    "        order = np.argsort(-scores)\n",
    "        return imp[order[:max_nodes], :], order[:max_nodes]\n",
    "\n",
    "    benign_imp_top, benign_nodes = top_nodes_by_importance(benign_imp, max_nodes=32)\n",
    "    attack_imp_top, attack_nodes = top_nodes_by_importance(attack_imp, max_nodes=32)\n",
    "\n",
    "    # Plot side-by-side heatmaps\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for ax, imp, title, nodes in [\n",
    "        (axes[0], benign_imp_top, \"Benign window (gradient×input)\", benign_nodes),\n",
    "        (axes[1], attack_imp_top, \"Attack window (gradient×input)\", attack_nodes),\n",
    "    ]:\n",
    "        im = ax.imshow(imp, aspect=\"auto\")\n",
    "        ax.set_xlabel(\"Node feature\")\n",
    "        ax.set_ylabel(\"Node index (subset)\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(np.arange(len(feature_names)), labels=feature_names, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(np.arange(len(nodes)), labels=[str(int(n)) for n in nodes])\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    attr_path = os.path.join(FIG_DIR, \"fig_gnn_gradient_attributions.png\")\n",
    "    plt.savefig(attr_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved gradient-based attribution heatmaps to: {attr_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
